{
    "summary": "The code defines neural network functions for data processing, including ResidualRecurrentBlocks and BatchNorm2d initialization, as well as MLPs, LSTM/RNN layers, Transformer blocks with recurrent forward pass. It also includes a function `get_norm` for normalization and another function `_banded_repeat`.",
    "details": [
        {
            "comment": "This code defines a function `get_module_log_keys_recursive` that recursively collects all keys that a module and its children want to log. It also defines a class `FanInInitReLULayer` which implements a slightly modified initialization for ReLU layers, initializing the weights with standard deviation of 1. The class takes parameters such as number of input and output channels, layer type (linear, conv or conv3d), initialization scale, and whether to use batch normalization.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":0-29",
            "content": "from typing import Dict, Optional\nimport torch as th\nfrom torch import nn\nfrom torch.nn import functional as F\nimport lib.torch_util as tu\nfrom lib.masked_attention import MaskedAttention\nfrom lib.minecraft_util import store_args\nfrom lib.tree_util import tree_map\ndef get_module_log_keys_recursive(m: nn.Module):\n    \"\"\"Recursively get all keys that a module and its children want to log.\"\"\"\n    keys = []\n    if hasattr(m, \"get_log_keys\"):\n        keys += m.get_log_keys()\n    for c in m.children():\n        keys += get_module_log_keys_recursive(c)\n    return keys\nclass FanInInitReLULayer(nn.Module):\n    \"\"\"Implements a slightly modified init that correctly produces std 1 outputs given ReLU activation\n    :param inchan: number of input channels\n    :param outchan: number of output channels\n    :param layer_args: positional layer args\n    :param layer_type: options are \"linear\" (dense layer), \"conv\" (2D Convolution), \"conv3d\" (3D convolution)\n    :param init_scale: multiplier on initial weights\n    :param batch_norm: use batch norm after the layer (for 2D data)"
        },
        {
            "comment": "This code defines a function `__init__` which initializes an object. It takes various parameters like `inchan`, `outchan`, `layer_args`, `layer_type`, `init_scale`, `batch_norm`, `batch_norm_kwargs`, `group_norm_groups`, `layer_norm`, `use_activation`, and `log_scope`. It also takes keyword arguments like `**layer_kwargs`. The function sets the normalization type based on the values of these parameters. If `batch_norm` is True, it uses BatchNorm2d. If `group_norm_groups` is not None, it uses GroupNorm. And if `layer_norm` is True, it uses LayerNorm. It also sets the norm variable to None initially.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":30-61",
            "content": "    :param group_norm_groups: if not None, use group norm with this many groups after the layer. Group norm 1\n        would be equivalent of layernorm for 2D data.\n    :param layer_norm: use layernorm after the layer (for 1D data)\n    :param layer_kwargs: keyword arguments for the layer\n    \"\"\"\n    @store_args\n    def __init__(\n        self,\n        inchan: int,\n        outchan: int,\n        *layer_args,\n        layer_type: str = \"conv\",\n        init_scale: int = 1,\n        batch_norm: bool = False,\n        batch_norm_kwargs: Dict = {},\n        group_norm_groups: Optional[int] = None,\n        layer_norm: bool = False,\n        use_activation=True,\n        log_scope: Optional[str] = None,\n        **layer_kwargs,\n    ):\n        super().__init__()\n        # Normalization\n        self.norm = None\n        if batch_norm:\n            self.norm = nn.BatchNorm2d(inchan, **batch_norm_kwargs)\n        elif group_norm_groups is not None:\n            self.norm = nn.GroupNorm(group_norm_groups, inchan)\n        elif layer_norm:\n            self.norm = nn.LayerNorm(inchan)"
        },
        {
            "comment": "This code defines a util module with functions for initializing and forwarding data through neural networks. The ResidualRecurrentBlocks class is used to create residual recurrent blocks, which help in improving the stability of the network during training.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":63-93",
            "content": "        layer = dict(conv=nn.Conv2d, conv3d=nn.Conv3d, linear=nn.Linear)[layer_type]\n        self.layer = layer(inchan, outchan, bias=self.norm is None, *layer_args, **layer_kwargs)\n        # Init Weights (Fan-In)\n        self.layer.weight.data *= init_scale / self.layer.weight.norm(\n            dim=tuple(range(1, self.layer.weight.data.ndim)), p=2, keepdim=True\n        )\n        # Init Bias\n        if self.layer.bias is not None:\n            self.layer.bias.data *= 0\n    def forward(self, x):\n        \"\"\"Norm after the activation. Experimented with this for both IAM and BC and it was slightly better.\"\"\"\n        if self.norm is not None:\n            x = self.norm(x)\n        x = self.layer(x)\n        if self.use_activation:\n            x = F.relu(x, inplace=True)\n        return x\n    def get_log_keys(self):\n        return [\n            f\"activation_mean/{self.log_scope}\",\n            f\"activation_std/{self.log_scope}\",\n        ]\nclass ResidualRecurrentBlocks(nn.Module):\n    @store_args\n    def __init__(\n        self,"
        },
        {
            "comment": "This code defines a class that initializes a list of ResidualRecurrentBlock instances, each with potentially different recurrence_type and block_kwargs. The forward method processes input through each block, while the initial_state method returns an initial state for the LSTM recurrence type based on batch size.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":94-125",
            "content": "        n_block=2,\n        recurrence_type=\"multi_layer_lstm\",\n        is_residual=True,\n        **block_kwargs,\n    ):\n        super().__init__()\n        init_scale = n_block ** -0.5 if is_residual else 1\n        self.blocks = nn.ModuleList(\n            [\n                ResidualRecurrentBlock(\n                    **block_kwargs,\n                    recurrence_type=recurrence_type,\n                    is_residual=is_residual,\n                    init_scale=init_scale,\n                    block_number=i,\n                )\n                for i in range(n_block)\n            ]\n        )\n    def forward(self, x, first, state):\n        state_out = []\n        assert len(state) == len(\n            self.blocks\n        ), f\"Length of state {len(state)} did not match length of blocks {len(self.blocks)}\"\n        for block, _s_in in zip(self.blocks, state):\n            x, _s_o = block(x, first, _s_in)\n            state_out.append(_s_o)\n        return x, state_out\n    def initial_state(self, batchsize):\n        if \"lstm\" in self.recurrence_type:"
        },
        {
            "comment": "The code defines a ResidualRecurrentBlock class, which is a type of neural network module. It initializes the block with specified parameters like hidsize, timesteps, init_scale, recurrence_type, and more. If is_residual and use_pointwise_layer are True, the mlp0 layer is added to the block with specific size and initialization settings. The method returns an array of initial states for each block in the self.blocks list.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":126-160",
            "content": "            return [None for b in self.blocks]\n        else:\n            return [b.r.initial_state(batchsize) for b in self.blocks]\nclass ResidualRecurrentBlock(nn.Module):\n    @store_args\n    def __init__(\n        self,\n        hidsize,\n        timesteps,\n        init_scale=1,\n        recurrence_type=\"multi_layer_lstm\",\n        is_residual=True,\n        use_pointwise_layer=True,\n        pointwise_ratio=4,\n        pointwise_use_activation=False,\n        attention_heads=8,\n        attention_memory_size=2048,\n        attention_mask_style=\"clipped_causal\",\n        log_scope=\"resblock\",\n        block_number=0,\n    ):\n        super().__init__()\n        self.log_scope = f\"{log_scope}{block_number}\"\n        s = init_scale\n        if use_pointwise_layer:\n            if is_residual:\n                s *= 2 ** -0.5  # second residual\n            self.mlp0 = FanInInitReLULayer(\n                hidsize,\n                hidsize * pointwise_ratio,\n                init_scale=1,\n                layer_type=\"linear\",\n                layer_norm=True,"
        },
        {
            "comment": "Creating a multi-layer perceptron (MLP) for pointwise features and layer normalization for pre-training.\n\nInitializing the LSTM or Transformer recurrent layer if specified, using normal distribution with scale 's'.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":161-183",
            "content": "                log_scope=self.log_scope + \"/ptwise_mlp0\",\n            )\n            self.mlp1 = FanInInitReLULayer(\n                hidsize * pointwise_ratio,\n                hidsize,\n                init_scale=s,\n                layer_type=\"linear\",\n                use_activation=pointwise_use_activation,\n                log_scope=self.log_scope + \"/ptwise_mlp1\",\n            )\n        self.pre_r_ln = nn.LayerNorm(hidsize)\n        if recurrence_type in [\"multi_layer_lstm\", \"multi_layer_bilstm\"]:\n            self.r = nn.LSTM(hidsize, hidsize, batch_first=True)\n            nn.init.normal_(self.r.weight_hh_l0, std=s * (self.r.weight_hh_l0.shape[0] ** -0.5))\n            nn.init.normal_(self.r.weight_ih_l0, std=s * (self.r.weight_ih_l0.shape[0] ** -0.5))\n            self.r.bias_hh_l0.data *= 0\n            self.r.bias_ih_l0.data *= 0\n        elif recurrence_type == \"transformer\":\n            self.r = MaskedAttention(\n                input_size=hidsize,\n                timesteps=timesteps,\n                memory_size=attention_memory_size,"
        },
        {
            "comment": "This function defines a recurrent forward pass for a Transformer block. It applies linear layers, LSTM/RNN, and optionally an MLP layer to input `x`. The result is returned along with the updated state.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":184-215",
            "content": "                heads=attention_heads,\n                init_scale=s,\n                norm=\"none\",\n                log_scope=log_scope + \"/sa\",\n                use_muP_factor=True,\n                mask=attention_mask_style,\n            )\n    def forward(self, x, first, state):\n        residual = x\n        x = self.pre_r_ln(x)\n        x, state_out = recurrent_forward(\n            self.r,\n            x,\n            first,\n            state,\n            reverse_lstm=self.recurrence_type == \"multi_layer_bilstm\" and (self.block_number + 1) % 2 == 0,\n        )\n        if self.is_residual and \"lstm\" in self.recurrence_type:  # Transformer already residual.\n            x = x + residual\n        if self.use_pointwise_layer:\n            # Residual MLP\n            residual = x\n            x = self.mlp1(self.mlp0(x))\n            if self.is_residual:\n                x = x + residual\n        return x, state_out\ndef recurrent_forward(module, x, first, state, reverse_lstm=False):\n    if isinstance(module, nn.LSTM):\n        if state is not None:"
        },
        {
            "comment": "This code is initializing a state for a recurrent model and passing input through the model. If reverse_lstm is True, it flips the input and output. The _banded_repeat function repeats an input sequence with a shift and the bandify function converts data from basis functions to a new shape.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":216-252",
            "content": "            # In case recurrent models do not accept a \"first\" argument we zero out the hidden state here\n            mask = 1 - first[:, 0, None, None].to(th.float)\n            state = tree_map(lambda _s: _s * mask, state)\n            state = tree_map(lambda _s: _s.transpose(0, 1), state)  # NL, B, H\n        if reverse_lstm:\n            x = th.flip(x, [1])\n        x, state_out = module(x, state)\n        if reverse_lstm:\n            x = th.flip(x, [1])\n        state_out = tree_map(lambda _s: _s.transpose(0, 1), state_out)  # B, NL, H\n        return x, state_out\n    else:\n        return module(x, first, state)\ndef _banded_repeat(x, t):\n    \"\"\"\n    Repeats x with a shift.\n    For example (ignoring the batch dimension):\n    _banded_repeat([A B C D E], 4)\n    =\n    [D E 0 0 0]\n    [C D E 0 0]\n    [B C D E 0]\n    [A B C D E]\n    \"\"\"\n    b, T = x.shape\n    x = th.cat([x, x.new_zeros(b, t - 1)], dim=1)\n    result = x.unfold(1, T, 1).flip(1)\n    return result\ndef bandify(b_nd, t, T):\n    \"\"\"\n    b_nd -> D_ntT, where\n        \"n\" indexes over basis functions"
        },
        {
            "comment": "This code defines a function `get_norm` for normalization, and another function (not shown) called `_banded_repeat`. The `B_ntT` shape is being assigned based on the `b_nd` shape and a time index `T`. If `bandsize >= T`, it assigns `b_nT` as `b_nd[:, -T:]`. Otherwise, it concatenates `b_nd.new_zeros(nbasis, T - bandsize)` and `b_nd` along dimension 1 to form `b_nT`. The function then returns the result of `_banded_repeat(b_nT, t)`.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/util.py\":253-275",
            "content": "        \"d\" indexes over time differences\n        \"t\" indexes over output time\n        \"T\" indexes over input time\n        only t >= T is nonzero\n    B_ntT[n, t, T] = b_nd[n, t - T]\n    \"\"\"\n    nbasis, bandsize = b_nd.shape\n    b_nd = b_nd[:, th.arange(bandsize - 1, -1, -1)]\n    if bandsize >= T:\n        b_nT = b_nd[:, -T:]\n    else:\n        b_nT = th.cat([b_nd.new_zeros(nbasis, T - bandsize), b_nd], dim=1)\n    D_tnT = _banded_repeat(b_nT, t)\n    return D_tnT\ndef get_norm(name, d, dtype=th.float32):\n    if name == \"none\":\n        return lambda x: x\n    elif name == \"layer\":\n        return tu.LayerNorm(d, dtype=dtype)\n    else:\n        raise NotImplementedError(name)"
        }
    ]
}