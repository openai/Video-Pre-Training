{
    "summary": "This code organizes player inputs in a video game using action mappings, manages camera actions, handles assertion checks and conversions for different action spaces like buttons, cameras, inventory keys, and factored action space mapping.",
    "details": [
        {
            "comment": "This code defines a class \"ActionMapping\" that maps between the standard Minecraft action space and a new one defined by the user. It uses ordered dictionaries to represent different action groups such as buttons, and requires an odd number of camera bins for initialization.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":0-31",
            "content": "import abc\nimport itertools\nfrom collections import OrderedDict\nfrom typing import Dict, List\nimport numpy as np\nfrom gym3.types import DictType, Discrete, TensorType\nfrom lib.actions import Buttons\nclass ActionMapping(abc.ABC):\n    \"\"\"Class that maps between the standard MC factored action space and a new one you define!\n    :param n_camera_bins: Need to specify this to define the original ac space for stats code\n    \"\"\"\n    # This is the default buttons groups, it can be changed for your action space\n    BUTTONS_GROUPS = OrderedDict(\n        hotbar=[\"none\"] + [f\"hotbar.{i}\" for i in range(1, 10)],\n        fore_back=[\"none\", \"forward\", \"back\"],\n        left_right=[\"none\", \"left\", \"right\"],\n        sprint_sneak=[\"none\", \"sprint\", \"sneak\"],\n        use=[\"none\", \"use\"],\n        drop=[\"none\", \"drop\"],\n        attack=[\"none\", \"attack\"],\n        jump=[\"none\", \"jump\"],\n    )\n    def __init__(self, n_camera_bins: int = 11):\n        assert n_camera_bins % 2 == 1, \"n_camera_bins should be odd\"\n        self.n_camera_bins = n_camera_bins"
        },
        {
            "comment": "This code defines an abstract base class for mapping actions to a new space. It includes methods for converting factored actions to the new space, converting actions in the new space back to the factored action space, returning a gym action space for updating the environment, and returning the null or zero action for this action space.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":32-63",
            "content": "        self.camera_null_bin = n_camera_bins // 2\n        self.stats_ac_space = DictType(\n            **{\n                \"buttons\": TensorType(shape=(len(Buttons.ALL),), eltype=Discrete(2)),\n                \"camera\": TensorType(shape=(2,), eltype=Discrete(n_camera_bins)),\n            }\n        )\n    @abc.abstractmethod\n    def from_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts a factored action (ac) to the new space\n        :param ac: Dictionary of actions that must have a batch dimension\n        \"\"\"\n        pass\n    @abc.abstractmethod\n    def to_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts an action in the new space (ac) to the factored action space.\n        :param ac: Dictionary of actions that must have a batch dimension\n        \"\"\"\n        pass\n    @abc.abstractmethod\n    def get_action_space_update(self):\n        \"\"\"Return a magym (gym3) action space. This will be used to update the env action space.\"\"\"\n        pass\n    @abc.abstractmethod\n    def get_zero_action(self):\n        \"\"\"Return the zero or null action for this action space\"\"\""
        },
        {
            "comment": "This function takes in button actions from a factored action space and a list of mutually exclusive buttons. It returns a list indicating which button (or none if no button was pressed) was chosen for each item in the input array, given that each group has the option of 'none'. The function checks if the shape of the input matches the expected number of buttons.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":64-81",
            "content": "        pass\n    def factored_buttons_to_groups(self, ac_buttons: np.ndarray, button_group: List[str]) -> List[str]:\n        \"\"\"For a mutually exclusive group of buttons in button_group, find which option\n        in the group was chosen. Assumes that each button group has the option of 'none'\n        meaning that no button in the group was pressed.\n        :param ac_buttons: button actions from the factored action space. Should dims [B, len(Buttons.ALL)]\n        :param button_group: List of buttons in a mutually exclusive group. Each item in the\n            list should appear in Buttons.ALL except for the special case 'none' which means\n            no button in the group was pressed. e.g. ['none', 'forward', 'back']. For now\n            'none' must be the first element of button_group\n        Returns a list of length B, where each element is an item from button_group.\n        \"\"\"\n        assert ac_buttons.shape[1] == len(\n            Buttons.ALL\n        ), f\"There should be {len(Buttons.ALL)} buttons in the factored buttons space\""
        },
        {
            "comment": "Ensures function works only when 'none' is in button_group. Maps non-zero action button indices to corresponding actions, handling special cases of mutual press for forward/back and left/right. Prioritizes later buttons in group if pressed at the same time.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":82-96",
            "content": "        assert button_group[0] == \"none\", \"This function only works if 'none' is in button_group\"\n        # Actions in ac_buttons with order according to button_group\n        group_indices = [Buttons.ALL.index(b) for b in button_group if b != \"none\"]\n        ac_choices = ac_buttons[:, group_indices]\n        # Special cases for forward/back, left/right where mutual press means do neither\n        if \"forward\" in button_group and \"back\" in button_group:\n            ac_choices[np.all(ac_choices, axis=-1)] = 0\n        if \"left\" in button_group and \"right\" in button_group:\n            ac_choices[np.all(ac_choices, axis=-1)] = 0\n        ac_non_zero = np.where(ac_choices)\n        ac_choice = [\"none\" for _ in range(ac_buttons.shape[0])]\n        # Iterate over the non-zero indices so that if two buttons in a group were pressed at the same time\n        # we give priority to the button later in the group. E.g. if hotbar.1 and hotbar.2 are pressed during the same\n        # timestep, hotbar.2 is marked as pressed"
        },
        {
            "comment": "This code defines two classes, IDMActionMapping and CameraHierarchicalMapping, which are action mappings used in a video game. The classes define methods to convert actions between factored and non-factored representations, get an action space update, and handle zero actions. These classes seem to be part of a larger system for mapping player inputs to actions in the game environment.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":97-121",
            "content": "        for index, action in zip(ac_non_zero[0], ac_non_zero[1]):\n            ac_choice[index] = button_group[action + 1]  # the zero'th index will mean no button pressed\n        return ac_choice\nclass IDMActionMapping(ActionMapping):\n    \"\"\"For IDM, but essentially this is just an identity mapping\"\"\"\n    def from_factored(self, ac: Dict) -> Dict:\n        return ac\n    def to_factored(self, ac: Dict) -> Dict:\n        return ac\n    def get_action_space_update(self):\n        \"\"\"Return a magym (gym3) action space. This will be used to update the env action space.\"\"\"\n        return {\n            \"buttons\": TensorType(shape=(len(Buttons.ALL),), eltype=Discrete(2)),\n            \"camera\": TensorType(shape=(2,), eltype=Discrete(self.n_camera_bins)),\n        }\n    def get_zero_action(self):\n        raise NotImplementedError()\nclass CameraHierarchicalMapping(ActionMapping):\n    \"\"\"Buttons are joint as in ButtonsJointMapping, but now a camera on/off meta action is added into this joint space.\n    When this meta action is triggered, the separate camera head chooses a camera action which is also now a joint space."
        },
        {
            "comment": "This code adds camera meta actions to the BUTTONS_GROUPS and defines functions for mapping between button combinations, indices, and names.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":123-141",
            "content": "    :param n_camera_bins: number of camera bins in the factored space\n    \"\"\"\n    # Add camera meta action to BUTTONS_GROUPS\n    BUTTONS_GROUPS = ActionMapping.BUTTONS_GROUPS.copy()\n    BUTTONS_GROUPS[\"camera\"] = [\"none\", \"camera\"]\n    BUTTONS_COMBINATIONS = list(itertools.product(*BUTTONS_GROUPS.values())) + [\"inventory\"]\n    BUTTONS_COMBINATION_TO_IDX = {comb: i for i, comb in enumerate(BUTTONS_COMBINATIONS)}\n    BUTTONS_IDX_TO_COMBINATION = {i: comb for i, comb in enumerate(BUTTONS_COMBINATIONS)}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera_groups = OrderedDict(\n            camera_x=[f\"camera_x{i}\" for i in range(self.n_camera_bins)],\n            camera_y=[f\"camera_y{i}\" for i in range(self.n_camera_bins)],\n        )\n        self.camera_combinations = list(itertools.product(*self.camera_groups.values()))\n        self.camera_combination_to_idx = {comb: i for i, comb in enumerate(self.camera_combinations)}\n        self.camera_idx_to_combination = {i: comb for i, comb in enumerate(self.camera_combinations)}"
        },
        {
            "comment": "Code chunk sets up arrays for button and camera action mappings.\nThe code defines button and camera indices, initializes arrays to store the factored actions for each joint action, and begins processing the button combinations.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":142-160",
            "content": "        self.camera_null_idx = self.camera_combination_to_idx[\n            (f\"camera_x{self.camera_null_bin}\", f\"camera_y{self.camera_null_bin}\")\n        ]\n        self._null_action = {\n            \"buttons\": self.BUTTONS_COMBINATION_TO_IDX[tuple(\"none\" for _ in range(len(self.BUTTONS_GROUPS)))]\n        }\n        self._precompute_to_factored()\n    def _precompute_to_factored(self):\n        \"\"\"Precompute the joint action -> factored action matrix.\"\"\"\n        button_dim = self.stats_ac_space[\"buttons\"].size\n        self.BUTTON_IDX_TO_FACTORED = np.zeros((len(self.BUTTONS_IDX_TO_COMBINATION), button_dim), dtype=int)\n        self.BUTTON_IDX_TO_CAMERA_META_OFF = np.zeros((len(self.BUTTONS_IDX_TO_COMBINATION)), dtype=bool)\n        self.CAMERA_IDX_TO_FACTORED = np.zeros((len(self.camera_idx_to_combination), 2), dtype=int)\n        # Pre compute Buttons\n        for jnt_ac, button_comb in self.BUTTONS_IDX_TO_COMBINATION.items():\n            new_button_ac = np.zeros(len(Buttons.ALL), dtype=\"i\")\n            if button_comb == \"inventory\":"
        },
        {
            "comment": "Code is creating factored representations of action and camera combinations for each joint. It initializes new_button_ac to 1 for all inventory buttons, then checks if any other groups are selected and assigns those indices to 1 in new_button_ac. If the last combination is not \"camera\", it sets the camera_meta_off flag. Then it creates new_camera_ac with indices based on the camera group combinations and stores these factored representations for both action and camera in their respective dictionaries.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":161-179",
            "content": "                new_button_ac[Buttons.ALL.index(\"inventory\")] = 1\n            else:\n                for group_choice in button_comb[:-1]:  # Last one is camera\n                    if group_choice != \"none\":\n                        new_button_ac[Buttons.ALL.index(group_choice)] = 1\n                if button_comb[-1] != \"camera\":  # This means camera meta action is off\n                    self.BUTTON_IDX_TO_CAMERA_META_OFF[jnt_ac] = True\n            self.BUTTON_IDX_TO_FACTORED[jnt_ac] = new_button_ac\n        # Pre compute camera\n        for jnt_ac, camera_comb in self.camera_idx_to_combination.items():\n            new_camera_ac = np.ones((2), dtype=\"i\") * self.camera_null_bin\n            new_camera_ac[0] = self.camera_groups[\"camera_x\"].index(camera_comb[0])\n            new_camera_ac[1] = self.camera_groups[\"camera_y\"].index(camera_comb[1])\n            self.CAMERA_IDX_TO_FACTORED[jnt_ac] = new_camera_ac\n    def from_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts a factored action (ac) to the new space. Assumes ac has a batch dim\"\"\""
        },
        {
            "comment": "This code is performing an assertion check to ensure that the \"camera\" and \"buttons\" labels have the correct dimensions. It then creates a dictionary of button choices for each group except camera, sets the camera action based on whether a non-null camera action was given, and finally generates new arrays of button and camera actions based on the choices.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":180-199",
            "content": "        assert ac[\"camera\"].ndim == 2, f\"bad camera label, {ac['camera']}\"\n        assert ac[\"buttons\"].ndim == 2, f\"bad buttons label, {ac['buttons']}\"\n        # Get button choices for everything but camera\n        choices_by_group = OrderedDict(\n            (k, self.factored_buttons_to_groups(ac[\"buttons\"], v)) for k, v in self.BUTTONS_GROUPS.items() if k != \"camera\"\n        )\n        # Set camera \"on off\" action based on whether non-null camera action was given\n        camera_is_null = np.all(ac[\"camera\"] == self.camera_null_bin, axis=1)\n        choices_by_group[\"camera\"] = [\"none\" if is_null else \"camera\" for is_null in camera_is_null]\n        new_button_ac = []\n        new_camera_ac = []\n        for i in range(ac[\"buttons\"].shape[0]):\n            # Buttons\n            key = tuple([v[i] for v in choices_by_group.values()])\n            if ac[\"buttons\"][i, Buttons.ALL.index(\"inventory\")] == 1:\n                key = \"inventory\"\n            new_button_ac.append(self.BUTTONS_COMBINATION_TO_IDX[key])\n            # Camera -- inventory is also exclusive with camera"
        },
        {
            "comment": "This code is converting an action in the new space (ac) to the factored action space. It first checks if the \"inventory\" key is present, and if so, uses a specific key format. For other keys, it uses a different key format. Then it appends the camera indices to a list. The function returns a dictionary with buttons and cameras in the new action space. If the input action has a batch dimension, the code asserts that the shape of both \"camera\" and \"buttons\" are 1, squeezes them, maps the button indices to factored action space, calculates camera offsets, maps the camera indices to factored action space, and replaces the null camera values with \"camera_null_bin\".",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":200-222",
            "content": "            if key == \"inventory\":\n                key = (\n                    f\"camera_x{self.camera_null_bin}\",\n                    f\"camera_y{self.camera_null_bin}\",\n                )\n            else:\n                key = (f\"camera_x{ac['camera'][i][0]}\", f\"camera_y{ac['camera'][i][1]}\")\n            new_camera_ac.append(self.camera_combination_to_idx[key])\n        return dict(\n            buttons=np.array(new_button_ac)[:, None],\n            camera=np.array(new_camera_ac)[:, None],\n        )\n    def to_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts an action in the new space (ac) to the factored action space. Assumes ac has a batch dim\"\"\"\n        assert ac[\"camera\"].shape[-1] == 1\n        assert ac[\"buttons\"].shape[-1] == 1\n        new_button_ac = self.BUTTON_IDX_TO_FACTORED[np.squeeze(ac[\"buttons\"], -1)]\n        camera_off = self.BUTTON_IDX_TO_CAMERA_META_OFF[np.squeeze(ac[\"buttons\"], -1)]\n        new_camera_ac = self.CAMERA_IDX_TO_FACTORED[np.squeeze(ac[\"camera\"], -1)]\n        new_camera_ac[camera_off] = self.camera_null_bin"
        },
        {
            "comment": "This code defines a class with three methods. The first method returns a dictionary containing the \"buttons\" and \"camera\" actions. The second method specifies the action space update, defining the shape and type for both \"camera\" and \"buttons\". The third method returns a null action.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/action_mapping.py\":224-233",
            "content": "        return dict(buttons=new_button_ac, camera=new_camera_ac)\n    def get_action_space_update(self):\n        return {\n            \"camera\": TensorType(shape=(1,), eltype=Discrete(len(self.camera_combinations))),\n            \"buttons\": TensorType(shape=(1,), eltype=Discrete(len(self.BUTTONS_COMBINATIONS))),\n        }\n    def get_zero_action(self):\n        return self._null_action"
        }
    ]
}