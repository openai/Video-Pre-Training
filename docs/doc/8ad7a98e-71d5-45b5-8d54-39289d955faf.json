{
    "summary": "The code uses a decorator function to compute normalized entropy from categorical head outputs, considering masks and ignoring single-option cases. It also calculates the entropy of categorical and diagonal Gaussian action heads within a module by iterating over key-value pairs and returns average entropy.",
    "details": [
        {
            "comment": "This code defines a decorator function `store_args` that takes a method as input, and when the decorated method is called, it stores its arguments as instance attributes of the class. It also handles default argument values and keyword-only arguments.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/minecraft_util.py\":0-31",
            "content": "import functools\nimport inspect\nfrom typing import Optional, Tuple\nimport numpy as np\nimport torch\nfrom lib.action_head import (CategoricalActionHead, DiagGaussianActionHead,\n                             DictActionHead)\ndef store_args(method):\n    \"\"\"Stores provided method args as instance attributes.\"\"\"\n    argspec = inspect.getfullargspec(method)\n    defaults = {}\n    if argspec.defaults is not None:\n        defaults = dict(zip(argspec.args[-len(argspec.defaults) :], argspec.defaults))\n    if argspec.kwonlydefaults is not None:\n        defaults.update(argspec.kwonlydefaults)\n    arg_names = argspec.args[1:]\n    @functools.wraps(method)\n    def wrapper(*positional_args, **keyword_args):\n        self = positional_args[0]\n        # Get default arg values\n        args = defaults.copy()\n        # Add provided arg values\n        for name, value in zip(arg_names, positional_args[1:]):\n            args[name] = value\n        args.update(keyword_args)\n        self.__dict__.update(args)\n        return method(*positional_args, **keyword_args)"
        },
        {
            "comment": "This code calculates the normalized entropy from categorical head outputs and applies a mask if necessary. It divides the entropy by the log of the number of possible options, ignoring cases where only one option is available to avoid nonsense results. The count variable keeps track of how many times the condition for ignoring an option has been met.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/minecraft_util.py\":33-54",
            "content": "    return wrapper\ndef get_norm_entropy_from_cat_head(module, name, masks, logits):\n    # Note that the mask has already been applied to the logits at this point\n    entropy = -torch.sum(torch.exp(logits) * logits, dim=-1)\n    if name in masks:\n        n = torch.sum(masks[name], dim=-1, dtype=torch.float)\n        norm_entropy = entropy / torch.log(n)\n        # When the mask only allows one option the normalized entropy makes no sense\n        # as it is basically both maximal (the distribution is as uniform as it can be)\n        # and minimal (there is no variance at all).\n        # A such, we ignore them for purpose of calculating entropy.\n        zero = torch.zeros_like(norm_entropy)\n        norm_entropy = torch.where(n.eq(1.0), zero, norm_entropy)\n        count = n.not_equal(1.0).int()\n    else:\n        n = torch.tensor(logits.shape[-1], dtype=torch.float)\n        norm_entropy = entropy / torch.log(n)\n        count = torch.ones_like(norm_entropy, dtype=torch.int)\n    # entropy is per-entry, still of size self.output_shape[:-1]; we need to reduce of the rest of it."
        },
        {
            "comment": "This code calculates the entropy of categorical and diagonal Gaussian action heads in a given module and returns the total entropy and counts.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/minecraft_util.py\":55-78",
            "content": "    for _ in module.output_shape[:-1]:\n        norm_entropy = norm_entropy.sum(dim=-1)\n        count = count.sum(dim=-1)\n    return norm_entropy, count\ndef get_norm_cat_entropy(module, masks, logits, template) -> Tuple[torch.Tensor, torch.Tensor]:\n    entropy_sum = torch.zeros_like(template, dtype=torch.float)\n    counts = torch.zeros_like(template, dtype=torch.int)\n    for k, subhead in module.items():\n        if isinstance(subhead, DictActionHead):\n            entropy, count = get_norm_cat_entropy(subhead, masks, logits[k], template)\n        elif isinstance(subhead, CategoricalActionHead):\n            entropy, count = get_norm_entropy_from_cat_head(subhead, k, masks, logits[k])\n        else:\n            continue\n        entropy_sum += entropy\n        counts += count\n    return entropy_sum, counts\ndef get_diag_guassian_entropy(module, logits, template) -> Optional[torch.Tensor]:\n    entropy_sum = torch.zeros_like(template, dtype=torch.float)\n    count = torch.zeros(1, device=template.device, dtype=torch.int)"
        },
        {
            "comment": "Iterates over each key-value pair in the module, adds entropy from DiagGaussianActionHead or DictActionHead to entropy_sum, and returns the average entropy.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/minecraft_util.py\":79-87",
            "content": "    for k, subhead in module.items():\n        if isinstance(subhead, DictActionHead):\n            entropy_sum += get_diag_guassian_entropy(subhead, logits[k], template)\n        elif isinstance(subhead, DiagGaussianActionHead):\n            entropy_sum += module.entropy(logits)\n        else:\n            continue\n        count += 1\n    return entropy_sum / count"
        }
    ]
}