{
    "summary": "The code defines an attention mechanism with preprocessing methods and StridedAttn class, as well as SelfAttentionLayer and residual MLP layers for transformer models, including operations like concatenation, reshaping, and activation functions.",
    "details": [
        {
            "comment": "The code snippet defines a function named \"attention\" which performs softmax(Q*K)*V operation. It takes query (Q), keys (K), and values (V) as input, along with the data type, mask, additional batch-to-time matrix (extra_btT), maximum length (maxlen), and a flag to check sentinel values (check_sentinel). The function checks if the data types match and then proceeds to perform the softmax operation.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":0-42",
            "content": "\"\"\"\nImplementation of transformer and reshaping-based sparse transformer\n\"\"\"\nimport functools\nimport math\nimport torch as th\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom lib import misc, mlp\nfrom lib import torch_util as tu\nfrom lib import util\nSENTINEL = 0.1337\ndef attention(\n    Q_bte,\n    K_bTe,\n    V_bTe,\n    dtype,\n    mask=True,\n    extra_btT=None,\n    maxlen=None,\n    check_sentinel=False,\n    use_muP_factor=False,\n):\n    \"\"\"\n    performs softmax(Q*K)*V operation\n    t : output (write) time axis, possibly size=1 for just the last timestep\n    T : input (read) time axis\n    t < T is OK\n    'check_sentinel' is used when you want to make it impossible to attend to certain keys.\n    All keys where every value is equal to the constant SENTINEL will be ignored.\n    Currently this is only used by StridedAttn.\n    \"\"\"\n    assert Q_bte.dtype == K_bTe.dtype == dtype, f\"{Q_bte.dtype}, {K_bTe.dtype}, {dtype} must all match\"\n    e = Q_bte.shape[2]\n    if check_sentinel:\n        invalid = (K_bTe == SENTINEL).int().sum(dim=-1) == e"
        },
        {
            "comment": "The code calculates the logits for a multi-head attention mechanism, taking into account masking and optional extra inputs. It applies the necessary transformations to the input tensors and performs the dot product between queries (Q) and keys (K). The result is then normalized using softmax function to obtain the weights (W_btT) for the attention process.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":43-67",
            "content": "        invalid = misc.reshape(invalid, \"b, T\", \"b, 1, T\")\n    if isinstance(mask, th.Tensor):\n        bias = (~mask).float() * -1e9\n    elif mask:\n        bias = get_attn_bias_cached(Q_bte.shape[1], K_bTe.shape[1], maxlen=maxlen, device=Q_bte.device, dtype=th.float32)\n    else:\n        bias = Q_bte.new_zeros((), dtype=th.float32)\n    if extra_btT is not None:\n        bias = bias + extra_btT\n    # Equivalent to bias + (1 / math.sqrt(e)) * th.einsum(\"bte,bpe->btp\", Q_bte, K_bte)\n    # but faster:\n    logit_btT = th.baddbmm(\n        bias,\n        Q_bte.float(),\n        K_bTe.float().transpose(-1, -2),\n        alpha=(1 / e) if use_muP_factor else (1 / math.sqrt(e)),\n    )\n    if check_sentinel:\n        logit_btT = logit_btT - 1e9 * invalid.float()\n    W_btT = th.softmax(logit_btT, dim=2).to(dtype)\n    if callable(V_bTe):\n        # This is used by the sharded video model to defer waiting on\n        # the broadcast of the values until they're needed\n        V_bTe = V_bTe()\n    # th.einsum only lets you use lowercase letters, so 'p' for 'past'"
        },
        {
            "comment": "This code defines an attention mechanism class and a function to split input into heads. The attention mechanism is initialized with parameters such as number of heads, maximum length, and mask. The \"preproc_qkv\" and \"preproc_r\" methods for preprocessing Q, K, V, and R are not implemented yet. The code also includes the \"split_heads\" function to split input into multiple heads.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":68-106",
            "content": "    # means 'T'\n    A_bte = th.einsum(\"btp,bpe->bte\", W_btT, V_bTe)\n    return A_bte\nclass Attn:\n    \"\"\"\n    Defines an attention mechanism\n    All the mechanisms here can be defined by two operations:\n    1. preprocessing Q,K,V,R[=relative attention query]\n        to move axes from embedding dimension to\n        batch dimension, and possibly doing shifts.\n    2. postprocessing the final result to move axes back to embedding\n        axis.\n    \"\"\"\n    def __init__(self, mask, maxlen):\n        self.mask = mask\n        self.maxlen = maxlen\n    def preproc_qkv(self, Q_bte, K_bte, V_bte):\n        raise NotImplementedError\n    def preproc_r(self, R_btn):\n        raise NotImplementedError\ndef split_heads(x_bte, h):\n    b, t, e = x_bte.shape\n    assert e % h == 0, \"Embsize must be divisible by number of heads\"\n    q = e // h\n    x_bthq = x_bte.reshape((b, t, h, q))\n    x_bhtq = misc.transpose(x_bthq, \"bthq\", \"bhtq\")\n    x_Btq = x_bhtq.reshape((b * h, t, q))\n    return x_Btq\nclass All2All(Attn):\n    def __init__(self, nhead, maxlen, mask=True, head_dim=None):"
        },
        {
            "comment": "This code initializes a class with optional nhead and head_dim arguments, and defines preproc_qkv and preproc_r functions to handle input shapes. It also includes a postproc_a function for reshaping the output shape. The _required_padding function checks if padding is needed for certain dimensions.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":107-137",
            "content": "        super().__init__(mask=mask, maxlen=maxlen)\n        assert (nhead is None) != (head_dim is None), \"exactly one of nhead and head_dim must be specified\"\n        self.h = nhead\n        self.head_dim = head_dim\n    def preproc_qkv(self, *xs):\n        q = xs[0].shape[-1]\n        for x in xs:\n            assert x.shape[-1] == q, \"embedding dimensions do not match\"\n        h = self.h or misc.exact_div(q, self.head_dim)\n        postproc = functools.partial(self.postproc_a, h=h)\n        return (postproc, *tuple(split_heads(x, h) for x in xs))\n    def preproc_r(self, R_btn):\n        _, ret = self.preproc_qkv(R_btn)\n        return ret\n    def postproc_a(self, A_Btq, h):\n        B, t, q = A_Btq.shape\n        b = B // h\n        A_bhtq = A_Btq.reshape((b, h, t, q))\n        A_bthq = misc.transpose(A_bhtq, \"bhtq\", \"bthq\")\n        A_bte = A_bthq.reshape((b, t, h * q))\n        return A_bte\ndef _required_padding(dim, target_div):\n    if dim % target_div == 0:\n        return 0\n    else:\n        return target_div - dim % target_div"
        },
        {
            "comment": "This code defines a StridedAttn class which is a subclass of Attn. The __init__ method initializes the number of heads, stride, maximum length, and whether or not to use a mask. The _preproc method preprocesses input data by reshaping, padding if necessary, and defining undo operations for later use. It also checks that the query tensor length is divisible by the maximum length.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":140-162",
            "content": "class StridedAttn(Attn):\n    def __init__(self, nhead, stride, maxlen, mask=True):\n        super().__init__(mask=mask, maxlen=maxlen)\n        self.h = nhead\n        self.stride = stride\n    def _preproc(self, x, name, Q_t=None, Q_pad=None):\n        x, undo = misc.reshape_undo(x, \"b, t*stride, e\", \"b, 1, t, stride*e\", stride=self.stride)\n        if name == \"Q\":\n            Q_pad = _required_padding(x.shape[2], self.maxlen)\n        original_t = x.shape[2]\n        x = F.pad(x, (0, 0, 0, Q_pad), value=SENTINEL)\n        undo = misc.compose_undo(undo, lambda x: x[:, :, :original_t])\n        if name == \"Q\":\n            Q_t = x.shape[2]\n            assert Q_t % self.maxlen == 0, f\"{Q_t} % {self.maxlen} != 0\"\n        else:\n            required_len = Q_t + self.maxlen\n            if x.shape[2] < required_len:\n                x = F.pad(x, (0, 0, required_len - x.shape[2], 0), value=SENTINEL)\n            assert x.shape[2] >= required_len\n            back = x[:, :, -Q_t - self.maxlen : -self.maxlen]\n            front = x[:, :, -Q_t:]"
        },
        {
            "comment": "This code block preprocesses input data for a deep learning model. It performs operations like concatenation, reshaping, and transposition to prepare the data in a suitable format for further processing. The code also includes padding operations to handle data with different dimensions.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":163-191",
            "content": "            x = th.cat([back, front], dim=1)\n        _, _, t, _ = x.shape\n        assert t == Q_t, f\"{t} != {Q_t}\"\n        x, undo = misc.reshape_undo(\n            x,\n            \"b, pad_shift, t*maxlen, stride*h*q\",\n            \"b, pad_shift, t, maxlen, stride, h, q\",\n            maxlen=self.maxlen,\n            h=self.h,\n            stride=self.stride,\n            undo=undo,\n        )\n        x, undo = misc.transpose_undo(x, \"bptmshq\", \"bthspmq\", undo=undo)\n        x, undo = misc.reshape_undo(\n            x,\n            \"b, t, h, stride, pad_shift, maxlen, q\",\n            \"b*t*h*stride, pad_shift*maxlen, q\",\n            undo=undo,\n        )\n        if name == \"Q\":\n            return x, undo, Q_t, Q_pad\n        else:\n            return x\n    def preproc_qkv(self, Q_bte, K_bte, V_bte):\n        pad = _required_padding(Q_bte.shape[1], self.stride)\n        if pad:\n            Q_bte = F.pad(Q_bte, (0, 0, 0, pad), value=SENTINEL)\n            K_bte = F.pad(K_bte, (0, 0, 0, pad), value=SENTINEL) if K_bte is not None else None"
        },
        {
            "comment": "This code performs preprocessing for query (Q), key (K), and value (V) tensors in a transformer model. If any of the tensors are None, they are padded with a sentinel value. The function returns preprocessing results including postprocessing operations (postproc) and prepared Q, K, and V tensors for training or inference.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":192-214",
            "content": "            V_bte = F.pad(V_bte, (0, 0, 0, pad), value=SENTINEL) if V_bte is not None else None\n            undo = lambda x, pad=pad: x[:, :-pad]\n        else:\n            undo = None\n        if K_bte is not None:\n            pad = _required_padding(K_bte.shape[1], self.stride)\n            if pad:\n                K_bte = F.pad(K_bte, (0, 0, pad, 0), value=SENTINEL)\n                V_bte = F.pad(V_bte, (0, 0, pad, 0), value=SENTINEL)\n        assert Q_bte.shape[1] % self.stride == 0\n        assert K_bte is None or K_bte.shape[1] % self.stride == 0\n        assert V_bte is None or V_bte.shape[1] % self.stride == 0\n        Q, postproc, Q_t, Q_pad = self._preproc(Q_bte, \"Q\")\n        postproc = misc.compose_undo(undo, postproc)\n        return (\n            postproc,\n            Q,\n            self._preproc(K_bte, \"K\", Q_t=Q_t, Q_pad=Q_pad) if K_bte is not None else None,\n            self._preproc(V_bte, \"V\", Q_t=Q_t, Q_pad=Q_pad) if V_bte is not None else None,\n        )\n    def preproc_r(self, R_bte):\n        _, R, _, _ = self.preproc_qkv(R_bte, None, None)"
        },
        {
            "comment": "This code defines an `AttentionLayerBase` class that inherits from `nn.Module`. It takes in several parameters such as `attn`, `scale`, `x_size`, `c_size`, `qk_size`, `v_size`, `dtype`, `relattn`, and `seqlens`. Inside the class, it initializes multiple layers using `MultiscaleLinear` with different scales and sizes based on the input parameters. These layers are used for query (Q), key (K), value (V) computations, and projection.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":215-253",
            "content": "        return R\nQ_SCALE = 0.1\nK_SCALE = 0.2\nV_SCALE = 1.0\nPROJ_SCALE = 1.0\nMLP0_SCALE = 1.0\nMLP1_SCALE = 1.0\nR_SCALE = 0.1\nB_SCALE = 0.2\nclass AttentionLayerBase(nn.Module):\n    def __init__(\n        self,\n        *,\n        attn,\n        scale,\n        x_size,\n        c_size,\n        qk_size,\n        v_size,\n        dtype,\n        relattn=False,\n        seqlens=None,\n        separate=False,\n    ):\n        super().__init__()\n        dtype = tu.parse_dtype(dtype)\n        self.attn = attn\n        self.x_size = x_size\n        self.c_size = c_size\n        s = math.sqrt(scale)\n        separgs = dict(seqlens=seqlens, separate=separate)\n        self.q_layer = MultiscaleLinear(x_size, qk_size, name=\"q\", scale=Q_SCALE, dtype=dtype, **separgs)\n        self.k_layer = MultiscaleLinear(c_size, qk_size, name=\"k\", scale=K_SCALE, bias=False, dtype=dtype, **separgs)\n        self.v_layer = MultiscaleLinear(c_size, v_size, name=\"v\", scale=V_SCALE * s, bias=False, dtype=dtype, **separgs)\n        self.proj_layer = MultiscaleLinear(v_size, x_size, name=\"proj\", scale=PROJ_SCALE * s, dtype=dtype, **separgs)"
        },
        {
            "comment": "This code defines a class called SelfAttentionLayer which inherits from AttentionLayerBase. It initializes the relattn attribute, checks if relattn is set, and then initializes r_layer and b_nd if relattn is true. The maxlen, dtype attributes are also initialized based on the input attn. Finally, a relattn_logits method is defined to compute the relative attention logits for the input X_bte and T. Additionally, there are two helper functions: relu, gelu, and none act as activation functions which can be applied to the output of the layer.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":254-290",
            "content": "        self.relattn = relattn\n        maxlen = attn.maxlen\n        assert maxlen > 0 or not attn.mask\n        if self.relattn:\n            nbasis = 10\n            self.r_layer = tu.NormedLinear(x_size, nbasis * attn.h, scale=R_SCALE, dtype=dtype)\n            self.b_nd = nn.Parameter(th.randn(nbasis, maxlen) * B_SCALE)\n        self.maxlen = maxlen\n        self.dtype = dtype\n    def relattn_logits(self, X_bte, T):\n        R_btn = self.r_layer(X_bte).float()\n        R_btn = self.attn.preproc_r(R_btn)\n        t = R_btn.shape[1]\n        D_ntT = util.bandify(self.b_nd, t, T)\n        extra_btT = th.einsum(\"btn,ntp->btp\", R_btn, D_ntT)\n        return extra_btT\ndef quick_gelu(x):\n    return x * th.sigmoid(1.702 * x)\ndef act(actname, x):\n    if actname == \"relu\":\n        return F.relu(x)\n    elif actname == \"gelu\":\n        return quick_gelu(x)\n    elif actname == \"none\":\n        return x\n    else:\n        raise NotImplementedError(actname)\nclass SelfAttentionLayer(AttentionLayerBase):\n    \"\"\"\n    Residual attention layer that takes a single tensor x and has it attend to itself"
        },
        {
            "comment": "This code defines a class constructor for an Attention module. It initializes the object with various parameters and sets up some attributes like normalization layers and cache lengths.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":291-330",
            "content": "    Has the form\n        output = x + f(x)\n    \"\"\"\n    def __init__(\n        self,\n        x_size,\n        attn,\n        scale,\n        dtype=\"float32\",\n        norm=\"layer\",\n        cache_keep_len=None,\n        relattn=False,\n        log_scope=\"sa\",\n        use_muP_factor=False,\n        **kwargs,\n    ):\n        super().__init__(\n            x_size=x_size,\n            c_size=x_size,\n            qk_size=x_size,\n            v_size=x_size,\n            attn=attn,\n            scale=scale,\n            relattn=relattn,\n            dtype=dtype,\n            **kwargs,\n        )\n        self.ln_x = util.get_norm(norm, x_size, dtype=dtype)\n        if cache_keep_len is None:\n            if hasattr(attn, \"cache_keep_len\"):\n                cache_keep_len = attn.cache_keep_len\n            else:\n                if isinstance(attn, StridedAttn):\n                    stride = attn.stride\n                else:\n                    stride = 1\n                cache_keep_len = stride * attn.maxlen\n        self.cache_keep_len = cache_keep_len\n        self.log_scope = log_scope"
        },
        {
            "comment": "This code defines a class with two methods: \"residual\" and \"forward\". The \"residual\" method applies attention to input data, using a self-attention mechanism. It also allows for updating the state based on an argument passed in. The \"forward\" method is a wrapper around the \"residual\" method which also returns the updated state.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":331-358",
            "content": "        self.use_muP_factor = use_muP_factor\n    def residual(self, X_bte, state):\n        X_bte = self.ln_x(X_bte)\n        Q_bte = self.q_layer(X_bte)\n        K_bte = self.k_layer(X_bte)\n        V_bte = self.v_layer(X_bte)\n        if state:\n            state, K_bte, V_bte = self.update_state(state, K_bte, V_bte)\n        postproc_closure, Q_bte, K_bte, V_bte = self.attn.preproc_qkv(Q_bte, K_bte, V_bte)\n        extra_btT = self.relattn_logits(X_bte, K_bte.shape[1]) if self.relattn else None\n        A_bte = attention(\n            Q_bte,\n            K_bte,\n            V_bte,\n            mask=self.attn.mask,\n            extra_btT=extra_btT,\n            maxlen=self.maxlen,\n            dtype=self.dtype,\n            check_sentinel=isinstance(self.attn, StridedAttn),\n            use_muP_factor=self.use_muP_factor,\n        )\n        A_bte = postproc_closure(A_bte)\n        Aproj_bte = self.proj_layer(A_bte)\n        return Aproj_bte, state\n    def forward(self, X_bte, state):\n        R_bte, state = self.residual(X_bte, state)"
        },
        {
            "comment": "The code defines three functions for a neural network:\n1. `forward` performs the forward pass of the network, taking input X_bte and state as arguments, and returns output and updated state.\n2. `stateless_forward` performs a forward pass without considering the state from the previous timestep, only taking input X_bte as an argument.\n3. `update_state` updates the network's internal state based on current and cached keys (K_bte and V_bte), returning the updated cache and full key matrix for the next timestep.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":359-381",
            "content": "        return X_bte + R_bte, state\n    def stateless_forward(self, X_bte):\n        out_bte, _state = self.forward(X_bte, None)\n        return out_bte\n    def update_state(self, state, K_bte, V_bte):\n        def append(prev, new):\n            \"\"\"\n            Given `prev` keys from cache, and `new` keys,\n            returns (cache, full), where\n            - cache goes into the output state, length chosen so that on the\n                next timestep, there are enough cached timesteps to get the full\n                context of lenth self.maxlen.\n            - full is used for the current forward pass, with length chosen so\n                that the first timestep new[:, 0] gets to see a context of\n                self.maxlen.\n            \"\"\"\n            tprev = prev.shape[1]\n            startfull = max(tprev - self.cache_keep_len, 0)\n            full = th.cat([prev[:, startfull:], new], dim=1)\n            outstate = full[:, max(full.shape[1] - (self.cache_keep_len), 0) :]\n            # To see that the preceding slicing is correct, consider the case"
        },
        {
            "comment": "The code initializes a residual MLP layer with a specified size, scale, data type, normalization method, activation function, and a ratio for the multi-layer perceptron (MLP). The class PointwiseLayer inherits from nn.Module and contains an instance of the Linear layer and a normalization layer, as well as a method to apply the MLP at each timestep in the input sequence.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":382-413",
            "content": "            # that maxlen==1. Then `full` only consists of `new`, and\n            # `outstate` is empty\n            return outstate, full\n        instate_K, instate_V = state\n        outstate_K, K_bte = append(instate_K, K_bte)\n        outstate_V, V_bte = append(instate_V, V_bte)\n        assert outstate_K.shape[-2] <= self.cache_keep_len\n        return (outstate_K, outstate_V), K_bte, V_bte\n    def initial_state(self, batchsize, initial_T=0):\n        return (\n            tu.zeros((batchsize, initial_T, self.x_size), dtype=self.dtype),\n            tu.zeros((batchsize, initial_T, self.x_size), dtype=self.dtype),\n        )\n    def empty_state(self):\n        return None\nclass PointwiseLayer(nn.Module):\n    \"\"\"\n    Residual MLP applied at each timestep\n    \"\"\"\n    def __init__(self, x_size, scale, dtype, norm, actname=\"relu\", mlp_ratio=2):\n        super().__init__()\n        s = math.sqrt(scale)\n        self.ln = util.get_norm(norm, x_size, dtype=dtype)\n        self.mlp = mlp.MLP(\n            insize=x_size,\n            nhidlayer=1,"
        },
        {
            "comment": "This code defines a class and a function for creating instances of a module, either for all resolutions or separate instances for each resolution based on the \"separate\" parameter. The class has an initializer that sets up the module's layers and applies scaling to their weights. The forward function performs a residual connection with the module. The _is_separate function checks if a separate instance should be created for the given name, removing it from the set if it should be created separately. The make_maybe_multiscale function creates either one instance or multiple instances of the module based on the separate parameter.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":414-449",
            "content": "            outsize=x_size,\n            hidsize=int(x_size * mlp_ratio),\n            hidactiv=functools.partial(act, actname),\n            dtype=dtype,\n        )\n        self.mlp.layers[0].weight.data *= MLP0_SCALE * s\n        self.mlp.layers[1].weight.data *= MLP1_SCALE * s\n    def residual(self, x):\n        x = self.ln(x)\n        x = self.mlp(x)\n        return x\n    def forward(self, x):\n        return x + self.residual(x)\ndef _is_separate(sep, name):\n    if isinstance(sep, bool):\n        return sep\n    assert isinstance(sep, set)\n    if name in sep:\n        sep.remove(name)\n        return True\n    else:\n        return False\ndef make_maybe_multiscale(make_fn, *args, seqlens, separate, name, **kwargs):\n    \"\"\"\n    This function either creates one instance of a module or creates\n    a separate instance of the module for each resolution of the image,\n    determined by the `separate` parameter. We create separate modules\n    if `separate` is True or if `separate` is a set containing `name`.\n    \"\"\"\n    if _is_separate(separate, name):"
        },
        {
            "comment": "This code defines a function `SplitCallJoin` that takes a list of modules and sequence lengths as inputs. It initializes the `SplitCallJoin` class, which splits the input tensor into multiple smaller tensors based on the sequence lengths, applies each module in parallel, and then concatenates the results back together. The function also defines two partial functions `MultiscaleLinear` and `MultiscalePointwise` using `functools.partial` to create variants of `make_maybe_multiscale` for `tu.NormedLinear` and `PointwiseLayer`, respectively.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/xf.py\":450-478",
            "content": "        modules = [make_fn(*args, **kwargs) for _ in seqlens]\n        return SplitCallJoin(modules, seqlens)\n    else:\n        return make_fn(*args, **kwargs)\nclass SplitCallJoin(nn.Module):\n    def __init__(self, mods, seqlens):\n        super().__init__()\n        self.mods = nn.ModuleList(mods)\n        self.seqlens = seqlens\n    def forward(self, x):\n        tl = sum(self.seqlens)\n        x, undo = misc.reshape_undo(x, \"..., z*tl, e\", \"..., z, tl, e\", tl=tl)\n        x = list(th.split(x, self.seqlens, dim=-2))\n        new_x = []\n        for x, mod in misc.safezip(x, self.mods):\n            x, this_undo = misc.reshape_undo(x, \"..., z, l, e\", \"..., z*l, e\")\n            x = mod(x)\n            x = this_undo(x)\n            new_x.append(x)\n        x = th.cat(new_x, dim=-2)\n        x = undo(x)\n        return x\nMultiscaleLinear = functools.partial(make_maybe_multiscale, tu.NormedLinear)\nMultiscalePointwise = functools.partial(make_maybe_multiscale, PointwiseLayer)"
        }
    ]
}