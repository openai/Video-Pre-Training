{
    "summary": "IDMAgent is a Minecraft action predictor using the IDM model, featuring functions for initializing, loading weights, resetting state, and processing video frames. It converts policy output to MineRL format for agent state prediction.",
    "details": [
        {
            "comment": "IDMAgent is a class representing an agent that uses the inverse dynamics model (IDM) to predict Minecraft player actions in videos. It has an action mapper and is initialized with idm_net_kwargs, pi_head_kwargs, and device (default device type if None).",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/inverse_dynamics_model.py\":0-32",
            "content": "import numpy as np\nimport torch as th\nimport cv2\nfrom gym3.types import DictType\nfrom gym import spaces\nfrom lib.action_mapping import CameraHierarchicalMapping, IDMActionMapping\nfrom lib.actions import ActionTransformer\nfrom lib.policy import InverseActionPolicy\nfrom lib.torch_util import default_device_type, set_default_torch_device\nfrom agent import resize_image, AGENT_RESOLUTION\nACTION_TRANSFORMER_KWARGS = dict(\n    camera_binsize=2,\n    camera_maxval=10,\n    camera_mu=10,\n    camera_quantization_scheme=\"mu_law\",\n)\nclass IDMAgent:\n    \"\"\"\n    Sugarcoating on the inverse dynamics model (IDM) used to predict actions Minecraft players take in videos.\n    Functionally same as MineRLAgent.\n    \"\"\"\n    def __init__(self, idm_net_kwargs, pi_head_kwargs, device=None):\n        if device is None:\n            device = default_device_type()\n        self.device = th.device(device)\n        # Set the default torch device for underlying code as well\n        set_default_torch_device(self.device)\n        self.action_mapper = IDMActionMapping(n_camera_bins=11)"
        },
        {
            "comment": "Function: __init__\n- Initializes the agent with specified parameters and loads initial weights.\n\nFunction: load_weights\n- Loads model weights from a path and resets the hidden state of the agent.\n\nFunction: reset\n- Resets the agent to its initial state by setting the hidden state to the result of the policy's initial_state method with an argument of 1.\n\nFunction:_video_obs_to_agent\n- Takes a list of video frames, resizes them to AGENT_RESOLUTION, and returns the processed images for the agent to use.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/inverse_dynamics_model.py\":33-55",
            "content": "        action_space = self.action_mapper.get_action_space_update()\n        action_space = DictType(**action_space)\n        self.action_transformer = ActionTransformer(**ACTION_TRANSFORMER_KWARGS)\n        idm_policy_kwargs = dict(idm_net_kwargs=idm_net_kwargs, pi_head_kwargs=pi_head_kwargs, action_space=action_space)\n        self.policy = InverseActionPolicy(**idm_policy_kwargs).to(device)\n        self.hidden_state = self.policy.initial_state(1)\n        self._dummy_first = th.from_numpy(np.array((False,))).to(device)\n    def load_weights(self, path):\n        \"\"\"Load model weights from a path, and reset hidden state\"\"\"\n        self.policy.load_state_dict(th.load(path, map_location=self.device), strict=False)\n        self.reset()\n    def reset(self):\n        \"\"\"Reset agent to initial state (i.e., reset hidden state)\"\"\"\n        self.hidden_state = self.policy.initial_state(1)\n    def _video_obs_to_agent(self, video_frames):\n        imgs = [resize_image(frame, AGENT_RESOLUTION) for frame in video_frames]\n        # Add time and batch dim"
        },
        {
            "comment": "Code snippet:\n```python\n    def _agent_action_to_env(self, agent_action):\n        \"\"\"Turn output from policy into action for MineRL\"\"\"\n        # Manual conversion to numpy is important.\n        action = {\n            \"buttons\": agent_action[\"buttons\"].cpu().numpy(),\n            \"camera\": agent_action[\"camera\"].cpu().numpy()\n        }\n```\nComment: Converts policy output to MineRL action format using manual numpy conversion",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/inverse_dynamics_model.py\":56-78",
            "content": "        imgs = np.stack(imgs)[None]\n        agent_input = {\"img\": th.from_numpy(imgs).to(self.device)}\n        return agent_input\n    def _agent_action_to_env(self, agent_action):\n        \"\"\"Turn output from policy into action for MineRL\"\"\"\n        # This is quite important step (for some reason).\n        # For the sake of your sanity, remember to do this step (manual conversion to numpy)\n        # before proceeding. Otherwise, your agent might be a little derp.\n        action = {\n            \"buttons\": agent_action[\"buttons\"].cpu().numpy(),\n            \"camera\": agent_action[\"camera\"].cpu().numpy()\n        }\n        minerl_action = self.action_mapper.to_factored(action)\n        minerl_action_transformed = self.action_transformer.policy2env(minerl_action)\n        return minerl_action_transformed\n    def predict_actions(self, video_frames):\n        \"\"\"\n        Predict actions for a sequence of frames.\n        `video_frames` should be of shape (N, H, W, C).\n        Returns MineRL action dict, where each action head"
        },
        {
            "comment": "This function takes video frames as input, converts them to agent input, and uses the policy model to predict actions. It also maintains an internal hidden state for tracking the agent's state and can be reset using `reset()`.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/inverse_dynamics_model.py\":79-94",
            "content": "        has shape (N, ...).\n        Agent's hidden state is tracked internally. To reset it,\n        call `reset()`.\n        \"\"\"\n        agent_input = self._video_obs_to_agent(video_frames)\n        # The \"first\" argument could be used to reset tell episode\n        # boundaries, but we are only using this for predicting (for now),\n        # so we do not hassle with it yet.\n        dummy_first = th.zeros((video_frames.shape[0], 1)).to(self.device)\n        predicted_actions, self.hidden_state, _ = self.policy.predict(\n            agent_input, first=dummy_first, state_in=self.hidden_state,\n            deterministic=True\n        )\n        predicted_minerl_action = self._agent_action_to_env(predicted_actions)\n        return predicted_minerl_action"
        }
    ]
}