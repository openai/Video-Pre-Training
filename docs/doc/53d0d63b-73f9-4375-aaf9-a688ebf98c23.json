{
    "summary": "The code initializes a game dictionary, defines model actions, manages camera resets, handles inputs, loads weights, captures video input, reads JSON data, and displays IDM predictions on a video stream with OpenCV functions.",
    "details": [
        {
            "comment": "This code is initializing a dictionary mapping keyboard button names to their respective actions in the game. The code is used for controlling the character's movements and actions in the game environment using keyboard inputs.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":0-35",
            "content": "# NOTE: this is _not_ the original code of IDM!\n# As such, while it is close and seems to function well,\n# its performance might be bit off from what is reported\n# in the paper.\nfrom argparse import ArgumentParser\nimport pickle\nimport cv2\nimport numpy as np\nimport json\nimport torch as th\nfrom agent import ENV_KWARGS\nfrom inverse_dynamics_model import IDMAgent\nKEYBOARD_BUTTON_MAPPING = {\n    \"key.keyboard.escape\" :\"ESC\",\n    \"key.keyboard.s\" :\"back\",\n    \"key.keyboard.q\" :\"drop\",\n    \"key.keyboard.w\" :\"forward\",\n    \"key.keyboard.1\" :\"hotbar.1\",\n    \"key.keyboard.2\" :\"hotbar.2\",\n    \"key.keyboard.3\" :\"hotbar.3\",\n    \"key.keyboard.4\" :\"hotbar.4\",\n    \"key.keyboard.5\" :\"hotbar.5\",\n    \"key.keyboard.6\" :\"hotbar.6\",\n    \"key.keyboard.7\" :\"hotbar.7\",\n    \"key.keyboard.8\" :\"hotbar.8\",\n    \"key.keyboard.9\" :\"hotbar.9\",\n    \"key.keyboard.e\" :\"inventory\",\n    \"key.keyboard.space\" :\"jump\",\n    \"key.keyboard.a\" :\"left\",\n    \"key.keyboard.d\" :\"right\",\n    \"key.keyboard.left.shift\" :\"sneak\",\n    \"key.keyboard.left.control\" :\"sprint\","
        },
        {
            "comment": "This code defines a set of actions that the model should predict for a given video. It also includes a template action and a message to be displayed with a cv2 window. The CAMERA_SCALER is used for mapping sensitivity from recorded Java code to the one used in the model. The json_action_to_env_action function converts a JSON action into a MineRL action.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":36-85",
            "content": "    \"key.keyboard.f\" :\"swapHands\",\n}\n# Template action\nNOOP_ACTION = {\n    \"ESC\": 0,\n    \"back\": 0,\n    \"drop\": 0,\n    \"forward\": 0,\n    \"hotbar.1\": 0,\n    \"hotbar.2\": 0,\n    \"hotbar.3\": 0,\n    \"hotbar.4\": 0,\n    \"hotbar.5\": 0,\n    \"hotbar.6\": 0,\n    \"hotbar.7\": 0,\n    \"hotbar.8\": 0,\n    \"hotbar.9\": 0,\n    \"inventory\": 0,\n    \"jump\": 0,\n    \"left\": 0,\n    \"right\": 0,\n    \"sneak\": 0,\n    \"sprint\": 0,\n    \"swapHands\": 0,\n    \"camera\": np.array([0, 0]),\n    \"attack\": 0,\n    \"use\": 0,\n    \"pickItem\": 0,\n}\nMESSAGE = \"\"\"\nThis script will take a video, predict actions for its frames and\nand show them with a cv2 window.\nPress any button the window to proceed to the next frame.\n\"\"\"\n# Matches a number in the MineRL Java code regarding sensitivity\n# This is for mapping from recorded sensitivity to the one used in the model\nCAMERA_SCALER = 360.0 / 2400.0\ndef json_action_to_env_action(json_action):\n    \"\"\"\n    Converts a json action into a MineRL action.\n    Returns (minerl_action, is_null_action)\n    \"\"\"\n    # This might be slow...\n    env_action = NOOP_ACTION.copy()"
        },
        {
            "comment": "This code resets the camera action to avoid overriding other actions and handles keyboard and mouse inputs for the environment.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":86-111",
            "content": "    # As a safeguard, make camera action again so we do not override anything\n    env_action[\"camera\"] = np.array([0, 0])\n    is_null_action = True\n    keyboard_keys = json_action[\"keyboard\"][\"keys\"]\n    for key in keyboard_keys:\n        # You can have keys that we do not use, so just skip them\n        # NOTE in original training code, ESC was removed and replaced with\n        #      \"inventory\" action if GUI was open.\n        #      Not doing it here, as BASALT uses ESC to quit the game.\n        if key in KEYBOARD_BUTTON_MAPPING:\n            env_action[KEYBOARD_BUTTON_MAPPING[key]] = 1\n            is_null_action = False\n    mouse = json_action[\"mouse\"]\n    camera_action = env_action[\"camera\"]\n    camera_action[0] = mouse[\"dy\"] * CAMERA_SCALER\n    camera_action[1] = mouse[\"dx\"] * CAMERA_SCALER\n    if mouse[\"dx\"] != 0 or mouse[\"dy\"] != 0:\n        is_null_action = False\n    else:\n        if abs(camera_action[0]) > 180:\n            camera_action[0] = 0\n        if abs(camera_action[1]) > 180:\n            camera_action[1] = 0"
        },
        {
            "comment": "This code handles mouse button events and initializes an inverse dynamics model agent for a game. It loads the agent's weights from a file, captures video input, and reads a JSON file containing game data.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":113-142",
            "content": "    mouse_buttons = mouse[\"buttons\"]\n    if 0 in mouse_buttons:\n        env_action[\"attack\"] = 1\n        is_null_action = False\n    if 1 in mouse_buttons:\n        env_action[\"use\"] = 1\n        is_null_action = False\n    if 2 in mouse_buttons:\n        env_action[\"pickItem\"] = 1\n        is_null_action = False\n    return env_action, is_null_action\ndef main(model, weights, video_path, json_path, n_batches, n_frames):\n    print(MESSAGE)\n    agent_parameters = pickle.load(open(model, \"rb\"))\n    net_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n    agent = IDMAgent(idm_net_kwargs=net_kwargs, pi_head_kwargs=pi_head_kwargs)\n    agent.load_weights(weights)\n    required_resolution = ENV_KWARGS[\"resolution\"]\n    cap = cv2.VideoCapture(video_path)\n    json_index = 0\n    with open(json_path) as json_file:\n        json_lines = json_file.readlines()\n        json_data = \"[\" + \",\".join(json_lines) + \"]\""
        },
        {
            "comment": "Loading and preprocessing video frames, converting actions from JSON to environment actions, predicting actions using the agent model, and displaying predictions on video frames.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":143-169",
            "content": "        json_data = json.loads(json_data)\n    for _ in range(n_batches):\n        th.cuda.empty_cache()\n        print(\"=== Loading up frames ===\")\n        frames = []\n        recorded_actions = []\n        for _ in range(n_frames):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            assert frame.shape[0] == required_resolution[1] and frame.shape[1] == required_resolution[0], \"Video must be of resolution {}\".format(required_resolution)\n            # BGR -> RGB\n            frames.append(frame[..., ::-1])\n            env_action, _ = json_action_to_env_action(json_data[json_index])\n            recorded_actions.append(env_action)\n            json_index += 1\n        frames = np.stack(frames)\n        print(\"=== Predicting actions ===\")\n        predicted_actions = agent.predict_actions(frames)\n        for i in range(n_frames):\n            frame = frames[i]\n            recorded_action = recorded_actions[i]\n            cv2.putText(\n                frame,\n                f\"name: prediction (true)\","
        },
        {
            "comment": "The code is displaying IDM model predictions on a video stream, with text labels for each action. It uses OpenCV's putText function to draw the labels on the frame and then displays the resulting image using cv2.imshow and waitKey functions. The code also takes arguments for weights and model files required to load the model.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":170-196",
            "content": "                (10, 10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.4,\n                (255, 255, 255),\n                1\n            )\n            for y, (action_name, action_array) in enumerate(predicted_actions.items()):\n                current_prediction = action_array[0, i]\n                cv2.putText(\n                    frame,\n                    f\"{action_name}: {current_prediction} ({recorded_action[action_name]})\",\n                    (10, 25 + y * 12),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.35,\n                    (255, 255, 255),\n                    1\n                )\n            # RGB -> BGR again...\n            cv2.imshow(\"MineRL IDM model predictions\", frame[..., ::-1])\n            cv2.waitKey(0)\n    cv2.destroyAllWindows()\nif __name__ == \"__main__\":\n    parser = ArgumentParser(\"Run IDM on MineRL recordings.\")\n    parser.add_argument(\"--weights\", type=str, required=True, help=\"Path to the '.weights' file to be loaded.\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the '.model' file to be loaded.\")"
        },
        {
            "comment": "This code sets up command line arguments for video path, JSONL file path, number of frames to process at a time, and the number of batches to process for visualization. It then parses these arguments into \"args\" and calls the main function with these arguments.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/run_inverse_dynamics_model.py\":197-204",
            "content": "    parser.add_argument(\"--video-path\", type=str, required=True, help=\"Path to a .mp4 file (Minecraft recording).\")\n    parser.add_argument(\"--jsonl-path\", type=str, required=True, help=\"Path to a .jsonl file (Minecraft recording).\")\n    parser.add_argument(\"--n-frames\", type=int, default=128, help=\"Number of frames to process at a time.\")\n    parser.add_argument(\"--n-batches\", type=int, default=10, help=\"Number of batches (n-frames) to process for visualization.\")\n    args = parser.parse_args()\n    main(args.model, args.weights, args.video_path, args.jsonl_path, args.n_batches, args.n_frames)"
        }
    ]
}