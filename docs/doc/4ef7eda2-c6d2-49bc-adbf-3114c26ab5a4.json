{
    "summary": "The code imports necessary libraries, defines parameters, and creates an agent object for policy-based actor-critic model training in a behavioral cloning task. It trains the model using batches of data, updates weights, and reports average loss at specified intervals.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines constants for basic behavioral cloning using gradient accumulation. It uses a smaller GPU, and it's not the original code used for VPT but serves to illustrate fine-tuning models with specific processing steps. The code specifies the number of epochs, batch size, number of workers, and device for training.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":0-33",
            "content": "# Basic behavioural cloning\n# Note: this uses gradient accumulation in batches of ones\n#       to perform training.\n#       This will fit inside even smaller GPUs (tested on 8GB one),\n#       but is slow.\n# NOTE: This is _not_ the original code used for VPT!\n#       This is merely to illustrate how to fine-tune the models and includes\n#       the processing steps used.\n# This will likely be much worse than what original VPT did:\n# we are not training on full sequences, but only one step at a time to save VRAM.\nfrom argparse import ArgumentParser\nimport pickle\nimport time\nimport gym\nimport minerl\nimport torch as th\nimport numpy as np\nfrom agent import PI_HEAD_KWARGS, MineRLAgent\nfrom data_loader import DataLoader\nfrom lib.tree_util import tree_map\nEPOCHS = 2\n# Needs to be <= number of videos\nBATCH_SIZE = 8\n# Ideally more than batch size to create\n# variation in datasets (otherwise, you will\n# get a bunch of consecutive samples)\n# Decrease this (and batch_size) if you run out of memory\nN_WORKERS = 12\nDEVICE = \"cuda\""
        },
        {
            "comment": "Load model parameters from file, define environment settings, and create agent object with defined policy and head parameters.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":35-59",
            "content": "LOSS_REPORT_RATE = 100\nLEARNING_RATE = 0.000181\nWEIGHT_DECAY = 0.039428\nMAX_GRAD_NORM = 5.0\ndef load_model_parameters(path_to_model_file):\n    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n    return policy_kwargs, pi_head_kwargs\ndef behavioural_cloning_train(data_dir, in_model, in_weights, out_weights):\n    agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n    # To create model with the right environment.\n    # All basalt environments have the same settings, so any of them works here\n    env = gym.make(\"MineRLBasaltFindCave-v0\")\n    agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n    agent.load_weights(in_weights)\n    env.close()\n    policy = agent.policy\n    trainable_parameters = policy.parameters()"
        },
        {
            "comment": "Setting up optimizer, data loader, and initializing variables for training.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":61-90",
            "content": "    # Parameters taken from the OpenAI VPT paper\n    optimizer = th.optim.Adam(\n        trainable_parameters,\n        lr=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY\n    )\n    data_loader = DataLoader(\n        dataset_dir=data_dir,\n        n_workers=N_WORKERS,\n        batch_size=BATCH_SIZE,\n        n_epochs=EPOCHS\n    )\n    start_time = time.time()\n    # Keep track of the hidden state per episode/trajectory.\n    # DataLoader provides unique id for each episode, which will\n    # be different even for the same trajectory when it is loaded\n    # up again\n    episode_hidden_states = {}\n    dummy_first = th.from_numpy(np.array((False,))).to(DEVICE)\n    loss_sum = 0\n    for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n        batch_loss = 0\n        for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n            agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n            if agent_action is None:\n                # Action was null"
        },
        {
            "comment": "The code is setting up the environment for a policy-based actor-critic model in a behavioral cloning task. It assigns the hidden state for the episode, gets the output for the observation, calculates the log probability of the action, and updates the agent's state.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":91-113",
            "content": "                continue\n            agent_obs = agent._env_obs_to_agent({\"pov\": image})\n            if episode_id not in episode_hidden_states:\n                # TODO need to clean up this hidden state after worker is done with the work item.\n                #      Leaks memory, but not tooooo much at these scales (will be a problem later).\n                episode_hidden_states[episode_id] = policy.initial_state(1)\n            agent_state = episode_hidden_states[episode_id]\n            pi_distribution, v_prediction, new_agent_state = policy.get_output_for_observation(\n                agent_obs,\n                agent_state,\n                dummy_first\n            )\n            log_prob  = policy.get_logprob_of_action(pi_distribution, agent_action)\n            # Make sure we do not try to backprop through sequence\n            # (fails with current accumulation)\n            new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n            episode_hidden_states[episode_id] = new_agent_state\n            # Finally, update the agent to increase the probability of the"
        },
        {
            "comment": "The code is training a policy model using behavioral cloning on batches of data. It calculates the batch loss, applies gradients and updates weights, saves state dictionary to a specified output file, and reports the average loss every LOSS_REPORT_RATE batches. The inputs are the path to the directory containing recordings for training, the path to the model file to be fine-tuned, and the path to the weights file to be fine-tuned.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":114-138",
            "content": "            # taken action.\n            # Remember to take mean over batch losses\n            loss = -log_prob / BATCH_SIZE\n            batch_loss += loss.item()\n            loss.backward()\n        th.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n        optimizer.step()\n        optimizer.zero_grad()\n        loss_sum += batch_loss\n        if batch_i % LOSS_REPORT_RATE == 0:\n            time_since_start = time.time() - start_time\n            print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {loss_sum / LOSS_REPORT_RATE:.4f}\")\n            loss_sum = 0\n    state_dict = policy.state_dict()\n    th.save(state_dict, out_weights)\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--data-dir\", type=str, required=True, help=\"Path to the directory containing recordings to be trained on\")\n    parser.add_argument(\"--in-model\", required=True, type=str, help=\"Path to the .model file to be finetuned\")\n    parser.add_argument(\"--in-weights\", required=True, type=str, help=\"Path to the .weights file to be finetuned\")"
        },
        {
            "comment": "The code adds an argument for the output weights path and parses the command line arguments, then calls the behavioral cloning training function.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/behavioural_cloning.py\":139-142",
            "content": "    parser.add_argument(\"--out-weights\", required=True, type=str, help=\"Path where finetuned weights will be saved\")\n    args = parser.parse_args()\n    behavioural_cloning_train(args.data_dir, args.in_model, args.in_weights, args.out_weights)"
        }
    ]
}