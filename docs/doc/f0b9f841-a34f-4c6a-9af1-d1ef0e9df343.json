{
    "summary": "The code handles libraries, device defaults, tensor and layer functions, CUDA availability, LayerNorm creation, dimension flattening, sequential application of layers, parameter loading from multiple paths, and function state saving. The function takes a dtype string and converts it to PyTorch tensor data type, with an index function for batched broadcasting 'xi' along specified 'gather_dim'.",
    "details": [
        {
            "comment": "This code imports various libraries, defines a function to convert context managers into decorators, checks if CUDA is available, sets the default device as either CUDA or CPU depending on availability, and then defines functions for creating tensors with zeros.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":0-56",
            "content": "import functools\nimport itertools\nimport math\nimport os\nimport pickle\nimport re\nimport subprocess\nimport tempfile\nfrom contextlib import contextmanager\nfrom hashlib import md5, sha1\nimport numpy as np\nimport torch as th\nimport torch.distributed as dist\nimport torch.distributions as dis\nimport torch.nn.functional as F\nfrom torch import nn\nimport lib.tree_util as tree_util\nfrom lib import misc\ndef contextmanager_to_decorator(cm):\n    def decorator(fn):\n        @functools.wraps(fn)\n        def newfn(*args, **kwargs):\n            with cm():\n                return fn(*args, **kwargs)\n        return newfn\n    return decorator\ndef have_cuda():\n    return th.has_cuda\ndef default_device_type():\n    return \"cuda\" if have_cuda() else \"cpu\"\nno_grad = contextmanager_to_decorator(th.no_grad)\nDEFAULT_DEVICE = th.device(type=default_device_type())\ndef set_default_torch_device(device):\n    global DEFAULT_DEVICE\n    DEFAULT_DEVICE = th.device(device)\ndef dev():\n    return DEFAULT_DEVICE\ndef zeros(*args, **kwargs):\n    return th.zeros(*args, **kwargs, device=dev())"
        },
        {
            "comment": "Code defines functions for creating normalized Linear layers, F16 linear and LayerNorm modules. It also includes a utility function to create Tensor objects on the device specified by dev() function.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":59-95",
            "content": "def ones(*args, **kwargs):\n    return th.ones(*args, **kwargs, device=dev())\ndef arange(*args, **kwargs):\n    return th.arange(*args, **kwargs, device=dev())\ndef NormedLinear(*args, scale=1.0, dtype=th.float32, **kwargs):\n    \"\"\"\n    nn.Linear but with normalized fan-in init\n    \"\"\"\n    dtype = parse_dtype(dtype)\n    if dtype == th.float32:\n        out = nn.Linear(*args, **kwargs)\n    elif dtype == th.float16:\n        out = LinearF16(*args, **kwargs)\n    else:\n        raise ValueError(dtype)\n    out.weight.data *= scale / out.weight.norm(dim=1, p=2, keepdim=True)\n    if kwargs.get(\"bias\", True):\n        out.bias.data *= 0\n    return out\nclass LinearF16(nn.Linear):\n    def forward(self, x):\n        return F.linear(x, self.weight.half(), self.bias.half() if self.bias is not None else None)\nclass LayerNormF16(nn.LayerNorm):\n    def forward(self, x):\n        return F.layer_norm(x, self.normalized_shape, self.weight.half(), self.bias.half(), self.eps)\ndef LayerNorm(*args, dtype=th.float32, **kwargs):\n    dtype = parse_dtype(dtype)"
        },
        {
            "comment": "- Code snippets from \"Video-Pre-Training/lib/torch_util.py\":\n- 96-130: LayerNorm creation depending on dtype (float32, float16), sets weight no_scale to True.\n- flatten_image: Flattens the last three dimensions of a tensor.\n- sequential: Applies layers in order to input tensor, returns final result.\n- load_average_with_metadata: Loads models from multiple paths and averages their parameters.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":96-130",
            "content": "    if dtype == th.float32:\n        out = nn.LayerNorm(*args, **kwargs)\n    elif dtype == th.float16:\n        out = LayerNormF16(*args, **kwargs)\n    else:\n        raise ValueError(dtype)\n    out.weight.no_scale = True\n    return out\ndef flatten_image(x):\n    \"\"\"\n    Flattens last three dims\n    \"\"\"\n    *batch_shape, h, w, c = x.shape\n    return x.reshape((*batch_shape, h * w * c))\ndef sequential(layers, x, *args, diag_name=None, use_checkpoint=False):\n    for (i, layer) in enumerate(layers):\n        x = layer(x, *args)\n    return x\n@no_grad\ndef load_average_with_metadata(paths, overrides):\n    n_models = len(paths)\n    model, metadata = load_with_metadata(paths[0], overrides=overrides)\n    for p in model.parameters():\n        p.mul_(1 / n_models)\n    for p in paths[1:]:\n        new_model, _ = load_with_metadata(p, overrides=overrides)\n        for (n1, p1), (n2, p2) in misc.safezip(model.named_parameters(), new_model.named_parameters()):\n            assert n1 == n2, f\"names {n1} and {n2} don't match\"\n            p1.add_(p2.mul_(1 / n_models))"
        },
        {
            "comment": "The code defines a decorator, save_kwargs, that allows saving the function and its arguments used to create a PyTorch module, enabling later restoration of the model state. It also includes a utility function, parse_dtype, for converting data types into their equivalent PyTorch dtype objects.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":131-164",
            "content": "    return model, metadata\ndef save_kwargs(fn):\n    \"\"\"\n    This decorator passes through the user-provided kwargs and adds one more, called\n    save_kwargs, mapping to {\"create_fn\" : name_of_decorated_fn, \"kwargs\" : other_kwargs}\n    You put on this decorator on a function that creates a pytorch module. This will\n    save the kwargs and the function that was used to create the module.\n    This lets us restore the model state later.\n    \"\"\"\n    @functools.wraps(fn)\n    def wrapper(**kwargs):\n        if \"save_kwargs\" in kwargs:\n            return fn(**kwargs)\n        else:\n            sk = {**kwargs, \"create_fn\": f\"{fn.__module__}:{fn.__name__}\"}\n            return fn(save_kwargs=sk, **kwargs)\n    return wrapper\ndef parse_dtype(x):\n    if isinstance(x, th.dtype):\n        return x\n    elif isinstance(x, str):\n        if x == \"float32\" or x == \"float\":\n            return th.float32\n        elif x == \"float64\" or x == \"double\":\n            return th.float64\n        elif x == \"float16\" or x == \"half\":\n            return th.float16"
        },
        {
            "comment": "This function parses a dtype string and returns the corresponding PyTorch tensor data type. It also provides an index function for batched, broadcasting index of x along dimension i.ndim. The index function ensures that the input shape is compatible with the tensor shape and expands or gathers the tensor accordingly.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":165-198",
            "content": "        elif x == \"uint8\":\n            return th.uint8\n        elif x == \"int8\":\n            return th.int8\n        elif x == \"int16\" or x == \"short\":\n            return th.int16\n        elif x == \"int32\" or x == \"int\":\n            return th.int32\n        elif x == \"int64\" or x == \"long\":\n            return th.int64\n        elif x == \"bool\":\n            return th.bool\n        else:\n            raise ValueError(f\"cannot parse {x} as a dtype\")\n    else:\n        raise TypeError(f\"cannot parse {type(x)} as dtype\")\ndef index(x, i):\n    \"\"\"\n    Batched, broadcasting index of x along dimension i.ndim.\n    For example, if x has shape (1, 2, 3, 4, 5) and i has shape (1, 1, 3)\n    then the result has shape (1, 2, 3, 5) and each value in i must be between 0 and 3.\n    \"\"\"\n    assert x.ndim >= i.ndim + 1\n    gather_dim = i.ndim\n    while i.ndim < x.ndim:\n        i = i.unsqueeze(-1)\n    expand_shape = list(x.shape)\n    expand_shape[gather_dim] = 1\n    i = i.expand(*expand_shape)\n    xi = th.gather(x, gather_dim, i)\n    assert xi.shape[gather_dim] == 1"
        },
        {
            "comment": "This function is squeezing the dimensions of the tensor 'xi' based on the value in 'gather_dim'.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/torch_util.py\":199-199",
            "content": "    return xi.squeeze(gather_dim)"
        }
    ]
}