{
    "summary": "The code includes classes for image preprocessing, reinforcement learning with optional parameters, and a MinecraftAgentPolicy network using PyTorch neural networks. It handles policy decisions, actions, and probabilities in the policy network while utilizing 3D convolution layers for reinforcement learning models.",
    "details": [
        {
            "comment": "This code defines a class called \"ImgPreprocessing\" which is used to normalize incoming images. It has an optional parameter for img_statistics, a remote path to a npz file containing mean and std image values. If img_statistics is provided, the images are normalized using those values. Otherwise, if no img_statistics is provided but scale_img is True, the images are scaled by 1/255. The class inherits from nn.Module which allows it to be used as part of a neural network in PyTorch. The code also initializes an instance variable self.img_mean to None.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":0-31",
            "content": "from copy import deepcopy\nfrom email import policy\nfrom typing import Dict, Optional\nimport numpy as np\nimport torch as th\nfrom gym3.types import DictType\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom lib.action_head import make_action_head\nfrom lib.action_mapping import CameraHierarchicalMapping\nfrom lib.impala_cnn import ImpalaCNN\nfrom lib.normalize_ewma import NormalizeEwma\nfrom lib.scaled_mse_head import ScaledMSEHead\nfrom lib.tree_util import tree_map\nfrom lib.util import FanInInitReLULayer, ResidualRecurrentBlocks\nfrom lib.misc import transpose\nclass ImgPreprocessing(nn.Module):\n    \"\"\"Normalize incoming images.\n    :param img_statistics: remote path to npz file with a mean and std image. If specified\n        normalize images using this.\n    :param scale_img: If true and img_statistics not specified, scale incoming images by 1/255.\n    \"\"\"\n    def __init__(self, img_statistics: Optional[str] = None, scale_img: bool = True):\n        super().__init__()\n        self.img_mean = None\n        if img_statistics is not None:"
        },
        {
            "comment": "This code defines a class named \"ImgObsProcess\" which is a subclass of nn.Module used for preprocessing images and observations. It loads image statistics (mean and std) from a file or uses default scale values based on the provided \"scale_img\". The forward method normalizes the input image by subtracting mean and dividing by std if img_mean and img_std are not None, otherwise it divides by ob_scale. The class also accepts parameters for creating an instance of ImpalaCNN followed by a linear layer.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":32-61",
            "content": "            img_statistics = dict(**np.load(img_statistics))\n            self.img_mean = nn.Parameter(th.Tensor(img_statistics[\"mean\"]), requires_grad=False)\n            self.img_std = nn.Parameter(th.Tensor(img_statistics[\"std\"]), requires_grad=False)\n        else:\n            self.ob_scale = 255.0 if scale_img else 1.0\n    def forward(self, img):\n        x = img.to(dtype=th.float32)\n        if self.img_mean is not None:\n            x = (x - self.img_mean) / self.img_std\n        else:\n            x = x / self.ob_scale\n        return x\nclass ImgObsProcess(nn.Module):\n    \"\"\"ImpalaCNN followed by a linear layer.\n    :param cnn_outsize: impala output dimension\n    :param output_size: output size of the linear layer.\n    :param dense_init_norm_kwargs: kwargs for linear FanInInitReLULayer\n    :param init_norm_kwargs: kwargs for 2d and 3d conv FanInInitReLULayer\n    \"\"\"\n    def __init__(\n        self,\n        cnn_outsize: int,\n        output_size: int,\n        dense_init_norm_kwargs: Dict = {},\n        init_norm_kwargs: Dict = {},"
        },
        {
            "comment": "This code defines a class called \"Policy\" with an initializer and a forward method. The initializer takes various parameters, creates an ImpalaCNN and FanInInitReLULayer layers, and initializes the CNN layer with given parameters. The forward method applies these layers to input images and returns the result.\nThe code also defines a class called \"MinecraftPolicy\" that extends nn.Module and takes recurrence_type as parameter. It doesn't have any methods defined.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":62-90",
            "content": "        **kwargs,\n    ):\n        super().__init__()\n        self.cnn = ImpalaCNN(\n            outsize=cnn_outsize,\n            init_norm_kwargs=init_norm_kwargs,\n            dense_init_norm_kwargs=dense_init_norm_kwargs,\n            **kwargs,\n        )\n        self.linear = FanInInitReLULayer(\n            cnn_outsize,\n            output_size,\n            layer_type=\"linear\",\n            **dense_init_norm_kwargs,\n        )\n    def forward(self, img):\n        return self.linear(self.cnn(img))\nclass MinecraftPolicy(nn.Module):\n    \"\"\"\n    :param recurrence_type:\n        None                - No recurrence, adds no extra layers\n        lstm                - (Depreciated). Singular LSTM\n        multi_layer_lstm    - Multi-layer LSTM. Uses n_recurrence_layers to determine number of consecututive LSTMs\n            Does NOT support ragged batching\n        multi_masked_lstm   - Multi-layer LSTM that supports ragged batching via the first vector. This model is slower\n            Uses n_recurrence_layers to determine number of consecututive LSTMs"
        },
        {
            "comment": "This function is used to initialize an object of the class \"Policy\" which appears to be a deep learning model for reinforcement learning. The model can take both image and observation inputs, and uses a Dense transformer as part of its architecture. There are many optional parameters such as recurrence_type, impala_width, obs_processing_width, hidsize, single_output, img_shape, scale_input_img, only_img_input, init_norm_kwargs, impala_kwargs and more that can be used to customize the model.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":91-121",
            "content": "        transformer         - Dense transformer\n    :param init_norm_kwargs: kwargs for all FanInInitReLULayers.\n    \"\"\"\n    def __init__(\n        self,\n        recurrence_type=\"lstm\",\n        impala_width=1,\n        impala_chans=(16, 32, 32),\n        obs_processing_width=256,\n        hidsize=512,\n        single_output=False,  # True if we don't need separate outputs for action/value outputs\n        img_shape=None,\n        scale_input_img=True,\n        only_img_input=False,\n        init_norm_kwargs={},\n        impala_kwargs={},\n        # Unused argument assumed by forc.\n        input_shape=None,  # pylint: disable=unused-argument\n        active_reward_monitors=None,\n        img_statistics=None,\n        first_conv_norm=False,\n        diff_mlp_embedding=False,\n        attention_mask_style=\"clipped_causal\",\n        attention_heads=8,\n        attention_memory_size=2048,\n        use_pointwise_layer=True,\n        pointwise_ratio=4,\n        pointwise_use_activation=False,\n        n_recurrence_layers=1,\n        recurrence_is_residual=True,"
        },
        {
            "comment": "The code defines a class with an __init__ method that takes various arguments, including the recurrence_type, active_reward_monitors, single_output, impala_width, impala_chans, hidsize, init_norm_kwargs and timesteps. It performs an assertion on the recurrence_type, initializes some variables and dictionaries, and defines a few more attributes based on these arguments.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":122-149",
            "content": "        timesteps=None,\n        use_pre_lstm_ln=True,  # Not needed for transformer\n        **unused_kwargs,\n    ):\n        super().__init__()\n        assert recurrence_type in [\n            \"multi_layer_lstm\",\n            \"multi_layer_bilstm\",\n            \"multi_masked_lstm\",\n            \"transformer\",\n            \"none\",\n        ]\n        active_reward_monitors = active_reward_monitors or {}\n        self.single_output = single_output\n        chans = tuple(int(impala_width * c) for c in impala_chans)\n        self.hidsize = hidsize\n        # Dense init kwargs replaces batchnorm/groupnorm with layernorm\n        self.init_norm_kwargs = init_norm_kwargs\n        self.dense_init_norm_kwargs = deepcopy(init_norm_kwargs)\n        if self.dense_init_norm_kwargs.get(\"group_norm_groups\", None) is not None:\n            self.dense_init_norm_kwargs.pop(\"group_norm_groups\", None)\n            self.dense_init_norm_kwargs[\"layer_norm\"] = True\n        if self.dense_init_norm_kwargs.get(\"batch_norm\", False):\n            self.dense_init_norm_kwargs.pop(\"batch_norm\", False)"
        },
        {
            "comment": "Initializing layer norm for dense layers and setting up input processing components.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":150-177",
            "content": "            self.dense_init_norm_kwargs[\"layer_norm\"] = True\n        # Setup inputs\n        self.img_preprocess = ImgPreprocessing(img_statistics=img_statistics, scale_img=scale_input_img)\n        self.img_process = ImgObsProcess(\n            cnn_outsize=256,\n            output_size=hidsize,\n            inshape=img_shape,\n            chans=chans,\n            nblock=2,\n            dense_init_norm_kwargs=self.dense_init_norm_kwargs,\n            init_norm_kwargs=init_norm_kwargs,\n            first_conv_norm=first_conv_norm,\n            **impala_kwargs,\n        )\n        self.pre_lstm_ln = nn.LayerNorm(hidsize) if use_pre_lstm_ln else None\n        self.diff_obs_process = None\n        self.recurrence_type = recurrence_type\n        self.recurrent_layer = None\n        self.recurrent_layer = ResidualRecurrentBlocks(\n            hidsize=hidsize,\n            timesteps=timesteps,\n            recurrence_type=recurrence_type,\n            is_residual=recurrence_is_residual,\n            use_pointwise_layer=use_pointwise_layer,"
        },
        {
            "comment": "The code initializes a module with specified parameters including pointwise_ratio, pointwise_use_activation, attention_mask_style, attention_heads, attention_memory_size and n_block. Then it creates an instance of FanInInitReLULayer and LayerNorm for the last layer and final layer normalization respectively. It also defines a function output_latent_size to return the latent size, and another function forward which takes in observations, initial state, and context as input, performs image preprocessing and optional differential observation processing if specified, applies pre-LSTM normalization if present, then passes the processed data through the recurrent layer to obtain output x and updated state.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":178-207",
            "content": "            pointwise_ratio=pointwise_ratio,\n            pointwise_use_activation=pointwise_use_activation,\n            attention_mask_style=attention_mask_style,\n            attention_heads=attention_heads,\n            attention_memory_size=attention_memory_size,\n            n_block=n_recurrence_layers,\n        )\n        self.lastlayer = FanInInitReLULayer(hidsize, hidsize, layer_type=\"linear\", **self.dense_init_norm_kwargs)\n        self.final_ln = th.nn.LayerNorm(hidsize)\n    def output_latent_size(self):\n        return self.hidsize\n    def forward(self, ob, state_in, context):\n        first = context[\"first\"]\n        x = self.img_preprocess(ob[\"img\"])\n        x = self.img_process(x)\n        if self.diff_obs_process:\n            processed_obs = self.diff_obs_process(ob[\"diff_goal\"])\n            x = processed_obs + x\n        if self.pre_lstm_ln is not None:\n            x = self.pre_lstm_ln(x)\n        if self.recurrent_layer is not None:\n            x, state_out = self.recurrent_layer(x, first, state_in)\n        else:"
        },
        {
            "comment": "The code defines a class `MinecraftAgentPolicy` that inherits from `nn.Module`. It takes in an action space, policy kwargs, and pi_head kwargs as parameters during initialization. Inside the initialization, it creates a network `self.net` using `MinecraftPolicy`, a value head `self.value_head` using `make_value_head`, and a policy head `self.pi_head` using `make_action_head`. The code also defines a method `initial_state(batchsize)` that returns the initial state of the recurrent layer if it exists, otherwise it returns None.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":208-237",
            "content": "            state_out = state_in\n        x = F.relu(x, inplace=False)\n        x = self.lastlayer(x)\n        x = self.final_ln(x)\n        pi_latent = vf_latent = x\n        if self.single_output:\n            return pi_latent, state_out\n        return (pi_latent, vf_latent), state_out\n    def initial_state(self, batchsize):\n        if self.recurrent_layer:\n            return self.recurrent_layer.initial_state(batchsize)\n        else:\n            return None\nclass MinecraftAgentPolicy(nn.Module):\n    def __init__(self, action_space, policy_kwargs, pi_head_kwargs):\n        super().__init__()\n        self.net = MinecraftPolicy(**policy_kwargs)\n        self.action_space = action_space\n        self.value_head = self.make_value_head(self.net.output_latent_size())\n        self.pi_head = self.make_action_head(self.net.output_latent_size(), **pi_head_kwargs)\n    def make_value_head(self, v_out_size: int, norm_type: str = \"ewma\", norm_kwargs: Optional[Dict] = None):\n        return ScaledMSEHead(v_out_size, 1, norm_type=norm_type, norm_kwargs=norm_kwargs)"
        },
        {
            "comment": "This code defines a class that uses a neural network to make policy decisions. It includes methods for creating an action head, initializing the state, resetting parameters, and performing forward passes on input observations. The forward pass involves passing the observation through the network, extracting policy logits and value predictions using separate heads, and returning these outputs along with any updated state.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":239-268",
            "content": "    def make_action_head(self, pi_out_size: int, **pi_head_opts):\n        return make_action_head(self.action_space, pi_out_size, **pi_head_opts)\n    def initial_state(self, batch_size: int):\n        return self.net.initial_state(batch_size)\n    def reset_parameters(self):\n        super().reset_parameters()\n        self.net.reset_parameters()\n        self.pi_head.reset_parameters()\n        self.value_head.reset_parameters()\n    def forward(self, obs, first: th.Tensor, state_in):\n        if isinstance(obs, dict):\n            # We don't want to mutate the obs input.\n            obs = obs.copy()\n            # If special \"mask\" key is in obs,\n            # It's for masking the logits.\n            # We take it out (the network doesn't need it)\n            mask = obs.pop(\"mask\", None)\n        else:\n            mask = None\n        (pi_h, v_h), state_out = self.net(obs, state_in, context={\"first\": first})\n        pi_logits = self.pi_head(pi_h, mask=mask)\n        vpred = self.value_head(v_h)\n        return (pi_logits, vpred, None), state_out"
        },
        {
            "comment": "This code defines three functions for handling actions and probabilities in a policy network. The first function `get_logprob_of_action` calculates the log probability of taking a given action based on the provided probability distribution. The second function `get_kl_of_action_dists` computes the KL divergence between two action probability distributions. Lastly, the `get_output_for_observation` function returns the probability distribution, value prediction, and new state for a given observation using the previous two functions.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":270-298",
            "content": "    def get_logprob_of_action(self, pd, action):\n        \"\"\"\n        Get logprob of taking action `action` given probability distribution\n        (see `get_gradient_for_action` to get this distribution)\n        \"\"\"\n        ac = tree_map(lambda x: x.unsqueeze(1), action)\n        log_prob = self.pi_head.logprob(ac, pd)\n        assert not th.isnan(log_prob).any()\n        return log_prob[:, 0]\n    def get_kl_of_action_dists(self, pd1, pd2):\n        \"\"\"\n        Get the KL divergence between two action probability distributions\n        \"\"\"\n        return self.pi_head.kl_divergence(pd1, pd2)\n    def get_output_for_observation(self, obs, state_in, first):\n        \"\"\"\n        Return gradient-enabled outputs for given observation.\n        Use `get_logprob_of_action` to get log probability of action\n        with the given probability distribution.\n        Returns:\n          - probability distribution given observation\n          - value prediction for given observation\n          - new state\n        \"\"\"\n        # We need to add a fictitious time dimension everywhere"
        },
        {
            "comment": "Code is adding a time dimension to the observations and first state, then passing them through the model to get policies (pd), value predictions (vpred), and update the state. If a taken action is provided, it uses that for the current step instead of sampling from the policy. It calculates the log probability of the taken action and stores the results in a dictionary with keys \"log_prob\" and \"vpred\". The time dimension is removed after calculations are done.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":299-322",
            "content": "        obs = tree_map(lambda x: x.unsqueeze(1), obs)\n        first = first.unsqueeze(1)\n        (pd, vpred, _), state_out = self(obs=obs, first=first, state_in=state_in)\n        return pd, self.value_head.denormalize(vpred)[:, 0], state_out\n    @th.no_grad()\n    def act(self, obs, first, state_in, stochastic: bool = True, taken_action=None, return_pd=False):\n        # We need to add a fictitious time dimension everywhere\n        obs = tree_map(lambda x: x.unsqueeze(1), obs)\n        first = first.unsqueeze(1)\n        (pd, vpred, _), state_out = self(obs=obs, first=first, state_in=state_in)\n        if taken_action is None:\n            ac = self.pi_head.sample(pd, deterministic=not stochastic)\n        else:\n            ac = tree_map(lambda x: x.unsqueeze(1), taken_action)\n        log_prob = self.pi_head.logprob(ac, pd)\n        assert not th.isnan(log_prob).any()\n        # After unsqueezing, squeeze back to remove fictitious time dimension\n        result = {\"log_prob\": log_prob[:, 0], \"vpred\": self.value_head.denormalize(vpred)[:, 0]}"
        },
        {
            "comment": "This code defines a class called \"InverseActionNet\" that inherits from another class named \"MinecraftPolicy\". The class has an initializer which takes parameters for hidden size, 3D convolution parameters, and any other arguments passed to the parent class. It also contains two methods: \"policy\" and \"v\".\n\nThe \"policy\" method calculates the policy distribution (pd) and the value prediction (vpred) for a given observation (obs). It returns the pd, vpred, and an additional state_out. If return_pd is True, it also returns the first element of each vector in the pd array by using tree_map lambda function.\n\nThe \"v\" method predicts the value for a given MDP observation. It takes obs, first, and state_in as input parameters. After unsqueezing the obs and first variables, it calls the parent class's __call__ method to get pd, vpred, and state_out. Finally, it returns the denormalized vpred value of the first element in each vector by using self.value_head.denormalize function.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":323-356",
            "content": "        if return_pd:\n            result[\"pd\"] = tree_map(lambda x: x[:, 0], pd)\n        ac = tree_map(lambda x: x[:, 0], ac)\n        return ac, state_out, result\n    @th.no_grad()\n    def v(self, obs, first, state_in):\n        \"\"\"Predict value for a given mdp observation\"\"\"\n        obs = tree_map(lambda x: x.unsqueeze(1), obs)\n        first = first.unsqueeze(1)\n        (pd, vpred, _), state_out = self(obs=obs, first=first, state_in=state_in)\n        # After unsqueezing, squeeze back\n        return self.value_head.denormalize(vpred)[:, 0]\nclass InverseActionNet(MinecraftPolicy):\n    \"\"\"\n    Args:\n        conv3d_params: PRE impala 3D CNN params. They are just passed into th.nn.Conv3D.\n    \"\"\"\n    def __init__(\n        self,\n        hidsize=512,\n        conv3d_params=None,\n        **MCPoliy_kwargs,\n    ):\n        super().__init__(\n            hidsize=hidsize,\n            # If we're using 3dconv, then we normalize entire impala otherwise don't\n            # normalize the first impala layer since we normalize the input"
        },
        {
            "comment": "This code initializes a 3D convolution layer if the `conv3d_params` is not None. It also sets the initialization parameters for the 3D conv layer differently to avoid normalization of its input. The forward function applies the 3D convolution (if available) before processing the image stack.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":357-385",
            "content": "            first_conv_norm=conv3d_params is not None,\n            **MCPoliy_kwargs,\n        )\n        self.conv3d_layer = None\n        if conv3d_params is not None:\n            # 3D conv is the first layer, so don't normalize its input\n            conv3d_init_params = deepcopy(self.init_norm_kwargs)\n            conv3d_init_params[\"group_norm_groups\"] = None\n            conv3d_init_params[\"batch_norm\"] = False\n            self.conv3d_layer = FanInInitReLULayer(\n                layer_type=\"conv3d\",\n                log_scope=\"3d_conv\",\n                **conv3d_params,\n                **conv3d_init_params,\n            )\n    def forward(self, ob, state_in, context):\n        first = context[\"first\"]\n        x = self.img_preprocess(ob[\"img\"])\n        # Conv3D Prior to Impala\n        if self.conv3d_layer is not None:\n            x = self._conv3d_forward(x)\n        # Impala Stack\n        x = self.img_process(x)\n        if self.recurrent_layer is not None:\n            x, state_out = self.recurrent_layer(x, first, state_in)"
        },
        {
            "comment": "The code defines a class `InverseActionPolicy` that inherits from `nn.Module`. This class represents an inverse action policy for a reinforcement learning model. It consists of two components: a network (`self.net`) and a policy head (`self.pi_head`). The network is responsible for mapping observations to a latent space, while the policy head maps the latent representation to a distribution over actions.\n\nThe `__init__` method initializes the instance of the class by setting the action space, creating an instance of the `InverseActionNet`, and then creating the policy head based on the specified output size from the network and any additional keyword arguments provided.\n\nThe `_conv3d_forward` function is a helper function that performs 3D convolution on input data and returns the result. It transposes the input tensor, applies a series of 1D convolutions along different axes, and then transposes the resulting tensor back to the original format.\n\nThe `_policy_and_value` method calculates the policy and value for an input observation by passing it through the network and policy head, applying a ReLU activation function, and normalizing the output distribution. It also returns the current internal state of the module.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":387-423",
            "content": "        x = F.relu(x, inplace=False)\n        pi_latent = self.lastlayer(x)\n        pi_latent = self.final_ln(x)\n        return (pi_latent, None), state_out\n    def _conv3d_forward(self, x):\n        # Convert from (B, T, H, W, C) -> (B, H, W, C, T)\n        x = transpose(x, \"bthwc\", \"bcthw\")\n        new_x = []\n        for mini_batch in th.split(x, 1):\n            new_x.append(self.conv3d_layer(mini_batch))\n        x = th.cat(new_x)\n        # Convert back\n        x = transpose(x, \"bcthw\", \"bthwc\")\n        return x\nclass InverseActionPolicy(nn.Module):\n    def __init__(\n        self,\n        action_space,\n        pi_head_kwargs=None,\n        idm_net_kwargs=None,\n    ):\n        super().__init__()\n        self.action_space = action_space\n        self.net = InverseActionNet(**idm_net_kwargs)\n        pi_out_size = self.net.output_latent_size()\n        pi_head_kwargs = {} if pi_head_kwargs is None else pi_head_kwargs\n        self.pi_head = self.make_action_head(pi_out_size=pi_out_size, **pi_head_kwargs)\n    def make_action_head(self, **kwargs):"
        },
        {
            "comment": "This code defines a policy class for training reinforcement learning models. It has methods to reset the model's parameters, forward pass to obtain action-value logits and state, and a deterministic prediction method. The code uses PyTorch for tensor operations.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":424-456",
            "content": "        return make_action_head(self.action_space, **kwargs)\n    def reset_parameters(self):\n        super().reset_parameters()\n        self.net.reset_parameters()\n        self.pi_head.reset_parameters()\n    def forward(self, obs, first: th.Tensor, state_in, **kwargs):\n        if isinstance(obs, dict):\n            # We don't want to mutate the obs input.\n            obs = obs.copy()\n            # If special \"mask\" key is in obs,\n            # It's for masking the logits.\n            # We take it out (the network doesn't need it)\n            mask = obs.pop(\"mask\", None)\n        else:\n            mask = None\n        (pi_h, _), state_out = self.net(obs, state_in=state_in, context={\"first\": first}, **kwargs)\n        pi_logits = self.pi_head(pi_h, mask=mask)\n        return (pi_logits, None, None), state_out\n    @th.no_grad()\n    def predict(\n        self,\n        obs,\n        deterministic: bool = True,\n        **kwargs,\n    ):\n        (pd, _, _), state_out = self(obs=obs, **kwargs)\n        ac = self.pi_head.sample(pd, deterministic=deterministic)"
        },
        {
            "comment": "The code computes the log probability of actions using the pi_head, checks for NaN values, and returns a dictionary containing the log_probability and pd. It also includes functions for initializing the state based on batch size.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/policy.py\":457-466",
            "content": "        log_prob = self.pi_head.logprob(ac, pd)\n        assert not th.isnan(log_prob).any()\n        result = {\"log_prob\": log_prob, \"pd\": pd}\n        return ac, state_out, result\n    def initial_state(self, batch_size: int):\n        return self.net.initial_state(batch_size)"
        }
    ]
}