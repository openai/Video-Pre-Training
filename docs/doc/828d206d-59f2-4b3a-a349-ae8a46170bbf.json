{
    "summary": "The NormalizeEwma module normalizes data across dimensions, calculates debiased mean and variance, and provides methods for normalization and denormalization. It maintains running mean and variance for input vectors during training while avoiding backpropagation issues.",
    "details": [
        {
            "comment": "NormalizeEwma is an EWMA (Exponential Weighted Moving Average) normalization module for vectors of observations. It normalizes the data across specific dimensions, with optional per-element update and debiasing term.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/normalize_ewma.py\":0-27",
            "content": "import numpy as np\nimport torch\nimport torch.nn as nn\nclass NormalizeEwma(nn.Module):\n    \"\"\"Normalize a vector of observations - across the first norm_axes dimensions\"\"\"\n    def __init__(self, input_shape, norm_axes=2, beta=0.99999, per_element_update=False, epsilon=1e-5):\n        super().__init__()\n        self.input_shape = input_shape\n        self.norm_axes = norm_axes\n        self.epsilon = epsilon\n        self.beta = beta\n        self.per_element_update = per_element_update\n        self.running_mean = nn.Parameter(torch.zeros(input_shape, dtype=torch.float), requires_grad=False)\n        self.running_mean_sq = nn.Parameter(torch.zeros(input_shape, dtype=torch.float), requires_grad=False)\n        self.debiasing_term = nn.Parameter(torch.tensor(0.0, dtype=torch.float), requires_grad=False)\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_mean_sq.zero_()\n        self.debiasing_term.zero_()\n    def running_mean_var(self):\n        debiased_mean = self.running_mean / self.debiasing_term.clamp(min=self.epsilon)"
        },
        {
            "comment": "This code calculates the debiased mean and variance of input vectors for each batch while training. It normalizes the input to float32, updates running means and squared means with detached inputs, and applies weighted averages to avoid backpropagation through subsequent batches.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/normalize_ewma.py\":28-50",
            "content": "        debiased_mean_sq = self.running_mean_sq / self.debiasing_term.clamp(min=self.epsilon)\n        debiased_var = (debiased_mean_sq - debiased_mean ** 2).clamp(min=1e-2)\n        return debiased_mean, debiased_var\n    def forward(self, input_vector):\n        # Make sure input is float32\n        input_vector = input_vector.to(torch.float)\n        if self.training:\n            # Detach input before adding it to running means to avoid backpropping through it on\n            # subsequent batches.\n            detached_input = input_vector.detach()\n            batch_mean = detached_input.mean(dim=tuple(range(self.norm_axes)))\n            batch_sq_mean = (detached_input ** 2).mean(dim=tuple(range(self.norm_axes)))\n            if self.per_element_update:\n                batch_size = np.prod(detached_input.size()[: self.norm_axes])\n                weight = self.beta ** batch_size\n            else:\n                weight = self.beta\n            self.running_mean.mul_(weight).add_(batch_mean * (1.0 - weight))\n            self.running_mean_sq.mul_(weight).add_(batch_sq_mean * (1.0 - weight))"
        },
        {
            "comment": "This class provides methods to normalize and denormalize data. It also maintains running mean and variance.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/lib/normalize_ewma.py\":51-59",
            "content": "            self.debiasing_term.mul_(weight).add_(1.0 * (1.0 - weight))\n        mean, var = self.running_mean_var()\n        return (input_vector - mean[(None,) * self.norm_axes]) / torch.sqrt(var)[(None,) * self.norm_axes]\n    def denormalize(self, input_vector):\n        \"\"\"Transform normalized data back into original distribution\"\"\"\n        mean, var = self.running_mean_var()\n        return input_vector * torch.sqrt(var)[(None,) * self.norm_axes] + mean[(None,) * self.norm_axes]"
        }
    ]
}