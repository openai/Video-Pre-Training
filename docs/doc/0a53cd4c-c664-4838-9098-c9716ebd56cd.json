{
    "summary": "The code imports libraries, initializes a data loader class for simpler code, lacks sub-sequence support, and processes data for a batch of samples with workers outputting all samples to the same batch. The `__del__` method terminates and joins processes when object is deleted.",
    "details": [
        {
            "comment": "This code imports necessary libraries and defines functions for loading OpenAI MineRL VPT datasets, adjusting cursor position based on version-specific scalers, and compositing images with alpha transparency.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":0-39",
            "content": "# Code for loading OpenAI MineRL VPT datasets\n# NOTE: This is NOT original code used for the VPT experiments!\n#       (But contains all [or at least most] steps done in the original data loading)\nimport json\nimport glob\nimport os\nimport random\nfrom multiprocessing import Process, Queue, Event\nimport numpy as np\nimport cv2\nfrom run_inverse_dynamics_model import json_action_to_env_action\nfrom agent import resize_image, AGENT_RESOLUTION\nQUEUE_TIMEOUT = 10\nCURSOR_FILE = os.path.join(os.path.dirname(__file__), \"cursors\", \"mouse_cursor_white_16x16.png\")\nMINEREC_ORIGINAL_HEIGHT_PX = 720\n# If GUI is open, mouse dx/dy need also be adjusted with these scalers.\n# If data version is not present, assume it is 1.\nMINEREC_VERSION_SPECIFIC_SCALERS = {\n    \"5.7\": 0.5,\n    \"5.8\": 0.5,\n    \"6.7\": 2.0,\n    \"6.8\": 2.0,\n    \"6.9\": 2.0,\n}\ndef composite_images_with_alpha(image1, image2, alpha, x, y):\n    \"\"\"\n    Draw image2 over image1 at location x,y, using alpha as the opacity for image2.\n    Modifies image1 in-place\n    \"\"\"\n    ch = max(0, min(image1.shape[0] - y, image2.shape[0]))"
        },
        {
            "comment": "The code reads a video and its corresponding JSON file to extract frames and annotations for each frame. It initializes a cursor image and alpha channel, then continuously processes tasks from the tasks queue. If a task is None, it breaks the loop. The code checks if the video contains the game starting with attack always down by noting that it might be stuck down until the player presses attack.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":40-66",
            "content": "    cw = max(0, min(image1.shape[1] - x, image2.shape[1]))\n    if ch == 0 or cw == 0:\n        return\n    alpha = alpha[:ch, :cw]\n    image1[y:y + ch, x:x + cw, :] = (image1[y:y + ch, x:x + cw, :] * (1 - alpha) + image2[:ch, :cw, :] * alpha).astype(np.uint8)\ndef data_loader_worker(tasks_queue, output_queue, quit_workers_event):\n    \"\"\"\n    Worker for the data loader.\n    \"\"\"\n    cursor_image = cv2.imread(CURSOR_FILE, cv2.IMREAD_UNCHANGED)\n    # Assume 16x16\n    cursor_image = cursor_image[:16, :16, :]\n    cursor_alpha = cursor_image[:, :, 3:] / 255.0\n    cursor_image = cursor_image[:, :, :3]\n    while True:\n        task = tasks_queue.get()\n        if task is None:\n            break\n        trajectory_id, video_path, json_path = task\n        video = cv2.VideoCapture(video_path)\n        # NOTE: In some recordings, the game seems to start\n        #       with attack always down from the beginning, which\n        #       is stuck down until player actually presses attack\n        # NOTE: It is uncertain if this was the issue with the original code."
        },
        {
            "comment": "Checking if attack is stuck by monitoring scrollwheel actions and updating \"hotbar.#\" actions when hotbar selection changes.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":67-90",
            "content": "        attack_is_stuck = False\n        # Scrollwheel is allowed way to change items, but this is\n        # not captured by the recorder.\n        # Work around this by keeping track of selected hotbar item\n        # and updating \"hotbar.#\" actions when hotbar selection changes.\n        # NOTE: It is uncertain is this was/is an issue with the contractor data\n        last_hotbar = 0\n        with open(json_path) as json_file:\n            json_lines = json_file.readlines()\n            json_data = \"[\" + \",\".join(json_lines) + \"]\"\n            json_data = json.loads(json_data)\n        for i in range(len(json_data)):\n            if quit_workers_event.is_set():\n                break\n            step_data = json_data[i]\n            if i == 0:\n                # Check if attack will be stuck down\n                if step_data[\"mouse\"][\"newButtons\"] == [0]:\n                    attack_is_stuck = True\n            elif attack_is_stuck:\n                # Check if we press attack down, then it might not be stuck\n                if 0 in step_data[\"mouse\"][\"newButtons\"]:"
        },
        {
            "comment": "Checking for stuck state and removing action, updating hotbar selection, reading frame even if null to progress forward.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":91-112",
            "content": "                    attack_is_stuck = False\n            # If still stuck, remove the action\n            if attack_is_stuck:\n                step_data[\"mouse\"][\"buttons\"] = [button for button in step_data[\"mouse\"][\"buttons\"] if button != 0]\n            action, is_null_action = json_action_to_env_action(step_data)\n            # Update hotbar selection\n            current_hotbar = step_data[\"hotbar\"]\n            if current_hotbar != last_hotbar:\n                action[\"hotbar.{}\".format(current_hotbar + 1)] = 1\n            last_hotbar = current_hotbar\n            # Read frame even if this is null so we progress forward\n            ret, frame = video.read()\n            if ret:\n                # Skip null actions as done in the VPT paper\n                # NOTE: in VPT paper, this was checked _after_ transforming into agent's action-space.\n                #       We do this here as well to reduce amount of data sent over.\n                if is_null_action:\n                    continue\n                if step_data[\"isGuiOpen\"]:"
        },
        {
            "comment": "Applies camera scaling factor to mouse coordinates, composes cursor image with frame, converts image color, clips and resizes the frame, then puts (trajectory_id, frame, action) in output queue. If frame cannot be read, prints an error message. Finally, releases video and checks quit_workers_event before putting None in output queue to signal end of data loading.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":113-133",
            "content": "                    camera_scaling_factor = frame.shape[0] / MINEREC_ORIGINAL_HEIGHT_PX\n                    cursor_x = int(step_data[\"mouse\"][\"x\"] * camera_scaling_factor)\n                    cursor_y = int(step_data[\"mouse\"][\"y\"] * camera_scaling_factor)\n                    composite_images_with_alpha(frame, cursor_image, cursor_alpha, cursor_x, cursor_y)\n                cv2.cvtColor(frame, code=cv2.COLOR_BGR2RGB, dst=frame)\n                frame = np.asarray(np.clip(frame, 0, 255), dtype=np.uint8)\n                frame = resize_image(frame, AGENT_RESOLUTION)\n                output_queue.put((trajectory_id, frame, action), timeout=QUEUE_TIMEOUT)\n            else:\n                print(f\"Could not read frame from video {video_path}\")\n        video.release()\n        if quit_workers_event.is_set():\n            break\n    # Tell that we ended\n    output_queue.put(None)\nclass DataLoader:\n    \"\"\"\n    Generator class for loading batches from a dataset\n    This only returns a single step at a time per worker; no sub-sequences."
        },
        {
            "comment": "This code initializes a data loader class that tracks the model's hidden state and feeds it along with one sample at a time. It supports simpler loader code, lower end hardware, but is not very efficient and lacks support for sub-sequences. The loader loads individual files as trajectory files if they are split into multiple files.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":134-154",
            "content": "    Idea is that you keep track of the model's hidden state and feed that in,\n    along with one sample at a time.\n    + Simpler loader code\n    + Supports lower end hardware\n    - Not very efficient (could be faster)\n    - No support for sub-sequences\n    - Loads up individual files as trajectory files (i.e. if a trajectory is split into multiple files,\n      this code will load it up as a separate item).\n    \"\"\"\n    def __init__(self, dataset_dir, n_workers=8, batch_size=8, n_epochs=1, max_queue_size=16):\n        assert n_workers >= batch_size, \"Number of workers must be equal or greater than batch size\"\n        self.dataset_dir = dataset_dir\n        self.n_workers = n_workers\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.max_queue_size = max_queue_size\n        unique_ids = glob.glob(os.path.join(dataset_dir, \"*.mp4\"))\n        unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))\n        self.unique_ids = unique_ids\n        # Create tuples of (video_path, json_path) for each unique_id"
        },
        {
            "comment": "This code is creating a data loader for video demonstrations. It collects the video and JSONL file paths for each unique ID, shuffles them for each epoch, and adds them to the task queue. It also creates output queues for worker threads.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":155-177",
            "content": "        demonstration_tuples = []\n        for unique_id in unique_ids:\n            video_path = os.path.abspath(os.path.join(dataset_dir, unique_id + \".mp4\"))\n            json_path = os.path.abspath(os.path.join(dataset_dir, unique_id + \".jsonl\"))\n            demonstration_tuples.append((video_path, json_path))\n        assert n_workers <= len(demonstration_tuples), f\"n_workers should be lower or equal than number of demonstrations {len(demonstration_tuples)}\"\n        # Repeat dataset for n_epochs times, shuffling the order for\n        # each epoch\n        self.demonstration_tuples = []\n        for i in range(n_epochs):\n            random.shuffle(demonstration_tuples)\n            self.demonstration_tuples += demonstration_tuples\n        self.task_queue = Queue()\n        self.n_steps_processed = 0\n        for trajectory_id, task in enumerate(self.demonstration_tuples):\n            self.task_queue.put((trajectory_id, *task))\n        for _ in range(n_workers):\n            self.task_queue.put(None)\n        self.output_queues = [Queue(maxsize=max_queue_size) for _ in range(n_workers)]"
        },
        {
            "comment": "This code sets up data loading workers as separate processes, and then starts them. The iterator function retrieves batch frames, actions, and episode IDs from the output queues of these worker processes until one of the workers runs out of work.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":178-208",
            "content": "        self.quit_workers_event = Event()\n        self.processes = [\n            Process(\n                target=data_loader_worker,\n                args=(\n                    self.task_queue,\n                    output_queue,\n                    self.quit_workers_event,\n                ),\n                daemon=True\n            )\n            for output_queue in self.output_queues\n        ]\n        for process in self.processes:\n            process.start()\n    def __iter__(self):\n        return self\n    def __next__(self):\n        batch_frames = []\n        batch_actions = []\n        batch_episode_id = []\n        for i in range(self.batch_size):\n            workitem = self.output_queues[self.n_steps_processed % self.n_workers].get(timeout=QUEUE_TIMEOUT)\n            if workitem is None:\n                # Stop iteration when first worker runs out of work to do.\n                # Yes, this has a chance of cutting out a lot of the work,\n                # but this ensures batches will remain diverse, instead\n                # of having bad ones in the end where potentially"
        },
        {
            "comment": "This code is processing data for a batch of samples, where each worker outputs all samples to the same batch. It appends frames, actions, and episode IDs to their respective lists before returning them as a batch. The `__del__` method ensures all processes are terminated and joined when the object is deleted.",
            "location": "\"/media/root/Toshiba XG3/works/Video-Pre-Training/docs/src/data_loader.py\":209-221",
            "content": "                # one worker outputs all samples to the same batch.\n                raise StopIteration()\n            trajectory_id, frame, action = workitem\n            batch_frames.append(frame)\n            batch_actions.append(action)\n            batch_episode_id.append(trajectory_id)\n            self.n_steps_processed += 1\n        return batch_frames, batch_actions, batch_episode_id\n    def __del__(self):\n        for process in self.processes:\n            process.terminate()\n            process.join()"
        }
    ]
}