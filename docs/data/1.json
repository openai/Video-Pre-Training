{
    "100": {
        "file_id": 3,
        "content": "        demonstration_tuples = []\n        for unique_id in unique_ids:\n            video_path = os.path.abspath(os.path.join(dataset_dir, unique_id + \".mp4\"))\n            json_path = os.path.abspath(os.path.join(dataset_dir, unique_id + \".jsonl\"))\n            demonstration_tuples.append((video_path, json_path))\n        assert n_workers <= len(demonstration_tuples), f\"n_workers should be lower or equal than number of demonstrations {len(demonstration_tuples)}\"\n        # Repeat dataset for n_epochs times, shuffling the order for\n        # each epoch\n        self.demonstration_tuples = []\n        for i in range(n_epochs):\n            random.shuffle(demonstration_tuples)\n            self.demonstration_tuples += demonstration_tuples\n        self.task_queue = Queue()\n        self.n_steps_processed = 0\n        for trajectory_id, task in enumerate(self.demonstration_tuples):\n            self.task_queue.put((trajectory_id, *task))\n        for _ in range(n_workers):\n            self.task_queue.put(None)\n        self.output_queues = [Queue(maxsize=max_queue_size) for _ in range(n_workers)]",
        "type": "code",
        "location": "/data_loader.py:156-178"
    },
    "101": {
        "file_id": 3,
        "content": "This code is creating a data loader for video demonstrations. It collects the video and JSONL file paths for each unique ID, shuffles them for each epoch, and adds them to the task queue. It also creates output queues for worker threads.",
        "type": "comment"
    },
    "102": {
        "file_id": 3,
        "content": "        self.quit_workers_event = Event()\n        self.processes = [\n            Process(\n                target=data_loader_worker,\n                args=(\n                    self.task_queue,\n                    output_queue,\n                    self.quit_workers_event,\n                ),\n                daemon=True\n            )\n            for output_queue in self.output_queues\n        ]\n        for process in self.processes:\n            process.start()\n    def __iter__(self):\n        return self\n    def __next__(self):\n        batch_frames = []\n        batch_actions = []\n        batch_episode_id = []\n        for i in range(self.batch_size):\n            workitem = self.output_queues[self.n_steps_processed % self.n_workers].get(timeout=QUEUE_TIMEOUT)\n            if workitem is None:\n                # Stop iteration when first worker runs out of work to do.\n                # Yes, this has a chance of cutting out a lot of the work,\n                # but this ensures batches will remain diverse, instead\n                # of having bad ones in the end where potentially",
        "type": "code",
        "location": "/data_loader.py:179-209"
    },
    "103": {
        "file_id": 3,
        "content": "This code sets up data loading workers as separate processes, and then starts them. The iterator function retrieves batch frames, actions, and episode IDs from the output queues of these worker processes until one of the workers runs out of work.",
        "type": "comment"
    },
    "104": {
        "file_id": 3,
        "content": "                # one worker outputs all samples to the same batch.\n                raise StopIteration()\n            trajectory_id, frame, action = workitem\n            batch_frames.append(frame)\n            batch_actions.append(action)\n            batch_episode_id.append(trajectory_id)\n            self.n_steps_processed += 1\n        return batch_frames, batch_actions, batch_episode_id\n    def __del__(self):\n        for process in self.processes:\n            process.terminate()\n            process.join()",
        "type": "code",
        "location": "/data_loader.py:210-222"
    },
    "105": {
        "file_id": 3,
        "content": "This code is processing data for a batch of samples, where each worker outputs all samples to the same batch. It appends frames, actions, and episode IDs to their respective lists before returning them as a batch. The `__del__` method ensures all processes are terminated and joined when the object is deleted.",
        "type": "comment"
    },
    "106": {
        "file_id": 4,
        "content": "/inverse_dynamics_model.py",
        "type": "filepath"
    },
    "107": {
        "file_id": 4,
        "content": "IDMAgent is a Minecraft action predictor using the IDM model, featuring functions for initializing, loading weights, resetting state, and processing video frames. It converts policy output to MineRL format for agent state prediction.",
        "type": "summary"
    },
    "108": {
        "file_id": 4,
        "content": "import numpy as np\nimport torch as th\nimport cv2\nfrom gym3.types import DictType\nfrom gym import spaces\nfrom lib.action_mapping import CameraHierarchicalMapping, IDMActionMapping\nfrom lib.actions import ActionTransformer\nfrom lib.policy import InverseActionPolicy\nfrom lib.torch_util import default_device_type, set_default_torch_device\nfrom agent import resize_image, AGENT_RESOLUTION\nACTION_TRANSFORMER_KWARGS = dict(\n    camera_binsize=2,\n    camera_maxval=10,\n    camera_mu=10,\n    camera_quantization_scheme=\"mu_law\",\n)\nclass IDMAgent:\n    \"\"\"\n    Sugarcoating on the inverse dynamics model (IDM) used to predict actions Minecraft players take in videos.\n    Functionally same as MineRLAgent.\n    \"\"\"\n    def __init__(self, idm_net_kwargs, pi_head_kwargs, device=None):\n        if device is None:\n            device = default_device_type()\n        self.device = th.device(device)\n        # Set the default torch device for underlying code as well\n        set_default_torch_device(self.device)\n        self.action_mapper = IDMActionMapping(n_camera_bins=11)",
        "type": "code",
        "location": "/inverse_dynamics_model.py:1-33"
    },
    "109": {
        "file_id": 4,
        "content": "IDMAgent is a class representing an agent that uses the inverse dynamics model (IDM) to predict Minecraft player actions in videos. It has an action mapper and is initialized with idm_net_kwargs, pi_head_kwargs, and device (default device type if None).",
        "type": "comment"
    },
    "110": {
        "file_id": 4,
        "content": "        action_space = self.action_mapper.get_action_space_update()\n        action_space = DictType(**action_space)\n        self.action_transformer = ActionTransformer(**ACTION_TRANSFORMER_KWARGS)\n        idm_policy_kwargs = dict(idm_net_kwargs=idm_net_kwargs, pi_head_kwargs=pi_head_kwargs, action_space=action_space)\n        self.policy = InverseActionPolicy(**idm_policy_kwargs).to(device)\n        self.hidden_state = self.policy.initial_state(1)\n        self._dummy_first = th.from_numpy(np.array((False,))).to(device)\n    def load_weights(self, path):\n        \"\"\"Load model weights from a path, and reset hidden state\"\"\"\n        self.policy.load_state_dict(th.load(path, map_location=self.device), strict=False)\n        self.reset()\n    def reset(self):\n        \"\"\"Reset agent to initial state (i.e., reset hidden state)\"\"\"\n        self.hidden_state = self.policy.initial_state(1)\n    def _video_obs_to_agent(self, video_frames):\n        imgs = [resize_image(frame, AGENT_RESOLUTION) for frame in video_frames]\n        # Add time and batch dim",
        "type": "code",
        "location": "/inverse_dynamics_model.py:34-56"
    },
    "111": {
        "file_id": 4,
        "content": "Function: __init__\n- Initializes the agent with specified parameters and loads initial weights.\n\nFunction: load_weights\n- Loads model weights from a path and resets the hidden state of the agent.\n\nFunction: reset\n- Resets the agent to its initial state by setting the hidden state to the result of the policy's initial_state method with an argument of 1.\n\nFunction:_video_obs_to_agent\n- Takes a list of video frames, resizes them to AGENT_RESOLUTION, and returns the processed images for the agent to use.",
        "type": "comment"
    },
    "112": {
        "file_id": 4,
        "content": "        imgs = np.stack(imgs)[None]\n        agent_input = {\"img\": th.from_numpy(imgs).to(self.device)}\n        return agent_input\n    def _agent_action_to_env(self, agent_action):\n        \"\"\"Turn output from policy into action for MineRL\"\"\"\n        # This is quite important step (for some reason).\n        # For the sake of your sanity, remember to do this step (manual conversion to numpy)\n        # before proceeding. Otherwise, your agent might be a little derp.\n        action = {\n            \"buttons\": agent_action[\"buttons\"].cpu().numpy(),\n            \"camera\": agent_action[\"camera\"].cpu().numpy()\n        }\n        minerl_action = self.action_mapper.to_factored(action)\n        minerl_action_transformed = self.action_transformer.policy2env(minerl_action)\n        return minerl_action_transformed\n    def predict_actions(self, video_frames):\n        \"\"\"\n        Predict actions for a sequence of frames.\n        `video_frames` should be of shape (N, H, W, C).\n        Returns MineRL action dict, where each action head",
        "type": "code",
        "location": "/inverse_dynamics_model.py:57-79"
    },
    "113": {
        "file_id": 4,
        "content": "Code snippet:\n```python\n    def _agent_action_to_env(self, agent_action):\n        \"\"\"Turn output from policy into action for MineRL\"\"\"\n        # Manual conversion to numpy is important.\n        action = {\n            \"buttons\": agent_action[\"buttons\"].cpu().numpy(),\n            \"camera\": agent_action[\"camera\"].cpu().numpy()\n        }\n```\nComment: Converts policy output to MineRL action format using manual numpy conversion",
        "type": "comment"
    },
    "114": {
        "file_id": 4,
        "content": "        has shape (N, ...).\n        Agent's hidden state is tracked internally. To reset it,\n        call `reset()`.\n        \"\"\"\n        agent_input = self._video_obs_to_agent(video_frames)\n        # The \"first\" argument could be used to reset tell episode\n        # boundaries, but we are only using this for predicting (for now),\n        # so we do not hassle with it yet.\n        dummy_first = th.zeros((video_frames.shape[0], 1)).to(self.device)\n        predicted_actions, self.hidden_state, _ = self.policy.predict(\n            agent_input, first=dummy_first, state_in=self.hidden_state,\n            deterministic=True\n        )\n        predicted_minerl_action = self._agent_action_to_env(predicted_actions)\n        return predicted_minerl_action",
        "type": "code",
        "location": "/inverse_dynamics_model.py:80-95"
    },
    "115": {
        "file_id": 4,
        "content": "This function takes video frames as input, converts them to agent input, and uses the policy model to predict actions. It also maintains an internal hidden state for tracking the agent's state and can be reset using `reset()`.",
        "type": "comment"
    },
    "116": {
        "file_id": 5,
        "content": "/requirements.txt",
        "type": "filepath"
    },
    "117": {
        "file_id": 5,
        "content": "Installs necessary libraries: PyTorch, Gym, attrs, and OpenCV Python.",
        "type": "summary"
    },
    "118": {
        "file_id": 5,
        "content": "torch==1.9.0\ngym3\nattrs\nopencv-python",
        "type": "code",
        "location": "/requirements.txt:1-4"
    },
    "119": {
        "file_id": 5,
        "content": "Installs necessary libraries: PyTorch, Gym, attrs, and OpenCV Python.",
        "type": "comment"
    },
    "120": {
        "file_id": 6,
        "content": "/run_agent.py",
        "type": "filepath"
    },
    "121": {
        "file_id": 6,
        "content": "The code imports libraries, defines a function 'main' that loads and uses a pre-trained model in the MineRL environment, taking two arguments: the path to the model file and weights file. It also adds an optional argument \"--model\" of type string for the file path loading.",
        "type": "summary"
    },
    "122": {
        "file_id": 6,
        "content": "from argparse import ArgumentParser\nimport pickle\nfrom minerl.herobraine.env_specs.human_survival_specs import HumanSurvival\nfrom agent import MineRLAgent, ENV_KWARGS\ndef main(model, weights):\n    env = HumanSurvival(**ENV_KWARGS).make()\n    print(\"---Loading model---\")\n    agent_parameters = pickle.load(open(model, \"rb\"))\n    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n    agent = MineRLAgent(env, policy_kwargs=policy_kwargs, pi_head_kwargs=pi_head_kwargs)\n    agent.load_weights(weights)\n    print(\"---Launching MineRL enviroment (be patient)---\")\n    obs = env.reset()\n    while True:\n        minerl_action = agent.get_action(obs)\n        obs, reward, done, info = env.step(minerl_action)\n        env.render()\nif __name__ == \"__main__\":\n    parser = ArgumentParser(\"Run pretrained models on MineRL environment\")\n    parser.add_argument(\"--weights\", type=str, required=True, help=\"Path to the '.weights' file to be loaded.\")",
        "type": "code",
        "location": "/run_agent.py:1-30"
    },
    "123": {
        "file_id": 6,
        "content": "The code imports necessary libraries and defines a function named \"main\" which loads a pre-trained model, creates an agent, and then launches the MineRL environment. The main function takes two arguments: 'model', the path to the pickle file containing the loaded model's parameters; and 'weights', the path to the '.weights' file to be loaded. It then continuously takes actions in the environment based on the pre-trained agent's recommendations until the MineRL environment is completed or terminated.",
        "type": "comment"
    },
    "124": {
        "file_id": 6,
        "content": "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the '.model' file to be loaded.\")\n    args = parser.parse_args()\n    main(args.model, args.weights)",
        "type": "code",
        "location": "/run_agent.py:31-35"
    },
    "125": {
        "file_id": 6,
        "content": "This code is adding a required argument \"--model\" to the parser, specifying its type as string and loading the file path from this argument.",
        "type": "comment"
    },
    "126": {
        "file_id": 7,
        "content": "/run_inverse_dynamics_model.py",
        "type": "filepath"
    },
    "127": {
        "file_id": 7,
        "content": "The code initializes a game dictionary, defines model actions, manages camera resets, handles inputs, loads weights, captures video input, reads JSON data, and displays IDM predictions on a video stream with OpenCV functions.",
        "type": "summary"
    },
    "128": {
        "file_id": 7,
        "content": "# NOTE: this is _not_ the original code of IDM!\n# As such, while it is close and seems to function well,\n# its performance might be bit off from what is reported\n# in the paper.\nfrom argparse import ArgumentParser\nimport pickle\nimport cv2\nimport numpy as np\nimport json\nimport torch as th\nfrom agent import ENV_KWARGS\nfrom inverse_dynamics_model import IDMAgent\nKEYBOARD_BUTTON_MAPPING = {\n    \"key.keyboard.escape\" :\"ESC\",\n    \"key.keyboard.s\" :\"back\",\n    \"key.keyboard.q\" :\"drop\",\n    \"key.keyboard.w\" :\"forward\",\n    \"key.keyboard.1\" :\"hotbar.1\",\n    \"key.keyboard.2\" :\"hotbar.2\",\n    \"key.keyboard.3\" :\"hotbar.3\",\n    \"key.keyboard.4\" :\"hotbar.4\",\n    \"key.keyboard.5\" :\"hotbar.5\",\n    \"key.keyboard.6\" :\"hotbar.6\",\n    \"key.keyboard.7\" :\"hotbar.7\",\n    \"key.keyboard.8\" :\"hotbar.8\",\n    \"key.keyboard.9\" :\"hotbar.9\",\n    \"key.keyboard.e\" :\"inventory\",\n    \"key.keyboard.space\" :\"jump\",\n    \"key.keyboard.a\" :\"left\",\n    \"key.keyboard.d\" :\"right\",\n    \"key.keyboard.left.shift\" :\"sneak\",\n    \"key.keyboard.left.control\" :\"sprint\",",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:1-36"
    },
    "129": {
        "file_id": 7,
        "content": "This code is initializing a dictionary mapping keyboard button names to their respective actions in the game. The code is used for controlling the character's movements and actions in the game environment using keyboard inputs.",
        "type": "comment"
    },
    "130": {
        "file_id": 7,
        "content": "    \"key.keyboard.f\" :\"swapHands\",\n}\n# Template action\nNOOP_ACTION = {\n    \"ESC\": 0,\n    \"back\": 0,\n    \"drop\": 0,\n    \"forward\": 0,\n    \"hotbar.1\": 0,\n    \"hotbar.2\": 0,\n    \"hotbar.3\": 0,\n    \"hotbar.4\": 0,\n    \"hotbar.5\": 0,\n    \"hotbar.6\": 0,\n    \"hotbar.7\": 0,\n    \"hotbar.8\": 0,\n    \"hotbar.9\": 0,\n    \"inventory\": 0,\n    \"jump\": 0,\n    \"left\": 0,\n    \"right\": 0,\n    \"sneak\": 0,\n    \"sprint\": 0,\n    \"swapHands\": 0,\n    \"camera\": np.array([0, 0]),\n    \"attack\": 0,\n    \"use\": 0,\n    \"pickItem\": 0,\n}\nMESSAGE = \"\"\"\nThis script will take a video, predict actions for its frames and\nand show them with a cv2 window.\nPress any button the window to proceed to the next frame.\n\"\"\"\n# Matches a number in the MineRL Java code regarding sensitivity\n# This is for mapping from recorded sensitivity to the one used in the model\nCAMERA_SCALER = 360.0 / 2400.0\ndef json_action_to_env_action(json_action):\n    \"\"\"\n    Converts a json action into a MineRL action.\n    Returns (minerl_action, is_null_action)\n    \"\"\"\n    # This might be slow...\n    env_action = NOOP_ACTION.copy()",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:37-86"
    },
    "131": {
        "file_id": 7,
        "content": "This code defines a set of actions that the model should predict for a given video. It also includes a template action and a message to be displayed with a cv2 window. The CAMERA_SCALER is used for mapping sensitivity from recorded Java code to the one used in the model. The json_action_to_env_action function converts a JSON action into a MineRL action.",
        "type": "comment"
    },
    "132": {
        "file_id": 7,
        "content": "    # As a safeguard, make camera action again so we do not override anything\n    env_action[\"camera\"] = np.array([0, 0])\n    is_null_action = True\n    keyboard_keys = json_action[\"keyboard\"][\"keys\"]\n    for key in keyboard_keys:\n        # You can have keys that we do not use, so just skip them\n        # NOTE in original training code, ESC was removed and replaced with\n        #      \"inventory\" action if GUI was open.\n        #      Not doing it here, as BASALT uses ESC to quit the game.\n        if key in KEYBOARD_BUTTON_MAPPING:\n            env_action[KEYBOARD_BUTTON_MAPPING[key]] = 1\n            is_null_action = False\n    mouse = json_action[\"mouse\"]\n    camera_action = env_action[\"camera\"]\n    camera_action[0] = mouse[\"dy\"] * CAMERA_SCALER\n    camera_action[1] = mouse[\"dx\"] * CAMERA_SCALER\n    if mouse[\"dx\"] != 0 or mouse[\"dy\"] != 0:\n        is_null_action = False\n    else:\n        if abs(camera_action[0]) > 180:\n            camera_action[0] = 0\n        if abs(camera_action[1]) > 180:\n            camera_action[1] = 0",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:87-112"
    },
    "133": {
        "file_id": 7,
        "content": "This code resets the camera action to avoid overriding other actions and handles keyboard and mouse inputs for the environment.",
        "type": "comment"
    },
    "134": {
        "file_id": 7,
        "content": "    mouse_buttons = mouse[\"buttons\"]\n    if 0 in mouse_buttons:\n        env_action[\"attack\"] = 1\n        is_null_action = False\n    if 1 in mouse_buttons:\n        env_action[\"use\"] = 1\n        is_null_action = False\n    if 2 in mouse_buttons:\n        env_action[\"pickItem\"] = 1\n        is_null_action = False\n    return env_action, is_null_action\ndef main(model, weights, video_path, json_path, n_batches, n_frames):\n    print(MESSAGE)\n    agent_parameters = pickle.load(open(model, \"rb\"))\n    net_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n    agent = IDMAgent(idm_net_kwargs=net_kwargs, pi_head_kwargs=pi_head_kwargs)\n    agent.load_weights(weights)\n    required_resolution = ENV_KWARGS[\"resolution\"]\n    cap = cv2.VideoCapture(video_path)\n    json_index = 0\n    with open(json_path) as json_file:\n        json_lines = json_file.readlines()\n        json_data = \"[\" + \",\".join(json_lines) + \"]\"",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:114-143"
    },
    "135": {
        "file_id": 7,
        "content": "This code handles mouse button events and initializes an inverse dynamics model agent for a game. It loads the agent's weights from a file, captures video input, and reads a JSON file containing game data.",
        "type": "comment"
    },
    "136": {
        "file_id": 7,
        "content": "        json_data = json.loads(json_data)\n    for _ in range(n_batches):\n        th.cuda.empty_cache()\n        print(\"=== Loading up frames ===\")\n        frames = []\n        recorded_actions = []\n        for _ in range(n_frames):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            assert frame.shape[0] == required_resolution[1] and frame.shape[1] == required_resolution[0], \"Video must be of resolution {}\".format(required_resolution)\n            # BGR -> RGB\n            frames.append(frame[..., ::-1])\n            env_action, _ = json_action_to_env_action(json_data[json_index])\n            recorded_actions.append(env_action)\n            json_index += 1\n        frames = np.stack(frames)\n        print(\"=== Predicting actions ===\")\n        predicted_actions = agent.predict_actions(frames)\n        for i in range(n_frames):\n            frame = frames[i]\n            recorded_action = recorded_actions[i]\n            cv2.putText(\n                frame,\n                f\"name: prediction (true)\",",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:144-170"
    },
    "137": {
        "file_id": 7,
        "content": "Loading and preprocessing video frames, converting actions from JSON to environment actions, predicting actions using the agent model, and displaying predictions on video frames.",
        "type": "comment"
    },
    "138": {
        "file_id": 7,
        "content": "                (10, 10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.4,\n                (255, 255, 255),\n                1\n            )\n            for y, (action_name, action_array) in enumerate(predicted_actions.items()):\n                current_prediction = action_array[0, i]\n                cv2.putText(\n                    frame,\n                    f\"{action_name}: {current_prediction} ({recorded_action[action_name]})\",\n                    (10, 25 + y * 12),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.35,\n                    (255, 255, 255),\n                    1\n                )\n            # RGB -> BGR again...\n            cv2.imshow(\"MineRL IDM model predictions\", frame[..., ::-1])\n            cv2.waitKey(0)\n    cv2.destroyAllWindows()\nif __name__ == \"__main__\":\n    parser = ArgumentParser(\"Run IDM on MineRL recordings.\")\n    parser.add_argument(\"--weights\", type=str, required=True, help=\"Path to the '.weights' file to be loaded.\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to the '.model' file to be loaded.\")",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:171-197"
    },
    "139": {
        "file_id": 7,
        "content": "The code is displaying IDM model predictions on a video stream, with text labels for each action. It uses OpenCV's putText function to draw the labels on the frame and then displays the resulting image using cv2.imshow and waitKey functions. The code also takes arguments for weights and model files required to load the model.",
        "type": "comment"
    },
    "140": {
        "file_id": 7,
        "content": "    parser.add_argument(\"--video-path\", type=str, required=True, help=\"Path to a .mp4 file (Minecraft recording).\")\n    parser.add_argument(\"--jsonl-path\", type=str, required=True, help=\"Path to a .jsonl file (Minecraft recording).\")\n    parser.add_argument(\"--n-frames\", type=int, default=128, help=\"Number of frames to process at a time.\")\n    parser.add_argument(\"--n-batches\", type=int, default=10, help=\"Number of batches (n-frames) to process for visualization.\")\n    args = parser.parse_args()\n    main(args.model, args.weights, args.video_path, args.jsonl_path, args.n_batches, args.n_frames)",
        "type": "code",
        "location": "/run_inverse_dynamics_model.py:198-205"
    },
    "141": {
        "file_id": 7,
        "content": "This code sets up command line arguments for video path, JSONL file path, number of frames to process at a time, and the number of batches to process for visualization. It then parses these arguments into \"args\" and calls the main function with these arguments.",
        "type": "comment"
    },
    "142": {
        "file_id": 8,
        "content": "/lib/action_head.py",
        "type": "filepath"
    },
    "143": {
        "file_id": 8,
        "content": "The code introduces an `ActionHead` abstract base class for reinforcement learning action heads, including methods such as logprob, sample, entropy, and kl_divergence. It supports Discrete, Real, and DictType action spaces and has reset parameters and forward pass functionality.",
        "type": "summary"
    },
    "144": {
        "file_id": 8,
        "content": "import logging\nfrom typing import Any, Tuple\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom gym3.types import DictType, Discrete, Real, TensorType, ValType\nLOG0 = -100\ndef fan_in_linear(module: nn.Module, scale=1.0, bias=True):\n    \"\"\"Fan-in init\"\"\"\n    module.weight.data *= scale / module.weight.norm(dim=1, p=2, keepdim=True)\n    if bias:\n        module.bias.data *= 0\nclass ActionHead(nn.Module):\n    \"\"\"Abstract base class for action heads compatible with forc\"\"\"\n    def forward(self, input_data: torch.Tensor) -> Any:\n        \"\"\"\n        Just a forward pass through this head\n        :returns pd_params - parameters describing the probability distribution\n        \"\"\"\n        raise NotImplementedError\n    def logprob(self, action_sample: torch.Tensor, pd_params: torch.Tensor) -> torch.Tensor:\n        \"\"\"Logartithm of probability of sampling `action_sample` from a probability described by `pd_params`\"\"\"\n        raise NotImplementedError\n    def entropy(self, pd_params: torch.Tensor) -> torch.Tensor:",
        "type": "code",
        "location": "/lib/action_head.py:1-36"
    },
    "145": {
        "file_id": 8,
        "content": "This code defines an ActionHead class and a fan_in_linear function. ActionHead is an abstract base class for action heads, which are used in reinforcement learning to determine the optimal actions. The fan_in_linear function initializes the weights of the linear layer using the Fan-in initialization method.",
        "type": "comment"
    },
    "146": {
        "file_id": 8,
        "content": "        \"\"\"Entropy of this distribution\"\"\"\n        raise NotImplementedError\n    def sample(self, pd_params: torch.Tensor, deterministic: bool = False) -> Any:\n        \"\"\"\n        Draw a sample from probability distribution given by those params\n        :param pd_params Parameters of a probability distribution\n        :param deterministic Whether to return a stochastic sample or deterministic mode of a distribution\n        \"\"\"\n        raise NotImplementedError\n    def kl_divergence(self, params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n        \"\"\"KL divergence between two distribution described by these two params\"\"\"\n        raise NotImplementedError\nclass DiagGaussianActionHead(ActionHead):\n    \"\"\"\n    Action head where actions are normally distributed uncorrelated variables with specific means and variances.\n    Means are calculated directly from the network while standard deviations are a parameter of this module\n    \"\"\"\n    LOG2PI = np.log(2.0 * np.pi)\n    def __init__(self, input_dim: int, num_dimensions: int):",
        "type": "code",
        "location": "/lib/action_head.py:37-63"
    },
    "147": {
        "file_id": 8,
        "content": "This code defines an abstract base class `ActionHead` for entropy, sampling, and KL divergence calculation. It raises a NotImplementedError since subclasses should provide the actual implementation of these methods. The `DiagGaussianActionHead` class is also defined, which inherits from `ActionHead`, representing action heads with normally distributed uncorrelated variables based on network output mean and standard deviation parameters.",
        "type": "comment"
    },
    "148": {
        "file_id": 8,
        "content": "        super().__init__()\n        self.input_dim = input_dim\n        self.num_dimensions = num_dimensions\n        self.linear_layer = nn.Linear(input_dim, num_dimensions)\n        self.log_std = nn.Parameter(torch.zeros(num_dimensions), requires_grad=True)\n    def reset_parameters(self):\n        init.orthogonal_(self.linear_layer.weight, gain=0.01)\n        init.constant_(self.linear_layer.bias, 0.0)\n    def forward(self, input_data: torch.Tensor, mask=None) -> torch.Tensor:\n        assert not mask, \"Can not use a mask in a gaussian action head\"\n        means = self.linear_layer(input_data)\n        # Unsqueeze many times to get to the same shape\n        logstd = self.log_std[(None,) * (len(means.shape) - 1)]\n        mean_view, logstd = torch.broadcast_tensors(means, logstd)\n        return torch.stack([mean_view, logstd], dim=-1)\n    def logprob(self, action_sample: torch.Tensor, pd_params: torch.Tensor) -> torch.Tensor:\n        \"\"\"Log-likelihood\"\"\"\n        means = pd_params[..., 0]\n        log_std = pd_params[..., 1]",
        "type": "code",
        "location": "/lib/action_head.py:64-89"
    },
    "149": {
        "file_id": 8,
        "content": "Initializes an action head with specified input and output dimensions, sets the linear layer's weight and bias using orthogonal initialization and assigns them to None respectively.\nDefines methods to reset parameters for the action head, forward propagates data through linear layer to obtain means, and calculates log probabilities of action samples given parameters.",
        "type": "comment"
    },
    "150": {
        "file_id": 8,
        "content": "        std = torch.exp(log_std)\n        z_score = (action_sample - means) / std\n        return -(0.5 * ((z_score ** 2 + self.LOG2PI).sum(dim=-1)) + log_std.sum(dim=-1))\n    def entropy(self, pd_params: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Categorical distribution entropy calculation - sum probs * log(probs).\n        In case of diagonal gaussian distribution - 1/2 log(2 pi e sigma^2)\n        \"\"\"\n        log_std = pd_params[..., 1]\n        return (log_std + 0.5 * (self.LOG2PI + 1)).sum(dim=-1)\n    def sample(self, pd_params: torch.Tensor, deterministic: bool = False) -> torch.Tensor:\n        means = pd_params[..., 0]\n        log_std = pd_params[..., 1]\n        if deterministic:\n            return means\n        else:\n            return torch.randn_like(means) * torch.exp(log_std) + means\n    def kl_divergence(self, params_q: torch.Tensor, params_p: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Categorical distribution KL divergence calculation\n        KL(Q || P) = sum Q_i log (Q_i / P_i)\n        Formula is:",
        "type": "code",
        "location": "/lib/action_head.py:91-119"
    },
    "151": {
        "file_id": 8,
        "content": "Line 90: Calculate standard deviation from log_std\nLine 116: Calculate z-score for action sample\nLine 117: Return negative of sum of log probabilities\n\nComment for code: This code calculates the categorical distribution entropy, sample from a diagonal Gaussian distribution, and KL divergence for two sets of parameters.",
        "type": "comment"
    },
    "152": {
        "file_id": 8,
        "content": "        log(sigma_p) - log(sigma_q) + (sigma_q^2 + (mu_q - mu_p)^2))/(2 * sigma_p^2)\n        \"\"\"\n        means_q = params_q[..., 0]\n        log_std_q = params_q[..., 1]\n        means_p = params_p[..., 0]\n        log_std_p = params_p[..., 1]\n        std_q = torch.exp(log_std_q)\n        std_p = torch.exp(log_std_p)\n        kl_div = log_std_p - log_std_q + (std_q ** 2 + (means_q - means_p) ** 2) / (2.0 * std_p ** 2) - 0.5\n        return kl_div.sum(dim=-1, keepdim=True)\nclass CategoricalActionHead(ActionHead):\n    \"\"\"Action head with categorical actions\"\"\"\n    def __init__(\n        self, input_dim: int, shape: Tuple[int], num_actions: int, builtin_linear_layer: bool = True, temperature: float = 1.0\n    ):\n        super().__init__()\n        self.input_dim = input_dim\n        self.num_actions = num_actions\n        self.output_shape = shape + (num_actions,)\n        self.temperature = temperature\n        if builtin_linear_layer:\n            self.linear_layer = nn.Linear(input_dim, np.prod(self.output_shape))\n        else:",
        "type": "code",
        "location": "/lib/action_head.py:120-151"
    },
    "153": {
        "file_id": 8,
        "content": "This code defines an ActionHead class with categorical actions. It initializes the action head with input_dim, num_actions, shape, builtin_linear_layer (optional), and temperature parameters. If builtin_linear_layer is True, it uses a linear layer for feature extraction. The output shape is determined by the input shape and number of actions.",
        "type": "comment"
    },
    "154": {
        "file_id": 8,
        "content": "            assert (\n                input_dim == num_actions\n            ), f\"If input_dim ({input_dim}) != num_actions ({num_actions}), you need a linear layer to convert them.\"\n            self.linear_layer = None\n    def reset_parameters(self):\n        if self.linear_layer is not None:\n            init.orthogonal_(self.linear_layer.weight, gain=0.01)\n            init.constant_(self.linear_layer.bias, 0.0)\n            finit.fan_in_linear(self.linear_layer, scale=0.01)\n    def forward(self, input_data: torch.Tensor, mask=None) -> Any:\n        if self.linear_layer is not None:\n            flat_out = self.linear_layer(input_data)\n        else:\n            flat_out = input_data\n        shaped_out = flat_out.reshape(flat_out.shape[:-1] + self.output_shape)\n        shaped_out /= self.temperature\n        if mask is not None:\n            shaped_out[~mask] = LOG0\n        # Convert to float32 to avoid RuntimeError: \"log_softmax_lastdim_kernel_impl\" not implemented for 'Half'\n        return F.log_softmax(shaped_out.float(), dim=-1)",
        "type": "code",
        "location": "/lib/action_head.py:152-174"
    },
    "155": {
        "file_id": 8,
        "content": "This code defines a class for an action head, which is responsible for outputting action probabilities from input data. It asserts that the input dimension matches the number of actions, and if a linear layer is not None, it initializes its parameters orthogonally with gain 0.01 and sets the bias to 0. The forward function computes the output by either passing the input through a linear layer or using the input directly, reshapes the result based on the output shape, scales the result by temperature, applies a mask if provided, and then returns the log softmax of the shaped output as float32.",
        "type": "comment"
    },
    "156": {
        "file_id": 8,
        "content": "    def logprob(self, actions: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n        value = actions.long().unsqueeze(-1)\n        value, log_pmf = torch.broadcast_tensors(value, logits)\n        value = value[..., :1]\n        result = log_pmf.gather(-1, value).squeeze(-1)\n        # result is per-entry, still of size self.output_shape[:-1]; we need to reduce of the rest of it.\n        for _ in self.output_shape[:-1]:\n            result = result.sum(dim=-1)\n        return result\n    def entropy(self, logits: torch.Tensor) -> torch.Tensor:\n        \"\"\"Categorical distribution entropy calculation - sum probs * log(probs)\"\"\"\n        probs = torch.exp(logits)\n        entropy = -torch.sum(probs * logits, dim=-1)\n        # entropy is per-entry, still of size self.output_shape[:-1]; we need to reduce of the rest of it.\n        for _ in self.output_shape[:-1]:\n            entropy = entropy.sum(dim=-1)\n        return entropy\n    def sample(self, logits: torch.Tensor, deterministic: bool = False) -> Any:\n        if deterministic:",
        "type": "code",
        "location": "/lib/action_head.py:176-196"
    },
    "157": {
        "file_id": 8,
        "content": "The code contains three functions: `logprob`, `entropy`, and `sample`. \n- The `logprob` function calculates the log probability of a given set of actions against the provided logits. It returns the result in torch format.\n- The `entropy` function calculates the entropy of a categorical distribution from the given logits. It also returns the entropy in torch format.\n- The `sample` function generates a sample from the distribution represented by the given logits. If `deterministic` is set to True, it will always return the same value.",
        "type": "comment"
    },
    "158": {
        "file_id": 8,
        "content": "            return torch.argmax(logits, dim=-1)\n        else:\n            # Gumbel-Softmax trick.\n            u = torch.rand_like(logits)\n            # In float16, if you have around 2^{float_mantissa_bits} logits, sometimes you'll sample 1.0\n            # Then the log(-log(1.0)) will give -inf when it should give +inf\n            # This is a silly hack to get around that.\n            # This hack does not skew the probability distribution, because this event can't possibly win the argmax.\n            u[u == 1.0] = 0.999\n            return torch.argmax(logits - torch.log(-torch.log(u)), dim=-1)\n    def kl_divergence(self, logits_q: torch.Tensor, logits_p: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Categorical distribution KL divergence calculation\n        KL(Q || P) = sum Q_i log (Q_i / P_i)\n        When talking about logits this is:\n        sum exp(Q_i) * (Q_i - P_i)\n        \"\"\"\n        kl = (torch.exp(logits_q) * (logits_q - logits_p)).sum(-1, keepdim=True)\n        # kl is per-entry, still of size self.output_shape; we need to reduce of the rest of it.",
        "type": "code",
        "location": "/lib/action_head.py:197-217"
    },
    "159": {
        "file_id": 8,
        "content": "Returns the index with maximum value in logits\"\n\"Applies Gumbel-Softmax trick for training with float16 precision\"\n\"Calculates KL divergence between two categorical distributions using logits",
        "type": "comment"
    },
    "160": {
        "file_id": 8,
        "content": "        for _ in self.output_shape[:-1]:\n            kl = kl.sum(dim=-2)  # dim=-2 because we use keepdim=True above.\n        return kl\nclass DictActionHead(nn.ModuleDict):\n    \"\"\"Action head with multiple sub-actions\"\"\"\n    def reset_parameters(self):\n        for subhead in self.values():\n            subhead.reset_parameters()\n    def forward(self, input_data: torch.Tensor, **kwargs) -> Any:\n        \"\"\"\n        :param kwargs: each kwarg should be a dict with keys corresponding to self.keys()\n                e.g. if this ModuleDict has submodules keyed by 'A', 'B', and 'C', we could call:\n                    forward(input_data, foo={'A': True, 'C': False}, bar={'A': 7}}\n                Then children will be called with:\n                    A: forward(input_data, foo=True, bar=7)\n                    B: forward(input_data)\n                    C: forward(input_Data, foo=False)\n        \"\"\"\n        result = {}\n        for head_name, subhead in self.items():\n            head_kwargs = {\n                kwarg_name: kwarg[head_name]",
        "type": "code",
        "location": "/lib/action_head.py:218-243"
    },
    "161": {
        "file_id": 8,
        "content": "This code defines a DictActionHead class, which is an action head with multiple sub-actions. The class has methods to reset parameters and perform forward pass. During the forward pass, it takes input data and optional keyword arguments, and calls the forward method on each of its submodules using the provided keyword arguments. The results from all submodules are stored in a dictionary.",
        "type": "comment"
    },
    "162": {
        "file_id": 8,
        "content": "                for kwarg_name, kwarg in kwargs.items()\n                if kwarg is not None and head_name in kwarg\n            }\n            result[head_name] = subhead(input_data, **head_kwargs)\n        return result\n    def logprob(self, actions: torch.Tensor, logits: torch.Tensor) -> torch.Tensor:\n        return sum(subhead.logprob(actions[k], logits[k]) for k, subhead in self.items())\n    def sample(self, logits: torch.Tensor, deterministic: bool = False) -> Any:\n        return {k: subhead.sample(logits[k], deterministic) for k, subhead in self.items()}\n    def entropy(self, logits: torch.Tensor) -> torch.Tensor:\n        return sum(subhead.entropy(logits[k]) for k, subhead in self.items())\n    def kl_divergence(self, logits_q: torch.Tensor, logits_p: torch.Tensor) -> torch.Tensor:\n        return sum(subhead.kl_divergence(logits_q[k], logits_p[k]) for k, subhead in self.items())\ndef make_action_head(ac_space: ValType, pi_out_size: int, temperature: float = 1.0):\n    \"\"\"Helper function to create an action head corresponding to the environment action space\"\"\"",
        "type": "code",
        "location": "/lib/action_head.py:244-264"
    },
    "163": {
        "file_id": 8,
        "content": "The code defines an action head class that contains sub-heads corresponding to the environment's action space. It supports logprob, sample, entropy, and kl_divergence methods on a batch of actions and logits. The make_action_head function creates an action head based on the given action space and output size of the policy network.",
        "type": "comment"
    },
    "164": {
        "file_id": 8,
        "content": "    if isinstance(ac_space, TensorType):\n        if isinstance(ac_space.eltype, Discrete):\n            return CategoricalActionHead(pi_out_size, ac_space.shape, ac_space.eltype.n, temperature=temperature)\n        elif isinstance(ac_space.eltype, Real):\n            if temperature != 1.0:\n                logging.warning(\"Non-1 temperature not implemented for DiagGaussianActionHead.\")\n            assert len(ac_space.shape) == 1, \"Nontrivial shapes not yet implemented.\"\n            return DiagGaussianActionHead(pi_out_size, ac_space.shape[0])\n    elif isinstance(ac_space, DictType):\n        return DictActionHead({k: make_action_head(v, pi_out_size, temperature) for k, v in ac_space.items()})\n    raise NotImplementedError(f\"Action space of type {type(ac_space)} is not supported\")",
        "type": "code",
        "location": "/lib/action_head.py:265-275"
    },
    "165": {
        "file_id": 8,
        "content": "Checks the type of action space and returns a corresponding ActionHead object. Supports Discrete, Real, and DictType action spaces. Non-1 temperature and nontrivial shape actions are not implemented yet.",
        "type": "comment"
    },
    "166": {
        "file_id": 9,
        "content": "/lib/action_mapping.py",
        "type": "filepath"
    },
    "167": {
        "file_id": 9,
        "content": "This code organizes player inputs in a video game using action mappings, manages camera actions, handles assertion checks and conversions for different action spaces like buttons, cameras, inventory keys, and factored action space mapping.",
        "type": "summary"
    },
    "168": {
        "file_id": 9,
        "content": "import abc\nimport itertools\nfrom collections import OrderedDict\nfrom typing import Dict, List\nimport numpy as np\nfrom gym3.types import DictType, Discrete, TensorType\nfrom lib.actions import Buttons\nclass ActionMapping(abc.ABC):\n    \"\"\"Class that maps between the standard MC factored action space and a new one you define!\n    :param n_camera_bins: Need to specify this to define the original ac space for stats code\n    \"\"\"\n    # This is the default buttons groups, it can be changed for your action space\n    BUTTONS_GROUPS = OrderedDict(\n        hotbar=[\"none\"] + [f\"hotbar.{i}\" for i in range(1, 10)],\n        fore_back=[\"none\", \"forward\", \"back\"],\n        left_right=[\"none\", \"left\", \"right\"],\n        sprint_sneak=[\"none\", \"sprint\", \"sneak\"],\n        use=[\"none\", \"use\"],\n        drop=[\"none\", \"drop\"],\n        attack=[\"none\", \"attack\"],\n        jump=[\"none\", \"jump\"],\n    )\n    def __init__(self, n_camera_bins: int = 11):\n        assert n_camera_bins % 2 == 1, \"n_camera_bins should be odd\"\n        self.n_camera_bins = n_camera_bins",
        "type": "code",
        "location": "/lib/action_mapping.py:1-32"
    },
    "169": {
        "file_id": 9,
        "content": "This code defines a class \"ActionMapping\" that maps between the standard Minecraft action space and a new one defined by the user. It uses ordered dictionaries to represent different action groups such as buttons, and requires an odd number of camera bins for initialization.",
        "type": "comment"
    },
    "170": {
        "file_id": 9,
        "content": "        self.camera_null_bin = n_camera_bins // 2\n        self.stats_ac_space = DictType(\n            **{\n                \"buttons\": TensorType(shape=(len(Buttons.ALL),), eltype=Discrete(2)),\n                \"camera\": TensorType(shape=(2,), eltype=Discrete(n_camera_bins)),\n            }\n        )\n    @abc.abstractmethod\n    def from_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts a factored action (ac) to the new space\n        :param ac: Dictionary of actions that must have a batch dimension\n        \"\"\"\n        pass\n    @abc.abstractmethod\n    def to_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts an action in the new space (ac) to the factored action space.\n        :param ac: Dictionary of actions that must have a batch dimension\n        \"\"\"\n        pass\n    @abc.abstractmethod\n    def get_action_space_update(self):\n        \"\"\"Return a magym (gym3) action space. This will be used to update the env action space.\"\"\"\n        pass\n    @abc.abstractmethod\n    def get_zero_action(self):\n        \"\"\"Return the zero or null action for this action space\"\"\"",
        "type": "code",
        "location": "/lib/action_mapping.py:33-64"
    },
    "171": {
        "file_id": 9,
        "content": "This code defines an abstract base class for mapping actions to a new space. It includes methods for converting factored actions to the new space, converting actions in the new space back to the factored action space, returning a gym action space for updating the environment, and returning the null or zero action for this action space.",
        "type": "comment"
    },
    "172": {
        "file_id": 9,
        "content": "        pass\n    def factored_buttons_to_groups(self, ac_buttons: np.ndarray, button_group: List[str]) -> List[str]:\n        \"\"\"For a mutually exclusive group of buttons in button_group, find which option\n        in the group was chosen. Assumes that each button group has the option of 'none'\n        meaning that no button in the group was pressed.\n        :param ac_buttons: button actions from the factored action space. Should dims [B, len(Buttons.ALL)]\n        :param button_group: List of buttons in a mutually exclusive group. Each item in the\n            list should appear in Buttons.ALL except for the special case 'none' which means\n            no button in the group was pressed. e.g. ['none', 'forward', 'back']. For now\n            'none' must be the first element of button_group\n        Returns a list of length B, where each element is an item from button_group.\n        \"\"\"\n        assert ac_buttons.shape[1] == len(\n            Buttons.ALL\n        ), f\"There should be {len(Buttons.ALL)} buttons in the factored buttons space\"",
        "type": "code",
        "location": "/lib/action_mapping.py:65-82"
    },
    "173": {
        "file_id": 9,
        "content": "This function takes in button actions from a factored action space and a list of mutually exclusive buttons. It returns a list indicating which button (or none if no button was pressed) was chosen for each item in the input array, given that each group has the option of 'none'. The function checks if the shape of the input matches the expected number of buttons.",
        "type": "comment"
    },
    "174": {
        "file_id": 9,
        "content": "        assert button_group[0] == \"none\", \"This function only works if 'none' is in button_group\"\n        # Actions in ac_buttons with order according to button_group\n        group_indices = [Buttons.ALL.index(b) for b in button_group if b != \"none\"]\n        ac_choices = ac_buttons[:, group_indices]\n        # Special cases for forward/back, left/right where mutual press means do neither\n        if \"forward\" in button_group and \"back\" in button_group:\n            ac_choices[np.all(ac_choices, axis=-1)] = 0\n        if \"left\" in button_group and \"right\" in button_group:\n            ac_choices[np.all(ac_choices, axis=-1)] = 0\n        ac_non_zero = np.where(ac_choices)\n        ac_choice = [\"none\" for _ in range(ac_buttons.shape[0])]\n        # Iterate over the non-zero indices so that if two buttons in a group were pressed at the same time\n        # we give priority to the button later in the group. E.g. if hotbar.1 and hotbar.2 are pressed during the same\n        # timestep, hotbar.2 is marked as pressed",
        "type": "code",
        "location": "/lib/action_mapping.py:83-97"
    },
    "175": {
        "file_id": 9,
        "content": "Ensures function works only when 'none' is in button_group. Maps non-zero action button indices to corresponding actions, handling special cases of mutual press for forward/back and left/right. Prioritizes later buttons in group if pressed at the same time.",
        "type": "comment"
    },
    "176": {
        "file_id": 9,
        "content": "        for index, action in zip(ac_non_zero[0], ac_non_zero[1]):\n            ac_choice[index] = button_group[action + 1]  # the zero'th index will mean no button pressed\n        return ac_choice\nclass IDMActionMapping(ActionMapping):\n    \"\"\"For IDM, but essentially this is just an identity mapping\"\"\"\n    def from_factored(self, ac: Dict) -> Dict:\n        return ac\n    def to_factored(self, ac: Dict) -> Dict:\n        return ac\n    def get_action_space_update(self):\n        \"\"\"Return a magym (gym3) action space. This will be used to update the env action space.\"\"\"\n        return {\n            \"buttons\": TensorType(shape=(len(Buttons.ALL),), eltype=Discrete(2)),\n            \"camera\": TensorType(shape=(2,), eltype=Discrete(self.n_camera_bins)),\n        }\n    def get_zero_action(self):\n        raise NotImplementedError()\nclass CameraHierarchicalMapping(ActionMapping):\n    \"\"\"Buttons are joint as in ButtonsJointMapping, but now a camera on/off meta action is added into this joint space.\n    When this meta action is triggered, the separate camera head chooses a camera action which is also now a joint space.",
        "type": "code",
        "location": "/lib/action_mapping.py:98-122"
    },
    "177": {
        "file_id": 9,
        "content": "This code defines two classes, IDMActionMapping and CameraHierarchicalMapping, which are action mappings used in a video game. The classes define methods to convert actions between factored and non-factored representations, get an action space update, and handle zero actions. These classes seem to be part of a larger system for mapping player inputs to actions in the game environment.",
        "type": "comment"
    },
    "178": {
        "file_id": 9,
        "content": "    :param n_camera_bins: number of camera bins in the factored space\n    \"\"\"\n    # Add camera meta action to BUTTONS_GROUPS\n    BUTTONS_GROUPS = ActionMapping.BUTTONS_GROUPS.copy()\n    BUTTONS_GROUPS[\"camera\"] = [\"none\", \"camera\"]\n    BUTTONS_COMBINATIONS = list(itertools.product(*BUTTONS_GROUPS.values())) + [\"inventory\"]\n    BUTTONS_COMBINATION_TO_IDX = {comb: i for i, comb in enumerate(BUTTONS_COMBINATIONS)}\n    BUTTONS_IDX_TO_COMBINATION = {i: comb for i, comb in enumerate(BUTTONS_COMBINATIONS)}\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.camera_groups = OrderedDict(\n            camera_x=[f\"camera_x{i}\" for i in range(self.n_camera_bins)],\n            camera_y=[f\"camera_y{i}\" for i in range(self.n_camera_bins)],\n        )\n        self.camera_combinations = list(itertools.product(*self.camera_groups.values()))\n        self.camera_combination_to_idx = {comb: i for i, comb in enumerate(self.camera_combinations)}\n        self.camera_idx_to_combination = {i: comb for i, comb in enumerate(self.camera_combinations)}",
        "type": "code",
        "location": "/lib/action_mapping.py:124-142"
    },
    "179": {
        "file_id": 9,
        "content": "This code adds camera meta actions to the BUTTONS_GROUPS and defines functions for mapping between button combinations, indices, and names.",
        "type": "comment"
    },
    "180": {
        "file_id": 9,
        "content": "        self.camera_null_idx = self.camera_combination_to_idx[\n            (f\"camera_x{self.camera_null_bin}\", f\"camera_y{self.camera_null_bin}\")\n        ]\n        self._null_action = {\n            \"buttons\": self.BUTTONS_COMBINATION_TO_IDX[tuple(\"none\" for _ in range(len(self.BUTTONS_GROUPS)))]\n        }\n        self._precompute_to_factored()\n    def _precompute_to_factored(self):\n        \"\"\"Precompute the joint action -> factored action matrix.\"\"\"\n        button_dim = self.stats_ac_space[\"buttons\"].size\n        self.BUTTON_IDX_TO_FACTORED = np.zeros((len(self.BUTTONS_IDX_TO_COMBINATION), button_dim), dtype=int)\n        self.BUTTON_IDX_TO_CAMERA_META_OFF = np.zeros((len(self.BUTTONS_IDX_TO_COMBINATION)), dtype=bool)\n        self.CAMERA_IDX_TO_FACTORED = np.zeros((len(self.camera_idx_to_combination), 2), dtype=int)\n        # Pre compute Buttons\n        for jnt_ac, button_comb in self.BUTTONS_IDX_TO_COMBINATION.items():\n            new_button_ac = np.zeros(len(Buttons.ALL), dtype=\"i\")\n            if button_comb == \"inventory\":",
        "type": "code",
        "location": "/lib/action_mapping.py:143-161"
    },
    "181": {
        "file_id": 9,
        "content": "Code chunk sets up arrays for button and camera action mappings.\nThe code defines button and camera indices, initializes arrays to store the factored actions for each joint action, and begins processing the button combinations.",
        "type": "comment"
    },
    "182": {
        "file_id": 9,
        "content": "                new_button_ac[Buttons.ALL.index(\"inventory\")] = 1\n            else:\n                for group_choice in button_comb[:-1]:  # Last one is camera\n                    if group_choice != \"none\":\n                        new_button_ac[Buttons.ALL.index(group_choice)] = 1\n                if button_comb[-1] != \"camera\":  # This means camera meta action is off\n                    self.BUTTON_IDX_TO_CAMERA_META_OFF[jnt_ac] = True\n            self.BUTTON_IDX_TO_FACTORED[jnt_ac] = new_button_ac\n        # Pre compute camera\n        for jnt_ac, camera_comb in self.camera_idx_to_combination.items():\n            new_camera_ac = np.ones((2), dtype=\"i\") * self.camera_null_bin\n            new_camera_ac[0] = self.camera_groups[\"camera_x\"].index(camera_comb[0])\n            new_camera_ac[1] = self.camera_groups[\"camera_y\"].index(camera_comb[1])\n            self.CAMERA_IDX_TO_FACTORED[jnt_ac] = new_camera_ac\n    def from_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts a factored action (ac) to the new space. Assumes ac has a batch dim\"\"\"",
        "type": "code",
        "location": "/lib/action_mapping.py:162-180"
    },
    "183": {
        "file_id": 9,
        "content": "Code is creating factored representations of action and camera combinations for each joint. It initializes new_button_ac to 1 for all inventory buttons, then checks if any other groups are selected and assigns those indices to 1 in new_button_ac. If the last combination is not \"camera\", it sets the camera_meta_off flag. Then it creates new_camera_ac with indices based on the camera group combinations and stores these factored representations for both action and camera in their respective dictionaries.",
        "type": "comment"
    },
    "184": {
        "file_id": 9,
        "content": "        assert ac[\"camera\"].ndim == 2, f\"bad camera label, {ac['camera']}\"\n        assert ac[\"buttons\"].ndim == 2, f\"bad buttons label, {ac['buttons']}\"\n        # Get button choices for everything but camera\n        choices_by_group = OrderedDict(\n            (k, self.factored_buttons_to_groups(ac[\"buttons\"], v)) for k, v in self.BUTTONS_GROUPS.items() if k != \"camera\"\n        )\n        # Set camera \"on off\" action based on whether non-null camera action was given\n        camera_is_null = np.all(ac[\"camera\"] == self.camera_null_bin, axis=1)\n        choices_by_group[\"camera\"] = [\"none\" if is_null else \"camera\" for is_null in camera_is_null]\n        new_button_ac = []\n        new_camera_ac = []\n        for i in range(ac[\"buttons\"].shape[0]):\n            # Buttons\n            key = tuple([v[i] for v in choices_by_group.values()])\n            if ac[\"buttons\"][i, Buttons.ALL.index(\"inventory\")] == 1:\n                key = \"inventory\"\n            new_button_ac.append(self.BUTTONS_COMBINATION_TO_IDX[key])\n            # Camera -- inventory is also exclusive with camera",
        "type": "code",
        "location": "/lib/action_mapping.py:181-200"
    },
    "185": {
        "file_id": 9,
        "content": "This code is performing an assertion check to ensure that the \"camera\" and \"buttons\" labels have the correct dimensions. It then creates a dictionary of button choices for each group except camera, sets the camera action based on whether a non-null camera action was given, and finally generates new arrays of button and camera actions based on the choices.",
        "type": "comment"
    },
    "186": {
        "file_id": 9,
        "content": "            if key == \"inventory\":\n                key = (\n                    f\"camera_x{self.camera_null_bin}\",\n                    f\"camera_y{self.camera_null_bin}\",\n                )\n            else:\n                key = (f\"camera_x{ac['camera'][i][0]}\", f\"camera_y{ac['camera'][i][1]}\")\n            new_camera_ac.append(self.camera_combination_to_idx[key])\n        return dict(\n            buttons=np.array(new_button_ac)[:, None],\n            camera=np.array(new_camera_ac)[:, None],\n        )\n    def to_factored(self, ac: Dict) -> Dict:\n        \"\"\"Converts an action in the new space (ac) to the factored action space. Assumes ac has a batch dim\"\"\"\n        assert ac[\"camera\"].shape[-1] == 1\n        assert ac[\"buttons\"].shape[-1] == 1\n        new_button_ac = self.BUTTON_IDX_TO_FACTORED[np.squeeze(ac[\"buttons\"], -1)]\n        camera_off = self.BUTTON_IDX_TO_CAMERA_META_OFF[np.squeeze(ac[\"buttons\"], -1)]\n        new_camera_ac = self.CAMERA_IDX_TO_FACTORED[np.squeeze(ac[\"camera\"], -1)]\n        new_camera_ac[camera_off] = self.camera_null_bin",
        "type": "code",
        "location": "/lib/action_mapping.py:201-223"
    },
    "187": {
        "file_id": 9,
        "content": "This code is converting an action in the new space (ac) to the factored action space. It first checks if the \"inventory\" key is present, and if so, uses a specific key format. For other keys, it uses a different key format. Then it appends the camera indices to a list. The function returns a dictionary with buttons and cameras in the new action space. If the input action has a batch dimension, the code asserts that the shape of both \"camera\" and \"buttons\" are 1, squeezes them, maps the button indices to factored action space, calculates camera offsets, maps the camera indices to factored action space, and replaces the null camera values with \"camera_null_bin\".",
        "type": "comment"
    },
    "188": {
        "file_id": 9,
        "content": "        return dict(buttons=new_button_ac, camera=new_camera_ac)\n    def get_action_space_update(self):\n        return {\n            \"camera\": TensorType(shape=(1,), eltype=Discrete(len(self.camera_combinations))),\n            \"buttons\": TensorType(shape=(1,), eltype=Discrete(len(self.BUTTONS_COMBINATIONS))),\n        }\n    def get_zero_action(self):\n        return self._null_action",
        "type": "code",
        "location": "/lib/action_mapping.py:225-234"
    },
    "189": {
        "file_id": 9,
        "content": "This code defines a class with three methods. The first method returns a dictionary containing the \"buttons\" and \"camera\" actions. The second method specifies the action space update, defining the shape and type for both \"camera\" and \"buttons\". The third method returns a null action.",
        "type": "comment"
    },
    "190": {
        "file_id": 10,
        "content": "/lib/actions.py",
        "type": "filepath"
    },
    "191": {
        "file_id": 10,
        "content": "The code includes Minecraft action classes with quantization schemes and an ActionTransformer, along with three functions for mapping item IDs to names and converting environment data to policy format.",
        "type": "summary"
    },
    "192": {
        "file_id": 10,
        "content": "import attr\nimport minerl.herobraine.hero.mc as mc\nimport numpy as np\nfrom lib.minecraft_util import store_args\nclass Buttons:\n    ATTACK = \"attack\"\n    BACK = \"back\"\n    FORWARD = \"forward\"\n    JUMP = \"jump\"\n    LEFT = \"left\"\n    RIGHT = \"right\"\n    SNEAK = \"sneak\"\n    SPRINT = \"sprint\"\n    USE = \"use\"\n    DROP = \"drop\"\n    INVENTORY = \"inventory\"\n    ALL = [\n        ATTACK,\n        BACK,\n        FORWARD,\n        JUMP,\n        LEFT,\n        RIGHT,\n        SNEAK,\n        SPRINT,\n        USE,\n        DROP,\n        INVENTORY,\n    ] + [f\"hotbar.{i}\" for i in range(1, 10)]\nclass SyntheticButtons:\n    # Composite / scripted actions\n    CHANNEL_ATTACK = \"channel-attack\"\n    ALL = [CHANNEL_ATTACK]\nclass QuantizationScheme:\n    LINEAR = \"linear\"\n    MU_LAW = \"mu_law\"\n@attr.s(auto_attribs=True)\nclass CameraQuantizer:\n    \"\"\"\n    A camera quantizer that discretizes and undiscretizes a continuous camera input with y (pitch) and x (yaw) components.\n    Parameters:\n    - camera_binsize: The size of the bins used for quantization. In case of mu-law quantization, it corresponds to the average binsize.",
        "type": "code",
        "location": "/lib/actions.py:1-54"
    },
    "193": {
        "file_id": 10,
        "content": "This code defines classes for various action types and a camera quantizer in the context of Minecraft gameplay. The Buttons class represents different action buttons like attack, jump, inventory, etc. SyntheticButtons includes composite/scripted actions. QuantizationScheme has options for linear or mu-law quantization. CameraQuantizer is responsible for discretizing and undiscretizing continuous camera input (pitch and yaw).",
        "type": "comment"
    },
    "194": {
        "file_id": 10,
        "content": "    - camera_maxval: The maximum value of the camera action.\n    - quantization_scheme: The quantization scheme to use. Currently, two quantization schemes are supported:\n    - Linear quantization (default): Camera actions are split uniformly into discrete bins\n    - Mu-law quantization: Transforms the camera action using mu-law encoding (https://en.wikipedia.org/wiki/%CE%9C-law_algorithm)\n    followed by the same quantization scheme used by the linear scheme.\n    - mu: Mu is the parameter that defines the curvature of the mu-law encoding. Higher values of\n    mu will result in a sharper transition near zero. Below are some reference values listed\n    for choosing mu given a constant maxval and a desired max_precision value.\n    maxval = 10 | max_precision = 0.5  |   2.93826\n    maxval = 10 | max_precision = 0.4  |   4.80939\n    maxval = 10 | max_precision = 0.25 |   11.4887\n    maxval = 20 | max_precision = 0.5  |   2.7\n    maxval = 20 | max_precision = 0.4  |   4.39768\n    maxval = 20 | max_precision = 0.25 |   10.3194",
        "type": "code",
        "location": "/lib/actions.py:55-68"
    },
    "195": {
        "file_id": 10,
        "content": "This code defines two quantization schemes for camera actions: linear and mu-law. It also provides reference values for the mu parameter based on maxval and desired max_precision for mu-law encoding.",
        "type": "comment"
    },
    "196": {
        "file_id": 10,
        "content": "    maxval = 40 | max_precision = 0.5  |   2.60780\n    maxval = 40 | max_precision = 0.4  |   4.21554\n    maxval = 40 | max_precision = 0.25 |   9.81152\n    \"\"\"\n    camera_maxval: int\n    camera_binsize: int\n    quantization_scheme: str = attr.ib(\n        default=QuantizationScheme.LINEAR,\n        validator=attr.validators.in_([QuantizationScheme.LINEAR, QuantizationScheme.MU_LAW]),\n    )\n    mu: float = attr.ib(default=5)\n    def discretize(self, xy):\n        xy = np.clip(xy, -self.camera_maxval, self.camera_maxval)\n        if self.quantization_scheme == QuantizationScheme.MU_LAW:\n            xy = xy / self.camera_maxval\n            v_encode = np.sign(xy) * (np.log(1.0 + self.mu * np.abs(xy)) / np.log(1.0 + self.mu))\n            v_encode *= self.camera_maxval\n            xy = v_encode\n        # Quantize using linear scheme\n        return np.round((xy + self.camera_maxval) / self.camera_binsize).astype(np.int64)\n    def undiscretize(self, xy):\n        xy = xy * self.camera_binsize - self.camera_maxval",
        "type": "code",
        "location": "/lib/actions.py:69-95"
    },
    "197": {
        "file_id": 10,
        "content": "This code defines a class with properties for camera max value, bin size, quantization scheme (linear or Mu-Law), and mu value. The discretize method takes in xy coordinates, clips them within the camera range, applies the specified quantization scheme to discretize the values, and returns the rounded values as integers. The undiscretize method takes in discretized values and converts them back to their original continuous representation by multiplying with the bin size and subtracting the camera max value.",
        "type": "comment"
    },
    "198": {
        "file_id": 10,
        "content": "        if self.quantization_scheme == QuantizationScheme.MU_LAW:\n            xy = xy / self.camera_maxval\n            v_decode = np.sign(xy) * (1.0 / self.mu) * ((1.0 + self.mu) ** np.abs(xy) - 1.0)\n            v_decode *= self.camera_maxval\n            xy = v_decode\n        return xy\nclass ActionTransformer:\n    \"\"\"Transforms actions between internal array and minerl env format.\"\"\"\n    @store_args\n    def __init__(\n        self,\n        camera_maxval=10,\n        camera_binsize=2,\n        camera_quantization_scheme=\"linear\",\n        camera_mu=5,\n    ):\n        self.quantizer = CameraQuantizer(\n            camera_maxval=camera_maxval,\n            camera_binsize=camera_binsize,\n            quantization_scheme=camera_quantization_scheme,\n            mu=camera_mu,\n        )\n    def camera_zero_bin(self):\n        return self.camera_maxval // self.camera_binsize\n    def discretize_camera(self, xy):\n        return self.quantizer.discretize(xy)\n    def undiscretize_camera(self, pq):\n        return self.quantizer.undiscretize(pq)",
        "type": "code",
        "location": "/lib/actions.py:97-130"
    },
    "199": {
        "file_id": 10,
        "content": "This code defines a class called `ActionTransformer` that transforms actions between internal arrays and the MinerL environment format. It includes methods for discretizing and undiscretizing camera data, as well as calculating a zero bin value based on camera binsize. If the quantization scheme is set to \"mu_law\", it applies the mu-law quantization method to the input data.",
        "type": "comment"
    }
}