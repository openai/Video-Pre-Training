{
    "0": {
        "file_id": 0,
        "content": "/README.md",
        "type": "filepath"
    },
    "1": {
        "file_id": 0,
        "content": "The comments discuss Minecraft AI model training, reinforcement learning features, and limited resource data collection for the MineRL BASALT 2022 competition, as well as building a house in under 10 minutes without harming villages.",
        "type": "summary"
    },
    "2": {
        "file_id": 0,
        "content": "# Video-Pre-Training\nVideo PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\n> :page_facing_up: [Read Paper](https://cdn.openai.com/vpt/Paper.pdf) \\\n  :mega: [Blog Post](https://openai.com/blog/vpt) \\\n  :space_invader: [MineRL Environment](https://github.com/minerllabs/minerl) (note version 1.0+ required) \\\n  :checkered_flag: [MineRL BASALT Competition](https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition)\n# Running agent models\nInstall pre-requirements for [MineRL](https://minerl.readthedocs.io/en/latest/tutorials/index.html).\nThen install requirements with:\n```\npip install git+https://github.com/minerllabs/minerl\npip install -r requirements.txt\n```\nTo run the code, call\n```\npython run_agent.py --model [path to .model file] --weights [path to .weight file]\n```\nAfter loading up, you should see a window of the agent playing Minecraft.\n# Agent Model Zoo\nBelow are the model files and weights files for various pre-trained Minecraft models.\nThe 1x, 2x and 3x model files correspond to their respective model weights width.",
        "type": "code",
        "location": "/README.md:3-35"
    },
    "3": {
        "file_id": 0,
        "content": "This code provides instructions for setting up and running the Video PreTraining (VPT) model, which learns to act by watching unlabeled online videos. It also includes a link to the paper describing the methodology and provides information on where to find more resources related to VPT. The code includes commands to install pre-requisites, requirements, and run the agent models. Additionally, it lists various pre-trained Minecraft models with their respective model files and weights.",
        "type": "comment"
    },
    "4": {
        "file_id": 0,
        "content": "* [:arrow_down: 1x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.model)\n* [:arrow_down: 2x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/2x.model)\n* [:arrow_down: 3x Model](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.model)\n### Demonstration Only - Behavioral Cloning\nThese models are trained on video demonstrations of humans playing Minecraft\nusing behavioral cloning (BC) and are more general than later models which\nuse reinforcement learning (RL) to further optimize the policy.\nFoundational models are trained across all videos in a single training run\nwhile house and early game models refine their respective size foundational\nmodel further using either the housebuilding contractor data or early game video\nsub-set. See the paper linked above for more details.\n#### Foundational Model :chart_with_upwards_trend:\n  * [:arrow_down: 1x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-1x.weights)",
        "type": "code",
        "location": "/README.md:37-51"
    },
    "5": {
        "file_id": 0,
        "content": "This code provides links to download pre-trained models for Minecraft reinforcement learning, trained using behavioral cloning on video demonstrations. Foundational models are trained across all videos in a single training run, while house and early game models refine further with specific data sets.",
        "type": "comment"
    },
    "6": {
        "file_id": 0,
        "content": "  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-2x.weights)\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/foundation-model-3x.weights)\n#### Fine-Tuned from House :chart_with_upwards_trend:\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-house-3x.weights)\n#### Fine-Tuned from Early Game :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-2x.weights)\n  * [:arrow_down: 3x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/bc-early-game-3x.weights)\n### Models With Environment Interactions\nThese models further refine the above demonstration based models with a reward\nfunction targeted at obtaining diamond pickaxes. While less general then the behavioral\ncloning models, these models have the benefit of interacting with the environment",
        "type": "code",
        "location": "/README.md:52-65"
    },
    "7": {
        "file_id": 0,
        "content": "This code provides links to pre-trained models for Minecraft gameplay and mentions fine-tuned models from specific starting points. Additionally, it introduces models with environment interactions that are trained using a reward function aimed at obtaining diamond pickaxes.",
        "type": "comment"
    },
    "8": {
        "file_id": 0,
        "content": "using a reward function and excel at progressing through the tech tree quickly.\nSee the paper for more information\non how they were trained and the exact reward schedule.\n#### RL from Foundation :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-foundation-2x.weights)\n#### RL from House :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-house-2x.weights)\n#### RL from Early Game :chart_with_upwards_trend:\n  * [:arrow_down: 2x Width Weights](https://openaipublic.blob.core.windows.net/minecraft-rl/models/rl-from-early-game-2x.weights)\n# Running Inverse Dynamics Model (IDM)\nIDM aims to predict what actions player is taking in a video recording.\nSetup:\n* Install requirements: `pip install -r requirements.txt`\n* Download the IDM model [.model :arrow_down:](https://openaipublic.blob.core.windows.net/minecraft-rl/idm/4x_idm.model) and [.weight :arrow_down:](https://openaipublic.blob.core.windows.net/minecraft-rl/idm/4x_idm.weights) files",
        "type": "code",
        "location": "/README.md:66-85"
    },
    "9": {
        "file_id": 0,
        "content": "This code provides pre-trained models for Minecraft gameplay using reinforcement learning and an Inverse Dynamics Model (IDM). The models are trained with different reward functions and excel at progressing quickly through the tech tree. Users can download the models and weights from specified URLs to run the IDM, which predicts player actions in a video recording.",
        "type": "comment"
    },
    "10": {
        "file_id": 0,
        "content": "* For demonstration purposes, you can use the contractor recordings shared below to. For this demo we use\n  [this .mp4](https://openaipublic.blob.core.windows.net/minecraft-rl/data/10.0/cheeky-cornflower-setter-02e496ce4abb-20220421-092639.mp4)\n  and [this associated actions file (.jsonl)](https://openaipublic.blob.core.windows.net/minecraft-rl/data/10.0/cheeky-cornflower-setter-02e496ce4abb-20220421-092639.jsonl).\nTo run the model with above files placed in the root directory of this code:\n```\npython run_inverse_dynamics_model.py -weights 4x_idm.weights --model 4x_idm.model --video-path cheeky-cornflower-setter-02e496ce4abb-20220421-092639.mp4 --jsonl-path cheeky-cornflower-setter-02e496ce4abb-20220421-092639.jsonl\n```\nA window should pop up which shows the video frame-by-frame, showing the predicted and true (recorded) actions side-by-side on the left.\nNote that `run_inverse_dynamics_model.py` is designed to be a demo of the IDM, not code to put it into practice.\n# Using behavioural cloning to fine-tune the models",
        "type": "code",
        "location": "/README.md:86-99"
    },
    "11": {
        "file_id": 0,
        "content": "This code provides a demonstration of the Inverse Dynamics Model (IDM) using provided contractor recordings for video and actions. The model is run with specific file paths, and a window displays the predicted and true actions side-by-side. Note that this script is designed for demonstration purposes only, not for practical implementation.",
        "type": "comment"
    },
    "12": {
        "file_id": 0,
        "content": "**Disclaimer:** This code is a rough demonstration only and not an exact recreation of what original VPT paper did (but it contains some preprocessing steps you want to be aware of)! As such, do not expect replicate the original experiments with this code. This code has been designed to be run-able on consumer hardware (e.g., 8GB of VRAM).\nSetup:\n* Install requirements: `pip install -r requirements.txt`\n* Download `.weights` and `.model` file for model you want to fine-tune.\n* Download contractor data (below) and place the `.mp4` and `.jsonl` files to the same directory (e.g., `data`). With default settings, you need at least 12 recordings.\nIf you downloaded the \"1x Width\" models and placed some data under `data` directory, you can perform finetuning with\n```\npython behavioural_cloning.py --data-dir data --in-model foundation-model-1x.model --in-weights foundation-model-1x.weights --out-weights finetuned-1x.weights\n```\nYou can then use `finetuned-1x.weights` when running the agent. You can change the training settings at the top of `behavioural_cloning.py`.",
        "type": "code",
        "location": "/README.md:101-114"
    },
    "13": {
        "file_id": 0,
        "content": "This code is a demonstration of video pre-training using behavioral cloning. It requires installing dependencies, downloading model and data files, and then fine-tuning the model with custom weights for better performance.",
        "type": "comment"
    },
    "14": {
        "file_id": 0,
        "content": "Major limitations:\n- Only trains single step at the time, i.e., errors are not propagated through timesteps.\n- Computes gradients one sample at a time to keep memory use low, but also slows down the code.\n# Contractor Demonstrations\n### Versions\nOver the course of the project we requested various demonstrations from contractors\nwhich we release as index files below. In general, major recorder versions change for a new\nprompt or recording feature while bug-fixes were represented as minor version changes.\nHowever, some\nrecorder versions we asked contractors to change their username when recording particular\nmodalities. Also, as contractors internally ask questions, clarification from one contractor may\nresult in a behavioral change in the other contractor. It is intractable to share every contractor's\nview for each version, but we've shared the prompts and major clarifications for each recorder\nversion where the task changed significantly.\n  <details>\n  <summary>Initial Prompt</summary>\n  We are collect",
        "type": "code",
        "location": "/README.md:116-136"
    },
    "15": {
        "file_id": 0,
        "content": "This code is a README.md file that lists the major limitations of the pre-training, describes a contractor demonstrations section, and provides details about the versions used for this project. It also mentions how different versions were used to change modalities or prompt changes due to contractor's internal questions and clarifications.",
        "type": "comment"
    },
    "16": {
        "file_id": 0,
        "content": "ing data for training AI models in Minecraft. You'll need to install java, download the modified version of minecraft (that collects and uploads your play data), and play minecraft survival mode! Paid per hour of gameplay. Prior experience in minecraft not. necessary. We do not collect any data that is unrelated to minecraft from your computer.\n  </details>\nThe following is a list of the available versions:\n* **6.x** Core recorder features subject to change [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_6xx_Jun_29.json)\n  * 6.9 First feature complete recorder version\n  * 6.10 Fixes mouse scaling on Mac when gui is open\n  * 6.11 Tracks the hotbar slot\n  * 6.13 Sprinting, swap-hands, ... (see commits below)\n    <details>\n    <summary>Commits</summary>\n    * improve replays that are cut in the middle of gui; working on riding boats / replays cut in the middle of a run\n    * improve replays by adding dwheel action etc, also, loosen up replay tolerances\n    * opencv version bump",
        "type": "code",
        "location": "/README.md:136-152"
    },
    "17": {
        "file_id": 0,
        "content": "This code is a list of available versions for the Minecraft AI model training program. It includes a description of the features in each version and provides links to download the modified Minecraft version for training.",
        "type": "comment"
    },
    "18": {
        "file_id": 0,
        "content": "    * add swap hands, and recording of the step timestamp\n    * implement replaying from running and sprinting and tests\n    * do not record sprinting (can use stats for that)\n    * check for mouse button number, ignore >2\n    * handle the errors when mouse / keyboard are recorded as null\n    </details>\n* **7.x** Prompt changes [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_7xx_Apr_6.json)\n  * 7.6 Bump version for internal tracking\n    <details>\n    <summary>Additional ask to contractors</summary>\n    Right now, early game data is especially valuable to us. As such, we request that at least half of the data you upload is from the first 30 minutes of the game. This means that, for every hour of gameplay you spend in an older world, we ask you to play two sessions in which you create a new world and play for 30 minutes. You can play for longer in these worlds, but only the first 30 minutes counts as early game data.\n    </details>\n* **8.x** :clipboard",
        "type": "code",
        "location": "/README.md:153-168"
    },
    "19": {
        "file_id": 0,
        "content": "This code seems to be part of a README file for a Minecraft-based reinforcement learning project. The comments suggest that the developers are working on various features such as swapping hands and implementing replaying from running and sprinting. They also mention not recording sprinting data, checking for mouse button numbers, and handling errors related to recording mouse or keyboard input as null. Additionally, there is a note about prompting contractors to collect early game data (first 30 minutes) in newer worlds to help improve the AI's performance in those stages of the game.",
        "type": "comment"
    },
    "20": {
        "file_id": 0,
        "content": ": House Building from Scratch Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_8xx_Jun_29.json)\n  <details>\n  <summary>Changes and Prompt</summary>\n  Hi all! Thank you for your hard work so far.\n  This week we would like to have you all collect data on a specific task.\n  This comes with a new recorder version 8.0 which you will need to update your recording script to download.\n  This week we would like you to use a new world each time you play, so loading existing worlds is disabled.\n  The new task is as follows:\n  Starting in a new world, build a simple house in 10-15 minutes. This corresponds to one day and a bit of the night. Please use primarily wood, dirt, and sand, as well as crafted wood items such as doors, fences, ect. in constructing your house. Avoid using difficult items such as stone. Aside from those constraints, you may decorate the structure you build as you wish. It does not need to have any specific furniture. For example, it is ",
        "type": "code",
        "location": "/README.md:168-182"
    },
    "21": {
        "file_id": 0,
        "content": "This code is providing instructions for a new task, \"Building a Simple House\". The task involves building a house using primarily wood, dirt, and sand, along with crafted wood items. The structure can be decorated as desired but should not use difficult materials such as stone. It also provides information about the need to update the recording script and use a new world each time.",
        "type": "comment"
    },
    "22": {
        "file_id": 0,
        "content": "OK if there is no bed in your house. If you have not finished the house by the sunrise (20 minutes) please exit and continue to another demonstration. Please continue to narrate what you are doing while completing this task.\n  Since you will be unable to resume building after exiting Minecraft or going back to the main menu, you must finish these demonstrations in one session. Pausing via the menu is still supported. If you want to view your creations later, they will be saved locally so you can look at them in your own time. We may use these save files in a future task so if you have space, please leave the save files titled “build-house-15-min-“.\n  For this week try to avoid all cobblestone / stone / granite\n  For this week we just want simple houses without sleeping. If 10 minutes is too short, let us know and we can think of how to adjust!\n  Stone tools are ok but I think you may run-out of time\n  Changes:\n    * Timer ends episode after 10 realtime minutes\n    * Worlds are named: `\"build-house-15-min-\" + Math.abs(random.nextInt());`",
        "type": "code",
        "location": "/README.md:182-194"
    },
    "23": {
        "file_id": 0,
        "content": "This code is for a Minecraft demonstration task where the player must build a house within 20 minutes. If not finished, they should exit and continue with another demo. Narration is required throughout. The demonstrations need to be completed in one session without resuming after exiting or going back to the main menu. Save files will be saved locally for viewing later but may be used for future tasks if space permits. The task requires a simple house without sleep areas and avoids cobblestone, stone, and granite. Stone tools are acceptable but time-limited. The episode ends after 10 realtime minutes. World names follow the format \"build-house-15-min-\" + random number.",
        "type": "comment"
    },
    "24": {
        "file_id": 0,
        "content": "  </details>\n  * Note this version introduces 10-minute timer that ends the episode. It\n  cut experiments short occasionally and was fixed in 9.1\n  * 8.0 Simple House\n  * 8.2 Update upload script\n* **9.x** :clipboard: House Building from Random Starting Materials Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_9xx_Jun_29.json)\n    <details>\n    <summary>Changes and Prompt</summary>\n    You now will have 10 minutes to use the provided resources to build your house / home / or structure. In this version, the experiment will time out after 10 minutes if you are not complete so don't be alarmed if that happens, it is intentional.\n    No need to use up all the resources! It's ok to collect a few things but spend the majority of the time placing blocks (the act of placing seems to be harder to learn)\n    Changes:\n    * Worlds are named: `\"design-house-10-min-\" + Math.abs(random.nextInt());`\n    * Starting inventory given by code below\n    </details>\n    <details>",
        "type": "code",
        "location": "/README.md:196-215"
    },
    "25": {
        "file_id": 0,
        "content": "Introduces 10-minute timer for task completion.",
        "type": "comment"
    },
    "26": {
        "file_id": 0,
        "content": "    <summary>Random Starting Inventory Code</summary>\n  ```java\n        Random random = new Random();\n        List<ItemStack> hotbar = new ArrayList<>();\n        List<ItemStack> inventory = new ArrayList<>();\n        // Ensure we give the player the basic tools in their hot bar\n        hotbar.add(new ItemStack(Items.STONE_AXE));\n        hotbar.add(new ItemStack(Items.STONE_PICKAXE));\n        hotbar.add(new ItemStack(Items.STONE_SHOVEL));\n        hotbar.add(new ItemStack(Items.CRAFTING_TABLE));\n        // Add some random items to the player hotbar as well\n        addToList(hotbar, inventory, Items.TORCH, random.nextInt(16) * 2 + 2);\n        // Next add main building blocks\n        if (random.nextFloat() < 0.7) {\n           addToList(hotbar, inventory, Items.OAK_FENCE_GATE, random.nextInt(5));\n           addToList(hotbar, inventory, Items.OAK_FENCE, random.nextInt(5) * 64);\n           addToList(hotbar, inventory, Items.OAK_DOOR, random.nextInt(5));\n           addToList(hotbar, inventory, Items.OAK_TRAPDOOR, random.nextInt(2) * 2);",
        "type": "code",
        "location": "/README.md:216-237"
    },
    "27": {
        "file_id": 0,
        "content": "This code generates a random starting inventory for the player by adding basic tools, some random items, and building blocks to their hotbar and inventory.",
        "type": "comment"
    },
    "28": {
        "file_id": 0,
        "content": "           addToList(hotbar, inventory, Items.OAK_PLANKS, random.nextInt(3) * 64 + 128);\n           addToList(hotbar, inventory, Items.OAK_SLAB, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.OAK_STAIRS, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.OAK_LOG, random.nextInt(2) * 32);\n           addToList(hotbar, inventory, Items.OAK_PRESSURE_PLATE, random.nextInt(5));\n        } else {\n           addToList(hotbar, inventory, Items.BIRCH_FENCE_GATE, random.nextInt(5));\n           addToList(hotbar, inventory, Items.BIRCH_FENCE, random.nextInt(5) * 64);\n           addToList(hotbar, inventory, Items.BIRCH_DOOR, random.nextInt(5));\n           addToList(hotbar, inventory, Items.BIRCH_TRAPDOOR, random.nextInt(2) * 2);\n           addToList(hotbar, inventory, Items.BIRCH_PLANKS, random.nextInt(3) * 64 + 128);\n           addToList(hotbar, inventory, Items.BIRCH_SLAB, random.nextInt(3) * 64);\n           addToList(hotbar, inventory, Items.BIRCH_STAIRS, random.nextInt(3) * 64);",
        "type": "code",
        "location": "/README.md:238-250"
    },
    "29": {
        "file_id": 0,
        "content": "This code randomly selects items to add to the hotbar based on the type of biome the player is in. It uses different lists of items for oak and birch biomes.",
        "type": "comment"
    },
    "30": {
        "file_id": 0,
        "content": "           addToList(hotbar, inventory, Items.BIRCH_LOG, random.nextInt(2) * 32);\n           addToList(hotbar, inventory, Items.BIRCH_PRESSURE_PLATE, random.nextInt(5));\n        }\n        // Now add some random decoration items to the player inventory\n        addToList(hotbar, inventory, Items.CHEST, random.nextInt(3));\n        addToList(hotbar, inventory, Items.FURNACE, random.nextInt(2) + 1);\n        addToList(hotbar, inventory, Items.GLASS_PANE,  random.nextInt(5) * 4);\n        addToList(hotbar, inventory, Items.WHITE_BED, (int) (random.nextFloat() + 0.2)); // Bed 20% of the time\n        addToList(hotbar, inventory, Items.PAINTING, (int) (random.nextFloat() + 0.1)); // Painting 10% of the time\n        addToList(hotbar, inventory, Items.FLOWER_POT, (int) (random.nextFloat() + 0.1) * 4); // 4 Flower pots 10% of the time\n        addToList(hotbar, inventory, Items.OXEYE_DAISY, (int) (random.nextFloat() + 0.1) * 4); // 4 Oxeye daisies 10% of the time\n        addToList(hotbar, inventory, Items.POPPY, (int) (random.nextFloat() + 0.1) * 4); // 4 Poppies 10% of the time",
        "type": "code",
        "location": "/README.md:251-263"
    },
    "31": {
        "file_id": 0,
        "content": "This code is adding a variety of items to the player's inventory. It uses random number generation to decide how many of each item to add, with some items having a higher chance of appearing than others (e.g., beds have a 20% chance). This helps create a diverse and unpredictable inventory for the player to work with.",
        "type": "comment"
    },
    "32": {
        "file_id": 0,
        "content": "        addToList(hotbar, inventory, Items.SUNFLOWER, (int) (random.nextFloat() + 0.1) * 4); // 4 Sunflowers 10% of the time\n        // Shuffle the hotbar slots and inventory slots\n        Collections.shuffle(hotbar);\n        Collections.shuffle(inventory);\n        // Give the player the items\n        this.mc.getIntegratedServer().getPlayerList().getPlayers().forEach(p -> {\n           if (p.getUniqueID().equals(this.getUniqueID())) {\n               hotbar.forEach(p.inventory::addItemStackToInventory);\n               inventory.forEach(p.inventory::addItemStackToInventory);\n           }\n        });\n  ```\n    </details>\n     * 9.0 First version\n     * 9.1 Fixed timer bug\n* **10.0** :clipboard: Obtain Diamond Pickaxe Task [:arrow_down: index](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/all_10xx_Jun_29.json)\n  <details>\n  <summary>Changes and Prompt</summary>\n  Prompt:\n  For this new task we have given you 20 minutes to craft a diamond pickaxe. We ask that you do not try to search for vi",
        "type": "code",
        "location": "/README.md:264-288"
    },
    "33": {
        "file_id": 0,
        "content": "This code adds 4 sunflowers to the player's inventory 10% of the time, shuffles both the hotbar and inventory slots, and then gives the player the items if they are the same as the current player.",
        "type": "comment"
    },
    "34": {
        "file_id": 0,
        "content": "llages or other ways of getting diamonds, but if you are spawned in view of one, or happen to fall into a cave structure feel free to explore it for diamonds.\n  If 20 min is not enough that is OK. It will happen on some seeds because of bad luck. Please do not use glitches to find the diamonds.\n  Changes:\n  * change to 20 minute time limit\n  * _don't count gui time as part of the time limit_\n  * World are named `\"collect-diamond-pickaxe-15min-\" + Math.abs(random.nextInt());`\n  </details>\nSometimes we asked the contractors to signify other tasks besides changing the version. This\nprimarily occurred in versions 6 and 7 as 8, 9 and 10 are all task specific.\n<details>\n<summary>Prompt to contractors (click to show)</summary>\nAnother request about additional time - please use some of it to chop trees. Specifically, please start the recorder by adding --username treechop argument to the script (i.e. use play --username treechop on windows, ./play.sh --username treechop on osx/linux), and spend some time",
        "type": "code",
        "location": "/README.md:288-304"
    },
    "35": {
        "file_id": 0,
        "content": "Code discusses time limits for finding diamonds in Minecraft, changing version numbers, and requesting contractors to chop trees while recording gameplay with a specific username.",
        "type": "comment"
    },
    "36": {
        "file_id": 0,
        "content": " chopping trees! Getting wooden or stone tools is ok, but please spend the majority of the with username treechop specifically chopping. I did it myself for about 15 minutes, and it does get boring pretty quickly, so I don't expect you to do it all the time, but please do at least a little bit of chopping. Feel free to play normally the rest of the time (but please restart without --username treechop argument when you are not chopping)\nHowever, it is preferable that you start a new world though, and use only the tools that are easily obtainable in that world. I'll see what I can do about getting player an iron axe - that sounds reasonable, and should not be hard, but will require a code update.\n</details>\n### Environment\nWe restrict the contractors to playing Minecraft in windowed mode at 720p which we downsample at 20hz to 360p\nto minimize space. We also disabled the options screen to prevent the contractor from\nchanging things such as brightness, or rendering options. We ask contractors not to press keys",
        "type": "code",
        "location": "/README.md:304-311"
    },
    "37": {
        "file_id": 0,
        "content": "The code is providing instructions for the video game Minecraft, asking testers to spend a portion of their time chopping trees and using easily obtainable tools in a new world. It also mentions restrictions on the contractor's environment, such as windowed mode, downsampling, and disabling options like brightness and rendering settings.",
        "type": "comment"
    },
    "38": {
        "file_id": 0,
        "content": "such as f3 which shows a debug overlay, however some contractors may still do this.\n### Data format\nDemonstrations are broken up into up to 5 minute segments consisting of a series of\ncompressed screen observations, actions, environment statistics, and a checkpoint\nsave file from the start of the segment. Each relative path in the index will\nhave all the files for that given segment, however if a file was dropped while\nuploading, the corresponding relative path is not included in the index therefore\nthere may be missing chunks from otherwise continuous demonstrations.\nIndex files are provided for each version as a json file:\n```json\n{\n  \"basedir\": \"https://openaipublic.blob.core.windows.net/data/\",\n  \"relpaths\": [\n    \"8.0/cheeky-cornflower-setter-74ae6c2eae2e-20220315-122354\",\n    ...\n  ]\n}\n```\nRelative paths follow the following format:\n* `<recorder-version>/<contractor-alias>-<session-id>-<date>-<time>`\n> Note that due to network errors, some segments may be missing from otherwise\ncontinuous demonstrations.",
        "type": "code",
        "location": "/README.md:312-338"
    },
    "39": {
        "file_id": 0,
        "content": "The code provides information about the format of demonstrations, which are broken into 5-minute segments containing compressed screen observations, actions, environment statistics, and checkpoint save files. It also mentions that there may be missing chunks from continuous demonstrations due to network errors. The index files for each version are provided as JSON files with a list of relative paths following a specific format.",
        "type": "comment"
    },
    "40": {
        "file_id": 0,
        "content": "Your data loader can then find following files:\n* Video observation: `<basedir>/<relpath>.mp4`\n* Action file: `<basedir>/<relpath>.jsonl`\n* Options file: `<basedir>/<relpath>-options.json`\n* Checkpoint save file: `<basedir>/<relpath>.zip`\nThe action file is **not**  a valid json object: each line in\naction file is an individual action dictionary.\nFor v7.x, the actions are in form\n```json\n{\n  \"mouse\": {\n    \"x\": 274.0,\n    \"y\": 338.0,\n    \"dx\": 0.0,\n    \"dy\": 0.0,\n    \"scaledX\": -366.0,\n    \"scaledY\": -22.0,\n    \"dwheel\": 0.0,\n    \"buttons\": [],\n    \"newButtons\": []\n  },\n  \"keyboard\": {\n    \"keys\": [\n      \"key.keyboard.a\",\n      \"key.keyboard.s\"\n    ],\n    \"newKeys\": [],\n    \"chars\": \"\"\n  },\n  \"isGuiOpen\": false,\n  \"isGuiInventory\": false,\n  \"hotbar\": 4,\n  \"yaw\": -112.35006,\n  \"pitch\": 8.099996,\n  \"xpos\": 841.364694513396,\n  \"ypos\": 63.0,\n  \"zpos\": 24.956354839537802,\n  \"tick\": 0,\n  \"milli\": 1649575088006,\n  \"inventory\": [\n    {\n      \"type\": \"oak_door\",\n      \"quantity\": 3\n    },\n    {\n      \"type\": \"oak_planks\",\n      \"quantity\": 59",
        "type": "code",
        "location": "/README.md:340-388"
    },
    "41": {
        "file_id": 0,
        "content": "The code defines the structure of files required by the data loader, including video observation file, action file, options file, and checkpoint save file. The action file contains a list of individual action dictionaries in JSON format, each representing mouse and keyboard inputs, game status, player position, and inventory for a specific tick or frame.",
        "type": "comment"
    },
    "42": {
        "file_id": 0,
        "content": "    },\n    {\n      \"type\": \"stone_pickaxe\",\n      \"quantity\": 1\n    },\n    {\n      \"type\": \"oak_planks\",\n      \"quantity\": 64\n    }\n  ],\n  \"serverTick\": 6001,\n  \"serverTickDurationMs\": 36.3466,\n  \"stats\": {\n    \"minecraft.custom:minecraft.jump\": 4,\n    \"minecraft.custom:minecraft.time_since_rest\": 5999,\n    \"minecraft.custom:minecraft.play_one_minute\": 5999,\n    \"minecraft.custom:minecraft.time_since_death\": 5999,\n    \"minecraft.custom:minecraft.walk_one_cm\": 7554,\n    \"minecraft.use_item:minecraft.oak_planks\": 5,\n    \"minecraft.custom:minecraft.fall_one_cm\": 269,\n    \"minecraft.use_item:minecraft.glass_pane\": 3\n  }\n}\n```\n# BASALT 2022 dataset\nWe also collected a dataset of demonstrations for the [MineRL BASALT 2022](https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition) competition, with around 150GB of data per task.\n**Note**: To avoid confusion with the competition rules, the action files (.jsonl) have been stripped of information that is not allowed in the competition. We will upload unmodified dataset after the competition ends.",
        "type": "code",
        "location": "/README.md:389-418"
    },
    "43": {
        "file_id": 0,
        "content": "This code represents a JSON object containing game state information and player actions for Minecraft. It includes a list of items in the player's inventory, server tick data, and various statistics tracking the player's actions. The dataset is collected for the MineRL BASALT 2022 competition with around 150GB of data per task.",
        "type": "comment"
    },
    "44": {
        "file_id": 0,
        "content": "* **FindCave** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/find-cave-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n    ```\n    Look around for a cave. When you are inside one, quit the game by opening main menu and pressing \"Save and Quit To Title\".\n    You are not allowed to dig down from the surface to find a cave.\n    Timelimit: 3 minutes.\n    Example recordings: https://www.youtube.com/watch?v=TclP_ozH-eg\n    ```\n    </details>\n* **MakeWaterfall** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/waterfall-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n    ```\n    After spawning in a mountainous area with a water bucket and various tools, build a beautiful waterfall and then reposition yourself to “take a scenic picture” of the same waterfall, and then quit the game by opening the menu and selecting \"Save and Quit to Title\"\n    Timelimit: 5 minutes.",
        "type": "code",
        "location": "/README.md:420-439"
    },
    "45": {
        "file_id": 0,
        "content": "FindCave: Look for a cave and quit the game when inside one. No digging from surface. Timelimit: 3 minutes. Example recordings: https://www.youtube.com/watch?v=TclP_ozH-eg\n\nMakeWaterfall: Spawn in mountainous area, build waterfall, take a scenic picture, and quit the game. Timelimit: 5 minutes.",
        "type": "comment"
    },
    "46": {
        "file_id": 0,
        "content": "    Example recordings: https://youtu.be/NONcbS85NLA\n    ```\n    </details>\n* **MakeVillageAnimalPen** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/pen-animals-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n    ```\n    After spawning in a village, build an animal pen next to one of the houses in a village. Use your fence posts to build one animal pen that contains at least two of the same animal. (You are only allowed to pen chickens, cows, pigs, sheep or rabbits.) There should be at least one gate that allows players to enter and exit easily. The animal pen should not contain more than one type of animal. (You may kill any extra types of animals that accidentally got into the pen.) Don’t harm the village.\n    After you are done, quit the game by opening the menu and pressing \"Save and Quit to Title\".\n    You may need to terraform the area around a house to build a pen. When we say not to harm the village, e",
        "type": "code",
        "location": "/README.md:440-451"
    },
    "47": {
        "file_id": 0,
        "content": "This code provides a prompt for contractors to build an animal pen next to a village house using specific animals and materials, without harming the village.",
        "type": "comment"
    },
    "48": {
        "file_id": 0,
        "content": "xamples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers. Animal pens must have a single type of animal: pigs, cows, sheep, chicken or rabbits.\n    The food items can be used to lure in the animals: if you hold seeds in your hand, this attracts nearby chickens to you, for example.\n    Timelimit: 5 minutes.\n    Example recordings: https://youtu.be/SLO7sep7BO8\n    ```\n    </details>\n* **BuildVillageHouse** [:arrow_down: index file](https://openaipublic.blob.core.windows.net/minecraft-rl/snapshots/build-house-Jul-28.json)\n  * <details>\n    <summary>Prompt to contractors (click to show)</summary>\n    ```\n    Taking advantage of the items in your inventory, build a new house in the style of the village (random biome), in an appropriate location (e.g. next to the path through the village), without harming the village in the process.\n    Then give a brief tour of the house (i.e. spin around slowly such that all of the walls and the roof are visible).\n ",
        "type": "code",
        "location": "/README.md:451-467"
    },
    "49": {
        "file_id": 0,
        "content": "The code describes tasks for AI to complete in the game of Minecraft. Tasks include not harming animals or damaging existing structures while building a new house, and giving a brief tour of the constructed house. There is also a time limit of 5 minutes and an example recording provided for reference.",
        "type": "comment"
    },
    "50": {
        "file_id": 0,
        "content": "   * You start with a stone pickaxe and a stone axe, and various building blocks. It’s okay to break items that you misplaced (e.g. use the stone pickaxe to break cobblestone blocks).\n    * You are allowed to craft new blocks.\n    Please spend less than ten minutes constructing your house.\n    You don’t need to copy another house in the village exactly (in fact, we’re more interested in having slight deviations, while keeping the same \"style\"). You may need to terraform the area to make space for a new house.\n    When we say not to harm the village, examples include taking animals from existing pens, damaging existing houses or farms, and attacking villagers.\n    After you are done, quit the game by opening the menu and pressing \"Save and Quit to Title\".\n    Timelimit: 12 minutes.\n    Example recordings: https://youtu.be/WeVqQN96V_g\n    ```\n    </details>\n# Contribution\nThis was a large effort by a dedicated team at OpenAI:\n[Bowen Baker](https://github.com/bowenbaker),\n[Ilge Akkaya](https://github.com/ilge),",
        "type": "code",
        "location": "/README.md:467-487"
    },
    "51": {
        "file_id": 0,
        "content": "Instructions for building a house in Minecraft within 10 minutes, without causing harm to the village.",
        "type": "comment"
    },
    "52": {
        "file_id": 0,
        "content": "[Peter Zhokhov](https://github.com/pzhokhov),\n[Joost Huizinga](https://github.com/JoostHuizinga),\n[Jie Tang](https://github.com/jietang),\n[Adrien Ecoffet](https://github.com/AdrienLE),\n[Brandon Houghton](https://github.com/brandonhoughton),\n[Raul Sampedro](https://github.com/samraul),\nJeff Clune\nThe code here represents a minimal version of our model code which was\nprepared by [Anssi Kanervisto](https://github.com/miffyli) and others so that these models could be used as\npart of the MineRL BASALT competition.",
        "type": "code",
        "location": "/README.md:488-497"
    },
    "53": {
        "file_id": 0,
        "content": "This code is acknowledging the authors of the model and crediting Anssi Kanervisto for preparing a minimal version of the code to be used in the MineRL BASALT competition.",
        "type": "comment"
    },
    "54": {
        "file_id": 1,
        "content": "/agent.py",
        "type": "filepath"
    },
    "55": {
        "file_id": 1,
        "content": "The code establishes Minecraft agent settings for a reinforcement learning project, including environment configuration and an agent class with action mapping, transforming, policy-making capabilities, and device support for actions.",
        "type": "summary"
    },
    "56": {
        "file_id": 1,
        "content": "import numpy as np\nimport torch as th\nimport cv2\nfrom gym3.types import DictType\nfrom gym import spaces\nfrom lib.action_mapping import CameraHierarchicalMapping\nfrom lib.actions import ActionTransformer\nfrom lib.policy import MinecraftAgentPolicy\nfrom lib.torch_util import default_device_type, set_default_torch_device\n# Hardcoded settings\nAGENT_RESOLUTION = (128, 128)\nPOLICY_KWARGS = dict(\n    attention_heads=16,\n    attention_mask_style=\"clipped_causal\",\n    attention_memory_size=256,\n    diff_mlp_embedding=False,\n    hidsize=2048,\n    img_shape=[128, 128, 3],\n    impala_chans=[16, 32, 32],\n    impala_kwargs={\"post_pool_groups\": 1},\n    impala_width=8,\n    init_norm_kwargs={\"batch_norm\": False, \"group_norm_groups\": 1},\n    n_recurrence_layers=4,\n    only_img_input=True,\n    pointwise_ratio=4,\n    pointwise_use_activation=False,\n    recurrence_is_residual=True,\n    recurrence_type=\"transformer\",\n    timesteps=128,\n    use_pointwise_layer=True,\n    use_pre_lstm_ln=False,\n)\nPI_HEAD_KWARGS = dict(temperature=2.0)\nACTION_TRANSFORMER_KWARGS = dict(",
        "type": "code",
        "location": "/agent.py:1-40"
    },
    "57": {
        "file_id": 1,
        "content": "The code imports necessary libraries and defines various settings for a Minecraft agent. It includes hardcoded resolution, policy arguments, action transformer arguments, and other configuration options. The code seems to be part of a larger project involving reinforcement learning and a specific environment (Minecraft in this case).",
        "type": "comment"
    },
    "58": {
        "file_id": 1,
        "content": "    camera_binsize=2,\n    camera_maxval=10,\n    camera_mu=10,\n    camera_quantization_scheme=\"mu_law\",\n)\nENV_KWARGS = dict(\n    fov_range=[70, 70],\n    frameskip=1,\n    gamma_range=[2, 2],\n    guiscale_range=[1, 1],\n    resolution=[640, 360],\n    cursor_size_range=[16.0, 16.0],\n)\nTARGET_ACTION_SPACE = {\n    \"ESC\": spaces.Discrete(2),\n    \"attack\": spaces.Discrete(2),\n    \"back\": spaces.Discrete(2),\n    \"camera\": spaces.Box(low=-180.0, high=180.0, shape=(2,)),\n    \"drop\": spaces.Discrete(2),\n    \"forward\": spaces.Discrete(2),\n    \"hotbar.1\": spaces.Discrete(2),\n    \"hotbar.2\": spaces.Discrete(2),\n    \"hotbar.3\": spaces.Discrete(2),\n    \"hotbar.4\": spaces.Discrete(2),\n    \"hotbar.5\": spaces.Discrete(2),\n    \"hotbar.6\": spaces.Discrete(2),\n    \"hotbar.7\": spaces.Discrete(2),\n    \"hotbar.8\": spaces.Discrete(2),\n    \"hotbar.9\": spaces.Discrete(2),\n    \"inventory\": spaces.Discrete(2),\n    \"jump\": spaces.Discrete(2),\n    \"left\": spaces.Discrete(2),\n    \"pickItem\": spaces.Discrete(2),\n    \"right\": spaces.Discrete(2),\n    \"sneak\": spaces.Discrete(2),",
        "type": "code",
        "location": "/agent.py:41-77"
    },
    "59": {
        "file_id": 1,
        "content": "This code sets various parameters for a game agent. It defines action space, environment keywords, and camera settings for the game environment. The action space includes actions like ESC, attack, back, camera adjustment, dropping items, moving, using hotbar slots, inventory management, jumping, moving left/right, and sneaking. Environment keywords specify fields such as field of view (fov_range), frameskip, gamma value range, gui scale range, game resolution, and cursor size range. Camera settings include bin size, maximum value, mean value, and quantization scheme for the camera input.",
        "type": "comment"
    },
    "60": {
        "file_id": 1,
        "content": "    \"sprint\": spaces.Discrete(2),\n    \"swapHands\": spaces.Discrete(2),\n    \"use\": spaces.Discrete(2)\n}\ndef validate_env(env):\n    \"\"\"Check that the MineRL environment is setup correctly, and raise if not\"\"\"\n    for key, value in ENV_KWARGS.items():\n        if key == \"frameskip\":\n            continue\n        if getattr(env.task, key) != value:\n            raise ValueError(f\"MineRL environment setting {key} does not match {value}\")\n    action_names = set(env.action_space.spaces.keys())\n    if action_names != set(TARGET_ACTION_SPACE.keys()):\n        raise ValueError(f\"MineRL action space does match. Expected actions {set(TARGET_ACTION_SPACE.keys())}\")\n    for ac_space_name, ac_space_space in TARGET_ACTION_SPACE.items():\n        if env.action_space.spaces[ac_space_name] != ac_space_space:\n            raise ValueError(f\"MineRL action space setting {ac_space_name} does not match {ac_space_space}\")\ndef resize_image(img, target_resolution):\n    # For your sanity, do not resize with any function than INTER_LINEAR",
        "type": "code",
        "location": "/agent.py:78-101"
    },
    "61": {
        "file_id": 1,
        "content": "This code defines a dictionary ENV_KWARGS that contains expected environment settings for the MineRL environment. It also includes a function validate_env() to check if the environment is set up correctly and raise an error if any setting does not match the expected value. Additionally, it checks if the action space of the MineRL environment matches the expected actions and their respective spaces. The code also provides a comment suggesting to resize images using INTER_LINEAR resizing function for better results.",
        "type": "comment"
    },
    "62": {
        "file_id": 1,
        "content": "    img = cv2.resize(img, target_resolution, interpolation=cv2.INTER_LINEAR)\n    return img\nclass MineRLAgent:\n    def __init__(self, env, device=None, policy_kwargs=None, pi_head_kwargs=None):\n        validate_env(env)\n        if device is None:\n            device = default_device_type()\n        self.device = th.device(device)\n        # Set the default torch device for underlying code as well\n        set_default_torch_device(self.device)\n        self.action_mapper = CameraHierarchicalMapping(n_camera_bins=11)\n        action_space = self.action_mapper.get_action_space_update()\n        action_space = DictType(**action_space)\n        self.action_transformer = ActionTransformer(**ACTION_TRANSFORMER_KWARGS)\n        if policy_kwargs is None:\n            policy_kwargs = POLICY_KWARGS\n        if pi_head_kwargs is None:\n            pi_head_kwargs = PI_HEAD_KWARGS\n        agent_kwargs = dict(policy_kwargs=policy_kwargs, pi_head_kwargs=pi_head_kwargs, action_space=action_space)\n        self.policy = MinecraftAgentPolicy(**agent_kwargs).to(device)",
        "type": "code",
        "location": "/agent.py:102-128"
    },
    "63": {
        "file_id": 1,
        "content": "This code defines a MineRLAgent class with an __init__ method. It resizes the image using cv2.resize and returns it. The class has attributes for action_mapper, action_transformer, policy, and device. The policy is created with given policy_kwargs, pi_head_kwargs, and action_space. The device is set as the default torch device.",
        "type": "comment"
    },
    "64": {
        "file_id": 1,
        "content": "        self.hidden_state = self.policy.initial_state(1)\n        self._dummy_first = th.from_numpy(np.array((False,))).to(device)\n    def load_weights(self, path):\n        \"\"\"Load model weights from a path, and reset hidden state\"\"\"\n        self.policy.load_state_dict(th.load(path, map_location=self.device), strict=False)\n        self.reset()\n    def reset(self):\n        \"\"\"Reset agent to initial state (i.e., reset hidden state)\"\"\"\n        self.hidden_state = self.policy.initial_state(1)\n    def _env_obs_to_agent(self, minerl_obs):\n        \"\"\"\n        Turn observation from MineRL environment into model's observation\n        Returns torch tensors.\n        \"\"\"\n        agent_input = resize_image(minerl_obs[\"pov\"], AGENT_RESOLUTION)[None]\n        agent_input = {\"img\": th.from_numpy(agent_input).to(self.device)}\n        return agent_input\n    def _agent_action_to_env(self, agent_action):\n        \"\"\"Turn output from policy into action for MineRL\"\"\"\n        # This is quite important step (for some reason).\n        # For the sake of your sanity, remember to do this step (manual conversion to numpy)",
        "type": "code",
        "location": "/agent.py:129-154"
    },
    "65": {
        "file_id": 1,
        "content": "The code defines a class for an agent with methods to reset its hidden state, convert MineRL observations into the model's observation format, and convert policy output into actions for the MineRL environment. It also includes a method to load model weights from a given path while resetting the hidden state.",
        "type": "comment"
    },
    "66": {
        "file_id": 1,
        "content": "        # before proceeding. Otherwise, your agent might be a little derp.\n        action = agent_action\n        if isinstance(action[\"buttons\"], th.Tensor):\n            action = {\n                \"buttons\": agent_action[\"buttons\"].cpu().numpy(),\n                \"camera\": agent_action[\"camera\"].cpu().numpy()\n            }\n        minerl_action = self.action_mapper.to_factored(action)\n        minerl_action_transformed = self.action_transformer.policy2env(minerl_action)\n        return minerl_action_transformed\n    def _env_action_to_agent(self, minerl_action_transformed, to_torch=False, check_if_null=False):\n        \"\"\"\n        Turn action from MineRL to model's action.\n        Note that this will add batch dimensions to the action.\n        Returns numpy arrays, unless `to_torch` is True, in which case it returns torch tensors.\n        If `check_if_null` is True, check if the action is null (no action) after the initial\n        transformation. This matches the behaviour done in OpenAI's VPT work.\n        If action is null, return \"None\" instead",
        "type": "code",
        "location": "/agent.py:155-175"
    },
    "67": {
        "file_id": 1,
        "content": "This code snippet is part of a function that transforms actions from MineRL format to the model's action format and vice versa. It also handles the conversion between PyTorch tensors and numpy arrays, and checks if the action is null (no action).",
        "type": "comment"
    },
    "68": {
        "file_id": 1,
        "content": "        \"\"\"\n        minerl_action = self.action_transformer.env2policy(minerl_action_transformed)\n        if check_if_null:\n            if np.all(minerl_action[\"buttons\"] == 0) and np.all(minerl_action[\"camera\"] == self.action_transformer.camera_zero_bin):\n                return None\n        # Add batch dims if not existant\n        if minerl_action[\"camera\"].ndim == 1:\n            minerl_action = {k: v[None] for k, v in minerl_action.items()}\n        action = self.action_mapper.from_factored(minerl_action)\n        if to_torch:\n            action = {k: th.from_numpy(v).to(self.device) for k, v in action.items()}\n        return action\n    def get_action(self, minerl_obs):\n        \"\"\"\n        Get agent's action for given MineRL observation.\n        Agent's hidden state is tracked internally. To reset it,\n        call `reset()`.\n        \"\"\"\n        agent_input = self._env_obs_to_agent(minerl_obs)\n        # The \"first\" argument could be used to reset tell episode\n        # boundaries, but we are only using this for predicting (for now),",
        "type": "code",
        "location": "/agent.py:176-199"
    },
    "69": {
        "file_id": 1,
        "content": "This code defines a class that takes MineRL observations as input and outputs the corresponding action. It includes methods for transforming actions, mapping actions, and getting an agent's action for a given observation. The action is returned with batch dimensions if necessary, and can be converted to PyTorch tensors if needed.",
        "type": "comment"
    },
    "70": {
        "file_id": 1,
        "content": "        # so we do not hassle with it yet.\n        agent_action, self.hidden_state, _ = self.policy.act(\n            agent_input, self._dummy_first, self.hidden_state,\n            stochastic=True\n        )\n        minerl_action = self._agent_action_to_env(agent_action)\n        return minerl_action",
        "type": "code",
        "location": "/agent.py:200-206"
    },
    "71": {
        "file_id": 1,
        "content": "This code selects an action from the agent's policy and returns it after converting to environment format.",
        "type": "comment"
    },
    "72": {
        "file_id": 2,
        "content": "/behavioural_cloning.py",
        "type": "filepath"
    },
    "73": {
        "file_id": 2,
        "content": "The code imports necessary libraries, defines parameters, and creates an agent object for policy-based actor-critic model training in a behavioral cloning task. It trains the model using batches of data, updates weights, and reports average loss at specified intervals.",
        "type": "summary"
    },
    "74": {
        "file_id": 2,
        "content": "# Basic behavioural cloning\n# Note: this uses gradient accumulation in batches of ones\n#       to perform training.\n#       This will fit inside even smaller GPUs (tested on 8GB one),\n#       but is slow.\n# NOTE: This is _not_ the original code used for VPT!\n#       This is merely to illustrate how to fine-tune the models and includes\n#       the processing steps used.\n# This will likely be much worse than what original VPT did:\n# we are not training on full sequences, but only one step at a time to save VRAM.\nfrom argparse import ArgumentParser\nimport pickle\nimport time\nimport gym\nimport minerl\nimport torch as th\nimport numpy as np\nfrom agent import PI_HEAD_KWARGS, MineRLAgent\nfrom data_loader import DataLoader\nfrom lib.tree_util import tree_map\nEPOCHS = 2\n# Needs to be <= number of videos\nBATCH_SIZE = 8\n# Ideally more than batch size to create\n# variation in datasets (otherwise, you will\n# get a bunch of consecutive samples)\n# Decrease this (and batch_size) if you run out of memory\nN_WORKERS = 12\nDEVICE = \"cuda\"",
        "type": "code",
        "location": "/behavioural_cloning.py:1-34"
    },
    "75": {
        "file_id": 2,
        "content": "This code imports necessary libraries and defines constants for basic behavioral cloning using gradient accumulation. It uses a smaller GPU, and it's not the original code used for VPT but serves to illustrate fine-tuning models with specific processing steps. The code specifies the number of epochs, batch size, number of workers, and device for training.",
        "type": "comment"
    },
    "76": {
        "file_id": 2,
        "content": "LOSS_REPORT_RATE = 100\nLEARNING_RATE = 0.000181\nWEIGHT_DECAY = 0.039428\nMAX_GRAD_NORM = 5.0\ndef load_model_parameters(path_to_model_file):\n    agent_parameters = pickle.load(open(path_to_model_file, \"rb\"))\n    policy_kwargs = agent_parameters[\"model\"][\"args\"][\"net\"][\"args\"]\n    pi_head_kwargs = agent_parameters[\"model\"][\"args\"][\"pi_head_opts\"]\n    pi_head_kwargs[\"temperature\"] = float(pi_head_kwargs[\"temperature\"])\n    return policy_kwargs, pi_head_kwargs\ndef behavioural_cloning_train(data_dir, in_model, in_weights, out_weights):\n    agent_policy_kwargs, agent_pi_head_kwargs = load_model_parameters(in_model)\n    # To create model with the right environment.\n    # All basalt environments have the same settings, so any of them works here\n    env = gym.make(\"MineRLBasaltFindCave-v0\")\n    agent = MineRLAgent(env, device=DEVICE, policy_kwargs=agent_policy_kwargs, pi_head_kwargs=agent_pi_head_kwargs)\n    agent.load_weights(in_weights)\n    env.close()\n    policy = agent.policy\n    trainable_parameters = policy.parameters()",
        "type": "code",
        "location": "/behavioural_cloning.py:36-60"
    },
    "77": {
        "file_id": 2,
        "content": "Load model parameters from file, define environment settings, and create agent object with defined policy and head parameters.",
        "type": "comment"
    },
    "78": {
        "file_id": 2,
        "content": "    # Parameters taken from the OpenAI VPT paper\n    optimizer = th.optim.Adam(\n        trainable_parameters,\n        lr=LEARNING_RATE,\n        weight_decay=WEIGHT_DECAY\n    )\n    data_loader = DataLoader(\n        dataset_dir=data_dir,\n        n_workers=N_WORKERS,\n        batch_size=BATCH_SIZE,\n        n_epochs=EPOCHS\n    )\n    start_time = time.time()\n    # Keep track of the hidden state per episode/trajectory.\n    # DataLoader provides unique id for each episode, which will\n    # be different even for the same trajectory when it is loaded\n    # up again\n    episode_hidden_states = {}\n    dummy_first = th.from_numpy(np.array((False,))).to(DEVICE)\n    loss_sum = 0\n    for batch_i, (batch_images, batch_actions, batch_episode_id) in enumerate(data_loader):\n        batch_loss = 0\n        for image, action, episode_id in zip(batch_images, batch_actions, batch_episode_id):\n            agent_action = agent._env_action_to_agent(action, to_torch=True, check_if_null=True)\n            if agent_action is None:\n                # Action was null",
        "type": "code",
        "location": "/behavioural_cloning.py:62-91"
    },
    "79": {
        "file_id": 2,
        "content": "Setting up optimizer, data loader, and initializing variables for training.",
        "type": "comment"
    },
    "80": {
        "file_id": 2,
        "content": "                continue\n            agent_obs = agent._env_obs_to_agent({\"pov\": image})\n            if episode_id not in episode_hidden_states:\n                # TODO need to clean up this hidden state after worker is done with the work item.\n                #      Leaks memory, but not tooooo much at these scales (will be a problem later).\n                episode_hidden_states[episode_id] = policy.initial_state(1)\n            agent_state = episode_hidden_states[episode_id]\n            pi_distribution, v_prediction, new_agent_state = policy.get_output_for_observation(\n                agent_obs,\n                agent_state,\n                dummy_first\n            )\n            log_prob  = policy.get_logprob_of_action(pi_distribution, agent_action)\n            # Make sure we do not try to backprop through sequence\n            # (fails with current accumulation)\n            new_agent_state = tree_map(lambda x: x.detach(), new_agent_state)\n            episode_hidden_states[episode_id] = new_agent_state\n            # Finally, update the agent to increase the probability of the",
        "type": "code",
        "location": "/behavioural_cloning.py:92-114"
    },
    "81": {
        "file_id": 2,
        "content": "The code is setting up the environment for a policy-based actor-critic model in a behavioral cloning task. It assigns the hidden state for the episode, gets the output for the observation, calculates the log probability of the action, and updates the agent's state.",
        "type": "comment"
    },
    "82": {
        "file_id": 2,
        "content": "            # taken action.\n            # Remember to take mean over batch losses\n            loss = -log_prob / BATCH_SIZE\n            batch_loss += loss.item()\n            loss.backward()\n        th.nn.utils.clip_grad_norm_(trainable_parameters, MAX_GRAD_NORM)\n        optimizer.step()\n        optimizer.zero_grad()\n        loss_sum += batch_loss\n        if batch_i % LOSS_REPORT_RATE == 0:\n            time_since_start = time.time() - start_time\n            print(f\"Time: {time_since_start:.2f}, Batches: {batch_i}, Avrg loss: {loss_sum / LOSS_REPORT_RATE:.4f}\")\n            loss_sum = 0\n    state_dict = policy.state_dict()\n    th.save(state_dict, out_weights)\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--data-dir\", type=str, required=True, help=\"Path to the directory containing recordings to be trained on\")\n    parser.add_argument(\"--in-model\", required=True, type=str, help=\"Path to the .model file to be finetuned\")\n    parser.add_argument(\"--in-weights\", required=True, type=str, help=\"Path to the .weights file to be finetuned\")",
        "type": "code",
        "location": "/behavioural_cloning.py:115-139"
    },
    "83": {
        "file_id": 2,
        "content": "The code is training a policy model using behavioral cloning on batches of data. It calculates the batch loss, applies gradients and updates weights, saves state dictionary to a specified output file, and reports the average loss every LOSS_REPORT_RATE batches. The inputs are the path to the directory containing recordings for training, the path to the model file to be fine-tuned, and the path to the weights file to be fine-tuned.",
        "type": "comment"
    },
    "84": {
        "file_id": 2,
        "content": "    parser.add_argument(\"--out-weights\", required=True, type=str, help=\"Path where finetuned weights will be saved\")\n    args = parser.parse_args()\n    behavioural_cloning_train(args.data_dir, args.in_model, args.in_weights, args.out_weights)",
        "type": "code",
        "location": "/behavioural_cloning.py:140-143"
    },
    "85": {
        "file_id": 2,
        "content": "The code adds an argument for the output weights path and parses the command line arguments, then calls the behavioral cloning training function.",
        "type": "comment"
    },
    "86": {
        "file_id": 3,
        "content": "/data_loader.py",
        "type": "filepath"
    },
    "87": {
        "file_id": 3,
        "content": "The code imports libraries, initializes a data loader class for simpler code, lacks sub-sequence support, and processes data for a batch of samples with workers outputting all samples to the same batch. The `__del__` method terminates and joins processes when object is deleted.",
        "type": "summary"
    },
    "88": {
        "file_id": 3,
        "content": "# Code for loading OpenAI MineRL VPT datasets\n# NOTE: This is NOT original code used for the VPT experiments!\n#       (But contains all [or at least most] steps done in the original data loading)\nimport json\nimport glob\nimport os\nimport random\nfrom multiprocessing import Process, Queue, Event\nimport numpy as np\nimport cv2\nfrom run_inverse_dynamics_model import json_action_to_env_action\nfrom agent import resize_image, AGENT_RESOLUTION\nQUEUE_TIMEOUT = 10\nCURSOR_FILE = os.path.join(os.path.dirname(__file__), \"cursors\", \"mouse_cursor_white_16x16.png\")\nMINEREC_ORIGINAL_HEIGHT_PX = 720\n# If GUI is open, mouse dx/dy need also be adjusted with these scalers.\n# If data version is not present, assume it is 1.\nMINEREC_VERSION_SPECIFIC_SCALERS = {\n    \"5.7\": 0.5,\n    \"5.8\": 0.5,\n    \"6.7\": 2.0,\n    \"6.8\": 2.0,\n    \"6.9\": 2.0,\n}\ndef composite_images_with_alpha(image1, image2, alpha, x, y):\n    \"\"\"\n    Draw image2 over image1 at location x,y, using alpha as the opacity for image2.\n    Modifies image1 in-place\n    \"\"\"\n    ch = max(0, min(image1.shape[0] - y, image2.shape[0]))",
        "type": "code",
        "location": "/data_loader.py:1-40"
    },
    "89": {
        "file_id": 3,
        "content": "This code imports necessary libraries and defines functions for loading OpenAI MineRL VPT datasets, adjusting cursor position based on version-specific scalers, and compositing images with alpha transparency.",
        "type": "comment"
    },
    "90": {
        "file_id": 3,
        "content": "    cw = max(0, min(image1.shape[1] - x, image2.shape[1]))\n    if ch == 0 or cw == 0:\n        return\n    alpha = alpha[:ch, :cw]\n    image1[y:y + ch, x:x + cw, :] = (image1[y:y + ch, x:x + cw, :] * (1 - alpha) + image2[:ch, :cw, :] * alpha).astype(np.uint8)\ndef data_loader_worker(tasks_queue, output_queue, quit_workers_event):\n    \"\"\"\n    Worker for the data loader.\n    \"\"\"\n    cursor_image = cv2.imread(CURSOR_FILE, cv2.IMREAD_UNCHANGED)\n    # Assume 16x16\n    cursor_image = cursor_image[:16, :16, :]\n    cursor_alpha = cursor_image[:, :, 3:] / 255.0\n    cursor_image = cursor_image[:, :, :3]\n    while True:\n        task = tasks_queue.get()\n        if task is None:\n            break\n        trajectory_id, video_path, json_path = task\n        video = cv2.VideoCapture(video_path)\n        # NOTE: In some recordings, the game seems to start\n        #       with attack always down from the beginning, which\n        #       is stuck down until player actually presses attack\n        # NOTE: It is uncertain if this was the issue with the original code.",
        "type": "code",
        "location": "/data_loader.py:41-67"
    },
    "91": {
        "file_id": 3,
        "content": "The code reads a video and its corresponding JSON file to extract frames and annotations for each frame. It initializes a cursor image and alpha channel, then continuously processes tasks from the tasks queue. If a task is None, it breaks the loop. The code checks if the video contains the game starting with attack always down by noting that it might be stuck down until the player presses attack.",
        "type": "comment"
    },
    "92": {
        "file_id": 3,
        "content": "        attack_is_stuck = False\n        # Scrollwheel is allowed way to change items, but this is\n        # not captured by the recorder.\n        # Work around this by keeping track of selected hotbar item\n        # and updating \"hotbar.#\" actions when hotbar selection changes.\n        # NOTE: It is uncertain is this was/is an issue with the contractor data\n        last_hotbar = 0\n        with open(json_path) as json_file:\n            json_lines = json_file.readlines()\n            json_data = \"[\" + \",\".join(json_lines) + \"]\"\n            json_data = json.loads(json_data)\n        for i in range(len(json_data)):\n            if quit_workers_event.is_set():\n                break\n            step_data = json_data[i]\n            if i == 0:\n                # Check if attack will be stuck down\n                if step_data[\"mouse\"][\"newButtons\"] == [0]:\n                    attack_is_stuck = True\n            elif attack_is_stuck:\n                # Check if we press attack down, then it might not be stuck\n                if 0 in step_data[\"mouse\"][\"newButtons\"]:",
        "type": "code",
        "location": "/data_loader.py:68-91"
    },
    "93": {
        "file_id": 3,
        "content": "Checking if attack is stuck by monitoring scrollwheel actions and updating \"hotbar.#\" actions when hotbar selection changes.",
        "type": "comment"
    },
    "94": {
        "file_id": 3,
        "content": "                    attack_is_stuck = False\n            # If still stuck, remove the action\n            if attack_is_stuck:\n                step_data[\"mouse\"][\"buttons\"] = [button for button in step_data[\"mouse\"][\"buttons\"] if button != 0]\n            action, is_null_action = json_action_to_env_action(step_data)\n            # Update hotbar selection\n            current_hotbar = step_data[\"hotbar\"]\n            if current_hotbar != last_hotbar:\n                action[\"hotbar.{}\".format(current_hotbar + 1)] = 1\n            last_hotbar = current_hotbar\n            # Read frame even if this is null so we progress forward\n            ret, frame = video.read()\n            if ret:\n                # Skip null actions as done in the VPT paper\n                # NOTE: in VPT paper, this was checked _after_ transforming into agent's action-space.\n                #       We do this here as well to reduce amount of data sent over.\n                if is_null_action:\n                    continue\n                if step_data[\"isGuiOpen\"]:",
        "type": "code",
        "location": "/data_loader.py:92-113"
    },
    "95": {
        "file_id": 3,
        "content": "Checking for stuck state and removing action, updating hotbar selection, reading frame even if null to progress forward.",
        "type": "comment"
    },
    "96": {
        "file_id": 3,
        "content": "                    camera_scaling_factor = frame.shape[0] / MINEREC_ORIGINAL_HEIGHT_PX\n                    cursor_x = int(step_data[\"mouse\"][\"x\"] * camera_scaling_factor)\n                    cursor_y = int(step_data[\"mouse\"][\"y\"] * camera_scaling_factor)\n                    composite_images_with_alpha(frame, cursor_image, cursor_alpha, cursor_x, cursor_y)\n                cv2.cvtColor(frame, code=cv2.COLOR_BGR2RGB, dst=frame)\n                frame = np.asarray(np.clip(frame, 0, 255), dtype=np.uint8)\n                frame = resize_image(frame, AGENT_RESOLUTION)\n                output_queue.put((trajectory_id, frame, action), timeout=QUEUE_TIMEOUT)\n            else:\n                print(f\"Could not read frame from video {video_path}\")\n        video.release()\n        if quit_workers_event.is_set():\n            break\n    # Tell that we ended\n    output_queue.put(None)\nclass DataLoader:\n    \"\"\"\n    Generator class for loading batches from a dataset\n    This only returns a single step at a time per worker; no sub-sequences.",
        "type": "code",
        "location": "/data_loader.py:114-134"
    },
    "97": {
        "file_id": 3,
        "content": "Applies camera scaling factor to mouse coordinates, composes cursor image with frame, converts image color, clips and resizes the frame, then puts (trajectory_id, frame, action) in output queue. If frame cannot be read, prints an error message. Finally, releases video and checks quit_workers_event before putting None in output queue to signal end of data loading.",
        "type": "comment"
    },
    "98": {
        "file_id": 3,
        "content": "    Idea is that you keep track of the model's hidden state and feed that in,\n    along with one sample at a time.\n    + Simpler loader code\n    + Supports lower end hardware\n    - Not very efficient (could be faster)\n    - No support for sub-sequences\n    - Loads up individual files as trajectory files (i.e. if a trajectory is split into multiple files,\n      this code will load it up as a separate item).\n    \"\"\"\n    def __init__(self, dataset_dir, n_workers=8, batch_size=8, n_epochs=1, max_queue_size=16):\n        assert n_workers >= batch_size, \"Number of workers must be equal or greater than batch size\"\n        self.dataset_dir = dataset_dir\n        self.n_workers = n_workers\n        self.n_epochs = n_epochs\n        self.batch_size = batch_size\n        self.max_queue_size = max_queue_size\n        unique_ids = glob.glob(os.path.join(dataset_dir, \"*.mp4\"))\n        unique_ids = list(set([os.path.basename(x).split(\".\")[0] for x in unique_ids]))\n        self.unique_ids = unique_ids\n        # Create tuples of (video_path, json_path) for each unique_id",
        "type": "code",
        "location": "/data_loader.py:135-155"
    },
    "99": {
        "file_id": 3,
        "content": "This code initializes a data loader class that tracks the model's hidden state and feeds it along with one sample at a time. It supports simpler loader code, lower end hardware, but is not very efficient and lacks support for sub-sequences. The loader loads individual files as trajectory files if they are split into multiple files.",
        "type": "comment"
    }
}